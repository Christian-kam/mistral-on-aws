{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40de2463",
   "metadata": {},
   "source": [
    "# Deploy Mistral Small 3.1 on Amazon SageMaker with LMI\n",
    "\n",
    "**Recommended kernel(s):** This notebook can be run with any Amazon SageMaker Studio kernel.\n",
    "\n",
    "In this notebook, you will learn how to deploy the Mistral Small 3.1 24B instruct model (HuggingFace model ID: [mistralai/Mistral-Small-3.1-24B-Instruct-2503](https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503)) using Amazon SageMaker. The inference image will be the SageMaker-managed [LMI (Large Model Inference)](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-container-docs.html) Docker image. LMI images feature a [DJL serving](https://github.com/deepjavalibrary/djl-serving) stack powered by the [Deep Java Library](https://djl.ai/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f978a4b0",
   "metadata": {},
   "source": [
    "## Mistral Small 3.1: State-of-the-Art in a Compact Package\n",
    "\n",
    "Mistral Small 3.1 is the latest model from Mistral AI, featuring improved text performance, multimodal capabilities, and an expanded 128K token context window. With just 24 billion parameters, this model achieves state-of-the-art performance while being compact enough to run on a single RTX 4090 or a Mac with 32GB RAM when quantized.\n",
    "\n",
    "### Key Features and Capabilities\n",
    "\n",
    "- **Top-Tier Performance**: Outperforms comparable models like Gemma 2 27B and GPT-4o Mini across multiple benchmarks including MMLU, HumanEval, and various multimodal tasks\n",
    "- **Multimodal Understanding**: Handles both text and image inputs with advanced vision capabilities for tasks such as document verification, diagnostics, and visual inspection\n",
    "- **Long Context Window**: Supports up to 128K tokens for processing lengthy documents and conversations at speeds of 150 tokens per second\n",
    "- **Multilingual Support**: Fluent in dozens of languages including English, French, German, Chinese, Arabic, and many more making it suitable for global applications\n",
    "- **Efficiency**: Delivers impressive inference speeds (150 tokens/second) with minimal computational requirements compared to larger proprietary models\n",
    "- **Open Source**: Released under Apache 2.0 license for both research and commercial applications offering flexibility for developers and enterprises\n",
    "\n",
    "![Performance Comparison](imgs/image-mistral.webp)\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "Mistral Small 3.1 is well-suited for a variety of applications, including:\n",
    "\n",
    "- Fast-response conversational assistants\n",
    "- Low-latency function calling in automated workflows\n",
    "- Domain-specific expert systems via fine-tuning\n",
    "- Programming and math reasoning\n",
    "- Document analysis and processing\n",
    "- Image understanding and analysis\n",
    "\n",
    "### License agreement\n",
    "* This model is available under the Apache 2.0 license, as detailed on the original [model card](https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503).\n",
    "* This notebook is a sample notebook and not intended for production use.\n",
    "\n",
    "### Execution environment setup\n",
    "This notebook requires the following third-party Python dependencies:\n",
    "* AWS [`sagemaker`](https://sagemaker.readthedocs.io/en/stable/index.html) with a version greater than or equal to 2.242.0\n",
    "\n",
    "Let's install or upgrade these dependencies using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4840f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uq sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db80641f",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fac7ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.session import Session\n",
    "import logging\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b839ff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    \n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356106d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_MODEL_ID = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n",
    "\n",
    "base_name = HF_MODEL_ID.split('/')[-1].replace('.', '-').lower()\n",
    "model_lineage = HF_MODEL_ID.split(\"/\")[0]\n",
    "base_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d386c043",
   "metadata": {},
   "source": [
    "## Configure Model Serving Properties\n",
    "\n",
    "Now we'll create a `serving.properties` file that configures how the model will be served. This configuration is crucial for optimal performance and memory utilization.\n",
    "\n",
    "Key configurations explained:\n",
    "- **Engine**: Python backend for model serving\n",
    "- **Model Settings**:\n",
    "  - Using Mistral Small 3.1 24B Instruct model from Hugging Face\n",
    "  - Maximum sequence length for efficient processing\n",
    "  - Model loading timeout for proper initialization\n",
    "- **Performance Optimizations**:\n",
    "  - Tensor parallelism across all available GPUs\n",
    "  - Optimized GPU memory utilization target\n",
    "  - vLLM rolling batch with appropriate max size for efficient batching\n",
    "  - Tool call parser and config format specific to Mistral\n",
    "\n",
    "### Understanding KV Cache and Context Window\n",
    "\n",
    "The `max_model_len` parameter controls the maximum sequence length the model can handle, which directly affects the size of the KV (Key-Value) cache in GPU memory.\n",
    "\n",
    "1. Start with a conservative value (current: 8192)\n",
    "2. Monitor GPU memory usage\n",
    "3. Incrementally increase if memory permits\n",
    "4. Target the model's full context window capability (up to 128K tokens)\n",
    "\n",
    "While Mistral Small 3.1 supports up to 128K context window as mentioned in Mistral AI's official blog post, we begin with a smaller value for efficient deployment; this can be adjusted based on your specific requirements and available resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae550bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory that will contain the files\n",
    "from pathlib import Path\n",
    "\n",
    "model_dir = Path('code')\n",
    "model_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc214bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/serving.properties\n",
    "engine=Python\n",
    "option.tensor_parallel_degree=max\n",
    "option.gpu_memory_utilization=.87\n",
    "option.model_id=mistralai/Mistral-Small-3.1-24B-Instruct-2503\n",
    "option.rolling_batch=vllm\n",
    "option.max_model_len=8192\n",
    "option.tool_call_parser=mistral\n",
    "option.enable_auto_tool_choice=true\n",
    "option.trust_remote_code=true\n",
    "option.max_rolling_batch_size=16\n",
    "option.tokenizer_mode=mistral\n",
    "option.config_format=mistral\n",
    "option.load_format=mistral\n",
    "#option.limit_mm_per_prompt='image=4'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143b50d3",
   "metadata": {},
   "source": [
    "## Configure vLLM Requirements\n",
    "\n",
    "(Optional) The `requirements.txt` file specifies the vLLM version needed for model inference. vLLM inference framework provides optimized serving capabilities.\n",
    "\n",
    "### Version Considerations\n",
    "- **vLLM 0.8.1**: Required for Mistral Small 3.1 model\n",
    "- **transformers 4.50.0**: Updated transformers library\n",
    "\n",
    "### Performance Impact\n",
    "Different vLLM versions can affect:\n",
    "- Inference speed\n",
    "- Memory utilization\n",
    "- Batch processing efficiency\n",
    "- Compatibility with other libraries\n",
    "\n",
    "### API Considerations\n",
    "\n",
    "The vLLM API has changed in newer versions, so we need to patch the default DJL implementation of the VLLMRollingBatch.inference method with the following:\n",
    "\n",
    "```self.engine.add_request(request_id, prompt=prompt_inputs, params=sampling_params, **request_params)```\n",
    "\n",
    "So provide a custom model.py file that contains the patch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152819c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/requirements.txt\n",
    "transformers==4.50.0\n",
    "vllm==0.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dc3e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/model.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from djl_python.huggingface import HuggingFaceService\n",
    "from djl_python.inputs import Input\n",
    "from djl_python.rolling_batch.vllm_rolling_batch import VLLMRollingBatch\n",
    "from djl_python.rolling_batch.rolling_batch_vllm_utils import (\n",
    "    update_request_cache_with_output, create_lora_request, get_lora_request,\n",
    "    get_prompt_inputs)\n",
    "import types\n",
    "import logging\n",
    "\n",
    "# Create the service\n",
    "_service = HuggingFaceService()\n",
    "\n",
    "# Define the patched inference method for VLLMRollingBatch\n",
    "def patched_inference(self, new_requests):\n",
    "    \"\"\"\n",
    "    Patched version of the inference method that works with vLLM 0.8.1\n",
    "    \"\"\"\n",
    "    # Import necessary classes from vllm within the function to ensure they're available\n",
    "    from vllm.sampling_params import RequestOutputKind\n",
    "    from vllm import SamplingParams\n",
    "    self.add_new_requests(new_requests)\n",
    "    # step 0: register new requests to engine\n",
    "    for request in new_requests:\n",
    "        from vllm.utils import random_uuid\n",
    "        request_id = random_uuid()\n",
    "        # Chat completions request route\n",
    "        if request.parameters.get(\"sampling_params\") is not None:\n",
    "            prompt_inputs = request.parameters.get(\"engine_prompt\")\n",
    "            sampling_params = request.parameters.get(\"sampling_params\")\n",
    "            sampling_params.output_kind = RequestOutputKind.DELTA\n",
    "        # LMI request route\n",
    "        else:\n",
    "            prompt_inputs = get_prompt_inputs(request)\n",
    "            params = self.translate_vllm_params(request.parameters)\n",
    "            sampling_params = SamplingParams(**params)\n",
    "        request_params = dict()\n",
    "        if request.adapter is not None:\n",
    "            adapter_name = request.adapter.get_property(\"name\")\n",
    "            request_params[\"lora_request\"] = get_lora_request(\n",
    "                adapter_name, self.lora_requests)\n",
    "        \n",
    "        # This is the key change: using the new API format for add_request\n",
    "        # Changed from:\n",
    "        # self.engine.add_request(request_id=request_id, inputs=prompt_inputs, params=sampling_params, **request_params)\n",
    "        # To:\n",
    "        self.engine.add_request(request_id, prompt=prompt_inputs, params=sampling_params, **request_params)\n",
    "        \n",
    "        self.request_cache[request_id] = {\n",
    "            \"request_output\": request.request_output\n",
    "        }\n",
    "    request_outputs = self.engine.step()\n",
    "\n",
    "    # step 1: put result to cache and request_output\n",
    "    for request_output in request_outputs:\n",
    "        self.request_cache = update_request_cache_with_output(\n",
    "            self.request_cache, request_output, self.get_tokenizer())\n",
    "\n",
    "    for request in self.active_requests:\n",
    "        request_output = request.request_output\n",
    "        if request_output.finished:\n",
    "            request.last_token = True\n",
    "\n",
    "    return self.postprocess_results()\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    \"\"\"\n",
    "    Default handler function\n",
    "    \"\"\"\n",
    "    if not _service.initialized:\n",
    "        # Apply the monkey patch to VLLMRollingBatch.inference\n",
    "        try:\n",
    "            # Import the necessary modules from vllm\n",
    "            import vllm\n",
    "            from vllm import SamplingParams\n",
    "            from vllm.sampling_params import RequestOutputKind\n",
    "            \n",
    "            # Patch for vLLM 0.8.1\n",
    "            logging.info(\"Patching VLLMRollingBatch.inference for vLLM 0.8.1\")\n",
    "            VLLMRollingBatch.inference = patched_inference\n",
    "            logging.info(\"Successfully patched VLLMRollingBatch.inference\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to patch VLLMRollingBatch.inference: {e}\")\n",
    "        \n",
    "        # Initialize the service\n",
    "        props = inputs.get_properties()\n",
    "        _service.initialize(props)\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        # initialization request\n",
    "        return None\n",
    "\n",
    "    return _service.inference(inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198aa63e",
   "metadata": {},
   "source": [
    "## Upload Uncompressed Artifacts to S3\n",
    "SageMaker allows us to provide [uncompress files](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-uncompressed.html). Thus, we directly upload the folder that contains `model.py`, `serving.properties` and `requirements.txt` to S3.\n",
    "\n",
    "This process:\n",
    "1. Determines the S3 bucket location (using SageMaker default bucket)\n",
    "2. Defines a prefix path for organization\n",
    "3. Uploads the packaged artifacts\n",
    "\n",
    "> **Note**: The default SageMaker bucket follows the naming pattern: `sagemaker-{region}-{account-id}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1d9dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "sagemaker_default_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "code_model_uri = S3Uploader.upload(\n",
    "    local_path=\"code\",\n",
    "    desired_s3_uri=f\"s3://{sagemaker_default_bucket}/lmi/{base_name}/code\"\n",
    ")\n",
    "\n",
    "print(f\"code_model_uri: {code_model_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f306c5d",
   "metadata": {},
   "source": [
    "## Configure Model Container and Instance\n",
    "\n",
    "For deploying Mistral Small 3.1 24B Instruct, we'll use:\n",
    "- **LMI (Deep Java Library) Inference Container**: A container optimized for large language model inference\n",
    "- **G6 or G6e Instance**: AWS's GPU instance types powered by powerful NVIDIA GPUs\n",
    "\n",
    "Key configurations:\n",
    "- The container URI points to the DJL inference container in ECR (Elastic Container Registry)\n",
    "- We use a GPU instance that offers:\n",
    "  - Sufficient NVIDIA GPUs for tensor parallelism\n",
    "  - Adequate GPU memory for model weights and KV cache\n",
    "  - High network bandwidth\n",
    "  - Sufficient system memory\n",
    "\n",
    "> **Note**: The region in the container URI should match your AWS region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fb5bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_instance_type = \"ml.g6.12xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da05b518",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = \"763104351884.dkr.ecr.{}.amazonaws.com/djl-inference:0.32.0-lmi14.0.0-cu126\".format(sagemaker_session.boto_session.region_name)\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f7c765",
   "metadata": {},
   "source": [
    "## Create SageMaker Model\n",
    "\n",
    "Now we'll create a SageMaker Model object that combines our:\n",
    "- Container image (LMI)\n",
    "- Model artifacts (configuration files)\n",
    "- IAM role (for permissions)\n",
    "\n",
    "This step defines the model configuration but doesn't deploy it yet. The Model object represents the combination of:\n",
    "\n",
    "1. **Container Image** (`image_uri`): DJL Inference optimized for LLMs\n",
    "2. **Model Data** (`model_data`): Our configuration files in S3\n",
    "3. **IAM Role** (`role`): Permissions for model execution\n",
    "\n",
    "### Required Permissions\n",
    "The IAM role needs:\n",
    "- S3 read access for model artifacts\n",
    "- CloudWatch permissions for logging\n",
    "- ECR permissions to pull the container\n",
    "\n",
    "#### HUGGING_FACE_HUB_TOKEN \n",
    "Mistral Small 3.1 24B Instruct is available on Hugging Face, but you may need to provide your Hugging Face token if it's a gated model or if you want to access it from a private repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a708a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the S3 URI for your uncompressed code files \n",
    "model_data = {\n",
    "    \"S3DataSource\": {\n",
    "        \"S3Uri\": f\"{code_model_uri}/\",\n",
    "        \"S3DataType\": \"S3Prefix\",\n",
    "        \"CompressionType\": \"None\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1648dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_HUB_TOKEN = \"REPLACE WITH YOUR HF TOKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a297fe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.model import Model\n",
    "\n",
    "model_name = name_from_base(base_name, short=True)+\"-1\"\n",
    "print(model_name)\n",
    "# Create model\n",
    "mistral_3_model = Model(\n",
    "    name = model_name,\n",
    "    image_uri=image_uri,\n",
    "    model_data=model_data,  # Path to uncompressed code files\n",
    "    role=role,\n",
    "    env={\n",
    "        \"HF_TASK\": \"Image-Text-to-Text\",\n",
    "        \"OPTION_LIMIT_MM_PER_PROMPT\": \"image=4\",# For multimodal capabilities\n",
    "        'HUGGING_FACE_HUB_TOKEN': HUGGING_FACE_HUB_TOKEN # If needed\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94089ac0",
   "metadata": {},
   "source": [
    "## Deploy Model to SageMaker Endpoint\n",
    "\n",
    "Now we'll deploy our model to a SageMaker endpoint for real-time inference. This is a significant step that:\n",
    "1. Provisions the specified compute resources (G6 instance)\n",
    "2. Deploys the model container\n",
    "3. Sets up the endpoint for API access\n",
    "\n",
    "### Deployment Configuration\n",
    "- **Instance Count**: 1 instance for single-node deployment\n",
    "- **Instance Type**: GPU instance for high-performance inference\n",
    "\n",
    "![Accuracy Comparison](imgs/mistral-instruct-knowledge.webp)\n",
    "\n",
    "> ⚠️ **Important**: \n",
    "> - Deployment will take 15-20 minutes\n",
    "> - Monitor the CloudWatch logs for progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dc4ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "endpoint_name = name_from_base(base_name, short=True)\n",
    "\n",
    "mistral_3_model.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=gpu_instance_type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8860ce4f",
   "metadata": {},
   "source": [
    "### Use the code below to create a predictor from an existing endpoint and make inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa14ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer, IdentitySerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "endpoint_name = \"mistral-small-3-1-24b-instruct-2503-250401-1938\" # replace with your endpoint name\n",
    "\n",
    "small_3_predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec2a5b4",
   "metadata": {},
   "source": [
    "## Text-only Inference\n",
    "\n",
    "Mistral Small 3.1 handles standard text generation tasks with exceptional quality. With its 24B parameter size, it delivers responses that match or exceed much larger models while maintaining fast inference speeds of approximately 150 tokens per second as noted in Mistral AI's official documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca53d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "payload = {\n",
    "    \"messages\" : [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\n",
    "                \"type\": \"text\", \n",
    "                \"text\": \"Write me a poem about Machine Learning.\"\n",
    "            }]\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\":300,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"top_k\": 250\n",
    "}\n",
    "\n",
    "response = small_3_predictor.predict(payload)\n",
    "print(response['choices'][0]['message']['content'])\n",
    "\n",
    "# Print usage statistics\n",
    "print(\"=== Token Usage ===\")\n",
    "usage = response['usage']\n",
    "print(f\"Prompt Tokens: {usage['prompt_tokens']}\")\n",
    "print(f\"Completion Tokens: {usage['completion_tokens']}\")\n",
    "print(f\"Total Tokens: {usage['total_tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b0d5ce",
   "metadata": {},
   "source": [
    "## Multimodality\n",
    "\n",
    "Mistral Small 3.1 models are multimodal, handling both text and image input while generating text output. As described by Mistral AI, this multimodal capability makes the model well-suited for various image understanding tasks like document verification, diagnostics, visual inspection for quality checks, and object detection.\n",
    "\n",
    "Here's an example of how to query the model with an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a3dd07-30b8-44a2-bd53-952ae577380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as IPyImage\n",
    "\n",
    "IPyImage(url=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147c01f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "payload = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"image_url\", \n",
    "          \"image_url\": {\"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},\n",
    "        },\n",
    "        {\"type\": \"text\", \"text\": \"What animal is on the candy?\"}\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "response = small_3_predictor.predict(payload)\n",
    "print(response['choices'][0]['message']['content'])\n",
    "\n",
    "# Print usage statistics\n",
    "print(\"=== Token Usage ===\")\n",
    "usage = response['usage']\n",
    "print(f\"Prompt Tokens: {usage['prompt_tokens']}\")\n",
    "print(f\"Completion Tokens: {usage['completion_tokens']}\")\n",
    "print(f\"Total Tokens: {usage['total_tokens']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029acd6f-02dd-410a-b9b2-bb65fcd79dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_1_KITTEN = \"https://resources.djl.ai/images/kitten.jpg\"\n",
    "IMAGE_2_TRUCK = \"https://resources.djl.ai/images/truck.jpg\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e747adf-fa34-4519-aa74-9918e42098ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "response_kitten = requests.get(IMAGE_1_KITTEN)\n",
    "img_kitten = Image.open(BytesIO(response_kitten.content))\n",
    "\n",
    "response_truck = requests.get(IMAGE_2_TRUCK)\n",
    "img_truck = Image.open(BytesIO(response_truck.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6aa526-089d-481e-83ac-76441b1b05b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_image_payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Can you describe the following images and tell me what they have in common? If they have nothing in common, please explain why.\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": IMAGE_1_KITTEN\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": IMAGE_2_TRUCK\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\": 0.6,\n",
    "    \"top_p\": 0.9,\n",
    "}\n",
    "print(\"These are the images provided to the model\")\n",
    "img_kitten.show()\n",
    "img_truck.show()\n",
    "multi_image_output = small_3_predictor.predict(multi_image_payload)\n",
    "print(multi_image_output['choices'][0]['message']['content'])\n",
    "print('----------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86963c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "small_3_predictor.delete_model()\n",
    "small_3_predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523ff071",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You've successfully deployed the Mistral Small 3.1 24B Instruct model on Amazon SageMaker and learned how to interact with it for both text and image inputs. This powerful model combines efficient performance with state-of-the-art capabilities in a relatively compact package.\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "1. Mistral Small 3.1 offers exceptional performance comparable to much larger models\n",
    "2. The model supports both text and image inputs with its multimodal capabilities\n",
    "3. SageMaker provides a robust platform for deploying and scaling inference\n",
    "4. Streaming implementation allows for real-time interactive applications\n",
    "\n",
    "For production deployments, consider:\n",
    "- Adjusting instance types based on traffic patterns\n",
    "- Implementing auto-scaling for cost optimization\n",
    "- Fine-tuning the model for your specific domain\n",
    "- Experimenting with different inference parameters for optimal user experience\n",
    "\n",
    "For more information about Mistral Small 3.1, visit [Mistral AI's official page](https://mistral.ai/news/mistral-small-3-1/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e9f078-3f14-400b-95ae-cee0a536495d",
   "metadata": {},
   "source": [
    "#### Distributed by:\n",
    "- AWS\n",
    "- Mistral"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
