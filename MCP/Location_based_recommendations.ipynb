{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd8717cd-9df1-44b7-93d1-ad222d0a3994",
   "metadata": {},
   "source": [
    "## Building an Intelligent Restaurant Recommender with MCP and Mistral Large 2\n",
    "\n",
    "In this notebook, we'll build a personalized restaurant recommendation system using Mistral Large 2 with the Model Context Protocol (MCP). Our system will consider user preferences stored in memory to provide tailored restaurant recommendations.\n",
    "\n",
    "MCP (Model Context Protocol) is an open protocol that standardizes how LLMs interact with external data sources and tools. Think of MCP as a \"universal connector\" that lets AI models plug into different services while maintaining consistent interfaces.\n",
    "\n",
    "We'll use three MCP servers:\n",
    "\n",
    "1. **[Google Maps MCP Server](https://github.com/modelcontextprotocol/servers/tree/main/src/google-maps)**: For finding restaurants, getting details, and understanding locations\n",
    "2. **[Time MCP Server](https://github.com/modelcontextprotocol/servers/tree/main/src/time)**: To check local time and determine restaurant availability\n",
    "3. **[Memory MCP Server](https://github.com/modelcontextprotocol/servers/tree/main/src/memory)**: To store and retrieve user preferences\n",
    "\n",
    "### Getting a Google Maps API Key\n",
    "\n",
    "To use the Google Maps MCP server, you'll need your own API key:\n",
    "\n",
    "1. Go to the [Google Cloud Console](https://developers.google.com/maps/documentation/javascript/get-api-key#create-api-keys)\n",
    "2. Create a new project or select an existing one\n",
    "3. Follow the steps to create a new API key\n",
    "\n",
    "\n",
    "Our system works as follows:\n",
    "\n",
    "**User Request**: User asks for restaurant recommendations with location data\n",
    "\n",
    "**Location Analysis**: Maps MCP server converts coordinates to a readable address\n",
    "\n",
    "**Time Check**: Time MCP server determines local time to filter for open restaurants\n",
    "\n",
    "**Preference Retrieval**: Memory MCP server provides user's restaurant preferences\n",
    "\n",
    "**Restaurant Search**: Maps MCP searches for restaurants matching preferences\n",
    "\n",
    "**Detail Enrichment**: Get specific details about promising restaurant options\n",
    "\n",
    "**Recommendation**: Format personalized recommendations based on user preferences\n",
    "\n",
    "\n",
    "## ⚠️ Environment Note\n",
    "**This notebook is optimized for Jupyter Lab environment.**\n",
    "\n",
    "While this code can run in other environments (like SageMaker Studio), you may encounter async communication issues. For the best experience:\n",
    "- Run this in Jupyter Lab\n",
    "- Ensure all dependencies are properly installed\n",
    "- Make sure Node.js and npm are configured in your environment\n",
    "\n",
    "If you encounter any issues in other environments, try using the alternative time server configuration:\n",
    "```python\n",
    "\"time\": StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    args=[\"-m\", \"mcp_server_time\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18d1456-8853-42ee-90d7-105a075ee991",
   "metadata": {},
   "source": [
    "## Setting Up MCP Servers\n",
    "\n",
    "To set up our MCP servers, we need to install several components:\n",
    "\n",
    "### Required Packages\n",
    "```python\n",
    "!pip install -q mcp boto3 nest-asyncio uv mcp-server-time && npm install -g @modelcontextprotocol/server-google-maps @modelcontextprotocol/server-memory\n",
    "```\n",
    "This command installs:\n",
    "\n",
    "mcp: The MCP client library\n",
    "\n",
    "boto3: AWS SDK for Python\n",
    "\n",
    "nest-asyncio: Enables async operations in Jupyter\n",
    "                     \n",
    "uv: Required for the time server\n",
    "                     \n",
    "mcp-server-time: The Time MCP server\n",
    "                     \n",
    "@modelcontextprotocol/server-google-maps: The Google Maps MCP server\n",
    "                     \n",
    "@modelcontextprotocol/server-memory: The Memory MCP server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bbc915-860f-475b-9875-665a6b03499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q mcp boto3 nest-asyncio uv mcp-server-time && npm install -g @modelcontextprotocol/server-google-maps @modelcontextprotocol/server-memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6b0180-4179-4a76-ac69-79bead81e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from contextlib import AsyncExitStack\n",
    "import asyncio\n",
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a522e142-44c2-4aaf-b8e2-b93e9e882a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23280189-af26-4126-93ec-50c3c020aa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock = boto3.client('bedrock-runtime')\n",
    "MISTRAL_MODEL = 'mistral.mistral-large-2407-v1:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca1b343-8760-4682-87ec-579e5d8e9803",
   "metadata": {},
   "source": [
    "## The RestaurantRecommender class handles:\n",
    "\n",
    "**Server Connections**: Establishes connections to all three MCP servers\n",
    "\n",
    "**Tool Routing**: Routes tool calls to the appropriate MCP server\n",
    "\n",
    "**Query Processing**: Manages the conversation with Mistral Large 2, handling tool requests\n",
    "\n",
    "**Memory Management**: Sets up and retrieves user preferences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc27584-20e6-4173-9bed-b085f1974fea",
   "metadata": {},
   "source": [
    "Our notebook initializes these servers using the MCP client library, with specific configurations for each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de97490b-8dc6-4aa4-a23b-7cfb3e6044f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RestaurantRecommender:\n",
    "    def __init__(self):\n",
    "        self.exit_stack = AsyncExitStack()\n",
    "        self.sessions = {}\n",
    "        \n",
    "    async def connect_to_mcp_servers(self):\n",
    "        \"\"\"Initialize connections to MCP servers with improved error handling\"\"\"\n",
    "        \n",
    "        servers_config = {\n",
    "            \"maps\": StdioServerParameters(\n",
    "                command=\"npx\",\n",
    "                args=[\"@modelcontextprotocol/server-google-maps\"],\n",
    "                env={\"GOOGLE_MAPS_API_KEY\": \"Your_API_Key\"}\n",
    "            ),\n",
    "            \"time\": StdioServerParameters(\n",
    "                command=\"uvx\",  # Using uvx as recommended\n",
    "                args=[\"mcp-server-time\"]\n",
    "            ),\n",
    "            \"memory\": StdioServerParameters(\n",
    "                command=\"npx\",\n",
    "                args=[\"@modelcontextprotocol/server-memory\"]\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        for name, params in servers_config.items():\n",
    "            try:\n",
    "                print(f\"Connecting to {name} server...\")\n",
    "                stdio_transport = await self.exit_stack.enter_async_context(\n",
    "                    stdio_client(params)\n",
    "                )\n",
    "                stdio, write = stdio_transport\n",
    "                session = await self.exit_stack.enter_async_context(\n",
    "                    ClientSession(stdio, write)\n",
    "                )\n",
    "                self.sessions[name] = session\n",
    "                await session.initialize()\n",
    "                \n",
    "                # Test the connection by getting available tools\n",
    "                tools = await session.list_tools()\n",
    "                print(f\"Successfully connected to {name} server. Available tools:\")\n",
    "                # Modified this part to handle tuple format\n",
    "                for tool in tools.tools:  # Access the tools attribute\n",
    "                    print(f\"  - {tool.name}: {tool.description}\")\n",
    "                \n",
    "                # For time server, let's test it specifically\n",
    "                if name == \"time\":\n",
    "                    try:\n",
    "                        current_time = await session.call_tool(\"get_current_time\", {\"timezone\": \"UTC\"})\n",
    "                        print(f\"Time server test: Current UTC time is {current_time.content}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Time server test failed: {str(e)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error connecting to {name} server: {str(e)}\")\n",
    "                print(\"Stack trace:\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                raise\n",
    "\n",
    "    async def setup_memory(self):\n",
    "        \"\"\"Initialize memory with default user preferences\"\"\"\n",
    "        try:\n",
    "            await self.sessions[\"memory\"].call_tool(\n",
    "                \"create_entities\",\n",
    "                {\n",
    "                    \"entities\": [\n",
    "                        {\n",
    "                            \"name\": \"default_user\",\n",
    "                            \"entityType\": \"person\",\n",
    "                            \"observations\": [\n",
    "                                \"Prefers Japanese and sushi restaurants\",\n",
    "                                \"Typically spends $$ on meals\",\n",
    "                                \"Prefers restaurants within 2 miles\",\n",
    "                                \"Enjoys trying new ramen places\",\n",
    "                                \"Vegetarian options are important\"\n",
    "                            ]\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "            print(\"Memory initialized with user preferences\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error setting up memory: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    async def get_available_tools(self) -> List[Dict]:\n",
    "        \"\"\"Get list of available tools from all MCP servers\"\"\"\n",
    "        tools = []\n",
    "        for server_name, session in self.sessions.items():\n",
    "            try:\n",
    "                response = await session.list_tools()\n",
    "                tools.extend([{\n",
    "                    \"name\": t.name,\n",
    "                    \"description\": t.description,\n",
    "                    \"input_schema\": t.inputSchema\n",
    "                } for t in response.tools])  # Access response.tools instead of response\n",
    "            except Exception as e:\n",
    "                print(f\"Error getting tools from {server_name}: {str(e)}\")\n",
    "        return tools\n",
    "\n",
    "    async def execute_tool(self, tool_name: str, arguments: Dict) -> str:\n",
    "        tool_prefix_map = {\n",
    "            \"maps_\": \"maps\",\n",
    "            \"get_current_time\": \"time\",\n",
    "            \"convert_time\": \"time\", \n",
    "            \"search_nodes\": \"memory\",\n",
    "            \"open_nodes\": \"memory\",\n",
    "            \"create_entities\": \"memory\",\n",
    "            \"create_relations\": \"memory\",\n",
    "            \"add_observations\": \"memory\",\n",
    "            \"delete_entities\": \"memory\",\n",
    "            \"delete_observations\": \"memory\",\n",
    "            \"delete_relations\": \"memory\",\n",
    "            \"read_graph\": \"memory\"\n",
    "        }\n",
    "        \n",
    "        # First try to route based on tool prefix\n",
    "        server_name = None\n",
    "        for prefix, server in tool_prefix_map.items():\n",
    "            if tool_name == prefix or tool_name.startswith(prefix):\n",
    "                server_name = server\n",
    "                break\n",
    "        \n",
    "        if server_name and server_name in self.sessions:\n",
    "            try:\n",
    "                print(f\"Executing {tool_name} on {server_name} server\")\n",
    "                result = await self.sessions[server_name].call_tool(tool_name, arguments)\n",
    "                return result.content\n",
    "            except Exception as e:\n",
    "                print(f\"Error executing {tool_name} on {server_name} server: {str(e)}\")\n",
    "                raise ValueError(f\"Error executing {tool_name}: {str(e)}\")\n",
    "        \n",
    "        \n",
    "        if not server_name:\n",
    "            for name, session in self.sessions.items():\n",
    "                try:\n",
    "                    print(f\"Trying {tool_name} on {name} server\")\n",
    "                    result = await session.call_tool(tool_name, arguments)\n",
    "                    return result.content\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed on {name} server: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        raise ValueError(f\"Tool {tool_name} not found in any MCP server\")\n",
    "\n",
    "    async def process_query(self, query: str, lat: float, lng: float):\n",
    "        \"\"\"Process a query using Mistral with tool access\"\"\"\n",
    "        try:\n",
    "            # Initialize conversation with just the user query\n",
    "            conversation = [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"text\": f\"Find restaurants near latitude {lat}, longitude {lng}. {query}\"}]\n",
    "            }]\n",
    "            \n",
    "            # Set up tools\n",
    "            raw_tools = await self.get_available_tools()\n",
    "            formatted_tools = []\n",
    "            for tool in raw_tools:\n",
    "                formatted_tools.append({\n",
    "                    \"toolSpec\": {\n",
    "                        \"name\": tool[\"name\"],\n",
    "                        \"description\": tool[\"description\"],\n",
    "                        \"inputSchema\": {\n",
    "                            \"json\": tool[\"input_schema\"]\n",
    "                        }\n",
    "                    }\n",
    "                })\n",
    "            \n",
    "            # Format system correctly\n",
    "            system = [{\"text\": SYSTEM_PROMPT}]\n",
    "            \n",
    "            while True:\n",
    "                print(f\"Sending request to Bedrock with {len(conversation)} messages in conversation\")\n",
    "                \n",
    "                # Make API call\n",
    "                response = bedrock.converse(\n",
    "                    modelId=MISTRAL_MODEL,\n",
    "                    messages=conversation,\n",
    "                    system=system,\n",
    "                    toolConfig={\n",
    "                        \"tools\": formatted_tools,\n",
    "                        \"toolChoice\": {\"auto\": {}}\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                # Get response data\n",
    "                output = response.get(\"output\", {})\n",
    "                message = output.get(\"message\", {})\n",
    "                content = message.get(\"content\", [])\n",
    "                stop_reason = response.get(\"stopReason\", \"none\")\n",
    "                \n",
    "                print(f\"Got response with stop reason: {stop_reason}\")\n",
    "                \n",
    "                # Check if the model wants to use tools\n",
    "                if stop_reason == \"tool_use\":\n",
    "                    print(\"Model is requesting tool use\")\n",
    "                    \n",
    "                    # Store the assistant's message with tool requests in our conversation\n",
    "                    conversation.append({\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": content\n",
    "                    })\n",
    "                    \n",
    "                    # Extract all tool use requests\n",
    "                    tool_use_requests = []\n",
    "                    for idx, item in enumerate(content):\n",
    "                        if \"toolUse\" in item:\n",
    "                            tool_use_requests.append(item[\"toolUse\"])\n",
    "                    \n",
    "                    # Log the requests we've identified\n",
    "                    print(f\"Found {len(tool_use_requests)} tool requests\")\n",
    "                    for req in tool_use_requests:\n",
    "                        print(f\"  - {req.get('name')} (ID: {req.get('toolUseId')})\")\n",
    "                    \n",
    "                    # Collect all tool results in the same order as they were requested\n",
    "                    tool_results = []\n",
    "                    \n",
    "                    for req in tool_use_requests:\n",
    "                        tool_name = req.get(\"name\")\n",
    "                        tool_input = req.get(\"input\", {})\n",
    "                        tool_id = req.get(\"toolUseId\")\n",
    "                        \n",
    "                        print(f\"Executing tool: {tool_name} with ID {tool_id}\")\n",
    "                        \n",
    "                        try:\n",
    "                            result = await self.execute_tool(tool_name, tool_input)\n",
    "                            \n",
    "                            # Convert complex result objects to string\n",
    "                            if isinstance(result, list) and len(result) > 0:\n",
    "                                if hasattr(result[0], 'text'):\n",
    "                                    result = result[0].text\n",
    "                            \n",
    "                            print(f\"Tool result: {str(result)[:100]}...\")\n",
    "                            \n",
    "                            # Add to tool results collection\n",
    "                            tool_results.append({\n",
    "                                \"toolResult\": {\n",
    "                                    \"toolUseId\": tool_id,\n",
    "                                    \"content\": [{\"text\": str(result)}]\n",
    "                                }\n",
    "                            })\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Error executing {tool_name}: {e}\")\n",
    "                            tool_results.append({\n",
    "                                \"toolResult\": {\n",
    "                                    \"toolUseId\": tool_id,\n",
    "                                    \"content\": [{\"text\": f\"Error: {str(e)}\"}],\n",
    "                                    \"status\": \"error\"\n",
    "                                }\n",
    "                            })\n",
    "                    \n",
    "                    # Add all tool results in a single message\n",
    "                    conversation.append({\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": tool_results  # List of all tool results\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"Added {len(tool_results)} tool results to conversation\")\n",
    "                    \n",
    "                    # Continue loop to make another API call with updated conversation\n",
    "                    \n",
    "                else:\n",
    "                    # Model has provided a final answer\n",
    "                    print(\"Model has provided a final answer\")\n",
    "                    \n",
    "                    # Extract text from content\n",
    "                    text_parts = []\n",
    "                    for item in content:\n",
    "                        if isinstance(item, dict) and \"text\" in item:\n",
    "                            text_parts.append(item[\"text\"])\n",
    "                    \n",
    "                    result = \"\\n\".join(text_parts)\n",
    "                    return result if result else \"No text content found in response\"\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error in process_query: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e80777a-a4fe-4d2d-9ca7-146637107737",
   "metadata": {},
   "source": [
    "## System Prompt Overview\n",
    "The system prompt for our restaurant recommender instructs Mistral Large 2 to act as a personalized recommendation assistant with the following key elements:\n",
    "\n",
    "**Available Tools**\n",
    "\n",
    "**Google Maps Tools**: For location services and restaurant data\n",
    "\n",
    "**maps_search_places**: Find restaurants by query and location\n",
    "\n",
    "**maps_place_details**: Get detailed restaurant information\n",
    "\n",
    "**maps_reverse_geocode**: Convert coordinates to addresses\n",
    "\n",
    "**Time Tools**: For restaurant availability\n",
    "\n",
    "**get_current_time**: Check current time in specific timezones\n",
    "\n",
    "**convert_time**: Convert between timezones\n",
    "\n",
    "**Memory Tools**: For personalization\n",
    "\n",
    "**open_nodes**: Direct access to user preferences\n",
    "\n",
    "**search_nodes**: Search for preference data\n",
    "\n",
    "**Recommendation Process**\n",
    "\n",
    "- Determine the user's location using reverse geocoding\n",
    "- Check local time to filter for open restaurants\n",
    "- Access user's dining preferences from memory\n",
    "- Search for restaurants matching preferences\n",
    "- Get detailed information about promising options\n",
    "  \n",
    "Memory Access Strategy\n",
    "\n",
    "**Primary**: Use open_nodes with [\"default_user\"] for direct preference access\n",
    "\n",
    "**Fallback**: Use search_nodes if direct access fails\n",
    "\n",
    "**Relations**: Check the user's past dining experiences through entity relationships\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f74f8d5-e9ea-404f-8637-35be7de96dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "    You are a restaurant recommendation assistant with access to the following MCP tools:\n",
    "\n",
    "    Google Maps Tools:\n",
    "    - maps_search_places: Search for restaurants by query, location, and radius\n",
    "    - maps_place_details: Get detailed information about a specific place\n",
    "    - maps_reverse_geocode: Convert coordinates to address information\n",
    "    \n",
    "    Time Tools:\n",
    "    - get_current_time: Get current time in a specific timezone\n",
    "    - convert_time: Convert time between timezones\n",
    "    \n",
    "    Memory Tools:\n",
    "    - search_nodes: Access user preferences and dining history\n",
    "    \n",
    "    Process for making recommendations:\n",
    "    1. Use maps_reverse_geocode to understand the area\n",
    "    2. Use get_current_time to check local time and determine which restaurants are likely to be open\n",
    "    3. Use search_nodes to get user preferences\n",
    "    4. Use maps_search_places to find restaurants\n",
    "    5. Use maps_place_details to get detailed information about promising options\n",
    "    \n",
    "    When recommending:\n",
    "    - Always begin by using open_nodes with names [\"default_user\"] to directly access user preferences\n",
    "    - If that fails, fall back to search_nodes with queries like \"restaurant preferences\" or \"food preferences\"\n",
    "    - Consider user's previous dining experiences by checking relations where \"default_user\" is the source\n",
    "    \n",
    "    Format your response conversationally, explaining why each recommendation would suit the user.\n",
    "    If you need any information, use the appropriate tool to get it.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201bf40c-0458-418f-8e1f-d7fa7adee94f",
   "metadata": {},
   "source": [
    "## Working with User Preferences\n",
    "\n",
    "User preferences are stored in the Memory MCP server as entities with observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a2234b-f97f-4649-b2e1-5a6a27f00123",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def setup_test_memory(recommender):\n",
    "    \"\"\"Set up test user preferences in Memory MCP\"\"\"\n",
    "    \n",
    "    # Create user entity with preferences\n",
    "    await recommender.sessions[\"memory\"].call_tool(\n",
    "        \"create_entities\",\n",
    "        {\n",
    "            \"entities\": [\n",
    "                {\n",
    "                    \"name\": \"default_user\",\n",
    "                    \"entityType\": \"person\",\n",
    "                    \"observations\": [\n",
    "                        \"Prefers Japanese and sushi restaurants\",\n",
    "                        \"Typically spends $$ on meals\",\n",
    "                        \"Prefers restaurants within 2 miles\",\n",
    "                        \"Enjoys trying new ramen places\",\n",
    "                        \"Vegetarian options are important\",\n",
    "                        \"restaurant preferences\"\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"Sushi_Delight\",\n",
    "                    \"entityType\": \"restaurant\",\n",
    "                    \"observations\": [\n",
    "                        \"Japanese restaurant\",\n",
    "                        \"Known for fresh sushi\",\n",
    "                        \"Price level: $$\",\n",
    "                        \"Last visited: 2024-02-15\",\n",
    "                        \"restaurants\"\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"Ramen_House\",\n",
    "                    \"entityType\": \"restaurant\",\n",
    "                    \"observations\": [\n",
    "                        \"Japanese ramen restaurant\",\n",
    "                        \"Price level: $$\",\n",
    "                        \"Known for vegetarian options\",\n",
    "                        \"Last visited: 2024-03-01\"\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Create relations (previous experiences)\n",
    "    await recommender.sessions[\"memory\"].call_tool(\n",
    "        \"create_relations\",\n",
    "        {\n",
    "            \"relations\": [\n",
    "                {\n",
    "                    \"from\": \"default_user\",\n",
    "                    \"to\": \"Sushi_Delight\",\n",
    "                    \"relationType\": \"enjoyed_dining_at\"\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"default_user\",\n",
    "                    \"to\": \"Ramen_House\",\n",
    "                    \"relationType\": \"frequently_visits\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c984008c-07af-442f-b8ca-6359f59ecb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    try:\n",
    "        print(\"1. Initializing recommender...\")\n",
    "        recommender = RestaurantRecommender()\n",
    "        \n",
    "        print(\"2. Connecting to MCP servers...\")\n",
    "        await recommender.connect_to_mcp_servers()\n",
    "        \n",
    "        print(\"3. Setting up test memory...\")\n",
    "        await setup_test_memory(recommender)\n",
    "        \n",
    "        print(\"4. Getting available tools...\")\n",
    "        tools = await recommender.get_available_tools()\n",
    "        print(f\"Available tools: {tools}\")\n",
    "        \n",
    "        # Example coordinates - feel free to set your own!\n",
    "        lat, lng = 40.7484, -73.985428\n",
    "        \n",
    "        # Example query\n",
    "        query = \"I'm looking for dinner recommendations. Something similar to places I've enjoyed before.\"\n",
    "        \n",
    "        print(\"5. Processing query...\")\n",
    "        response = await recommender.process_query(query, lat, lng)\n",
    "        print(\"6. Got response!\")\n",
    "        print(response)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f4effd-643b-4925-bb80-b0a4e009f750",
   "metadata": {},
   "outputs": [],
   "source": [
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
