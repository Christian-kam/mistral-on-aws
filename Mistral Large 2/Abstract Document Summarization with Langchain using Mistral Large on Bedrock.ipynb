{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfda1339-68fe-416d-b3b8-6349872524b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Abstract Document Summarization with Langchain using Mistral Large on Bedrock\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a353a3bf-98e8-4eb2-a24d-05dd210e6b3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overview\n",
    "This notebook is meant to demonstrate using the [Mistral models](https://docs.mistral.ai/deployment/cloud/aws/) on Amazon Bedrock for abstract document summarization tasks. Although all the Mistral models have relatively large context window sizes, when working with multiple large documents, there are several challenges that can arise. One of the main challenges is that the input text might exceed the model's context length. This limitation can lead to incomplete or inaccurate responses, as the model may not have access to all the relevant information within the document. Another challenge is that language models can sometimes hallucinate or generate factually incorrect responses when dealing with very long documents. This can happen because the model may lose track of the overall context or make incorrect inferences based on partial information. Additionally, processing large documents can lead to out-of-memory errors, especially on resource-constrained systems or when working with large language models that have high memory requirements.\n",
    "\n",
    "To address these challenges, this notebook will go through various summarization strategies that will use [LangChain](https://python.langchain.com/docs/get_started/introduction.html), a popular framework for developing applications powered by large language models (LLMs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3664e61-3f39-4229-9cf6-26ee090a8608",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Mistral Model Selection\n",
    "\n",
    "Today, there are four Mistral models available on Amazon Bedrock. As mentioned in the title, this notebook will primarily use the **Mistral Large** model.\n",
    "\n",
    "\n",
    "### 1. Mistral 7B Instruct\n",
    "\n",
    "- **Description:** A 7B dense Transformer model, fast-deployed and easily customizable. Small yet powerful for a variety of use cases.\n",
    "- **Supported Use Cases:** Text summarization, structuration, question answering, and code completion\n",
    "- **Bedrock Model ID:** \"mistral.mistral-7b-instruct-v0:2\"\n",
    "\n",
    "### 2. Mixtral 8X7B Instruct\n",
    "\n",
    "- **Description:** A 7B sparse Mixture-of-Experts model with stronger capabilities than Mistral 7B. Utilizes 12B active parameters out of 45B total.\n",
    "- **Supported Use Cases:** Text summarization, structuration, question answering, and code completion\n",
    "- **Bedrock Model ID:** \"mistral.mixtral-8x7b-instruct-v0:1\"\n",
    "\n",
    "### 3. Mistral Small\n",
    "\n",
    "- **Description:** - Suitable for simple tasks that one can do in bulk\n",
    "- **Supported Use Cases:** Classification, Customer Support, or Text Generation\n",
    "- **Bedrock Model ID:** \"mistral.mistral-small-2402-v1:0\"\n",
    "\n",
    "### 4. Mistral Large\n",
    "\n",
    "- **Description:** A cutting-edge text generation model with top-tier reasoning capabilities. It can be used for complex multilingual reasoning tasks, including text understanding, transformation, and code generation.\n",
    "- **Max Tokens:** 8,196\n",
    "- **Context Window:** 32K\n",
    "- **Languages:** English, French, German, Spanish, Italian\n",
    "- **Supported Use Cases:** Synthetic Text Generation, Code Generation, RAG, or Agents\n",
    "- **Bedrock Model ID:** \"mistral.mistral-large-2402-v1:0\"\n",
    "\n",
    "#### Performance and Cost Information\n",
    "\n",
    "The table below shows Mistral Large's performance on the Massive Multitask Language Understanding (MMLU) benchmark and its on-demand pricing on Amazon Bedrock.\n",
    "\n",
    "| Model           | MMLU Score | Price per 1,000 Input Tokens | Price per 1,000 Output Tokens |\n",
    "|-----------------|------------|------------------------------|-------------------------------|\n",
    "| Mistral Large | 81.2%      | \\$0.008                   | \\$0.024                     |\n",
    "\n",
    "For more information, refer to the following links:\n",
    "\n",
    "1. [Mistral Model Selection Guide](https://docs.mistral.ai/guides/model-selection/)\n",
    "2. [Amazon Bedrock Pricing Page](https://aws.amazon.com/bedrock/pricing/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ce6389-42ff-405e-8c3b-1855c9db22cf",
   "metadata": {},
   "source": [
    "### Local Setup (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed96c9c-58c0-49a8-a032-8b547aa03419",
   "metadata": {
    "tags": []
   },
   "source": [
    "For a local server, follow these steps to execute this jupyter notebook:\n",
    "\n",
    "1. **Configure AWS CLI**: Configure [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) with your AWS credentials. Run `aws configure` and enter your AWS Access Key ID, AWS Secret Access Key, AWS Region, and default output format.\n",
    "\n",
    "2. **Install required libraries**: Install the necessary Python libraries for working with SageMaker, such as [sagemaker](https://github.com/aws/sagemaker-python-sdk/), [boto3](https://github.com/boto/boto3), and others. You can use a Python environment manager like [conda](https://docs.conda.io/en/latest/) or [virtualenv](https://virtualenv.pypa.io/en/latest/) to manage your Python packages in your preferred IDE (e.g. [Visual Studio Code](https://code.visualstudio.com/)).\n",
    "\n",
    "3. **Create an IAM role for SageMaker**: Create an AWS Identity and Access Management (IAM) role that grants your user [SageMaker permissions](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html). \n",
    "\n",
    "By following these steps, you can set up a local Jupyter Notebook environment capable of deploying machine learning models on Amazon SageMaker using the appropriate IAM role for granting the necessary permissions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145a29e7-6312-4ac6-a338-a33cbd83b084",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f67cb8-1a0c-416c-a8c8-66814a52b72c",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "1. Create an Amazon SageMaker Notebook Instance - [Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html)\n",
    "    - For Notebook Instance type, choose ml.t3.medium.\n",
    "2. For Select Kernel, choose [conda_pytorch_p310](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-prepare.html).\n",
    "3. Install the required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6e4415-8fbb-46ac-92e2-3ebcc4888153",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "Before we start building the agentic workflow, we'll first install some libraries:\n",
    "\n",
    "+ AWS Python SDKs [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) to be able to submit API calls to [Amazon Bedrock](https://aws.amazon.com/bedrock/).\n",
    "+ [LangChain](https://python.langchain.com/v0.1/docs/get_started/introduction/) is a framework that provides off the shelf components to make it easier to build applications with large language models. It is supported in multiple programming languages, such as Python, JavaScript, Java and Go. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25b1ce28-362c-44a7-bbde-fec066fea4ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "langchain==0.1.14\n",
    "boto3==1.34.58\n",
    "botocore==1.34.101\n",
    "sqlalchemy==2.0.29\n",
    "pypdf==4.1.0\n",
    "langchain-aws==0.1.6\n",
    "transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99ad3619-b8c3-49d3-9175-be3013c079e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25b8017-48a6-4369-a556-fd0ecdbd80f8",
   "metadata": {},
   "source": [
    "#### Restart the kernel with the updated packages that are installed through the dependencies above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c26578-567b-4629-a081-698ff82533fb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c109e15-ad6f-4a53-a077-5eae296c67ec",
   "metadata": {},
   "source": [
    "\n",
    "## Initiate the Bedrock Client\n",
    "\n",
    "Import the necessary libraries, along with langchain for bedrock model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ee536a3-1cb4-4bab-a8a0-6e41023c9cfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from boto3 import client\n",
    "from botocore.config import Config\n",
    "import json\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "import numpy as np\n",
    "from pypdf import PdfReader\n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac5aff5b-4b38-48ea-b5ab-5bebc44b1d20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = Config(read_timeout=2000)\n",
    "\n",
    "bedrock = boto3.client(service_name='bedrock-runtime', \n",
    "                       region_name='us-east-1',\n",
    "                       config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876864d8-d99b-45a3-b7e5-c8824ad8e9f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "<b>NOTE:</b> Ensure that you have access to the Mistral model you wish to use through Bedrock.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61231ed9-0cac-4fee-b932-059fc253bbf3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configure LangChain with Boto3\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "With LangChain, you can access Bedrock once you pass the boto3 session information to LangChain. Below, we also specify Mistral Large in `model_id` and pass Mistral's inference parameters as desired in `model_kwargs`.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Supported parameters\n",
    "\n",
    "The Mistral AI models have the following inference parameters.\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "    \"prompt\": string,\n",
    "    \"max_tokens\" : int,\n",
    "    \"stop\" : [string],    \n",
    "    \"temperature\": float,\n",
    "    \"top_p\": float,\n",
    "    \"top_k\": int\n",
    "}\n",
    "```\n",
    "\n",
    "The Mistral AI models have the following inference parameters:\n",
    "\n",
    "- **Temperature** - Tunes the degree of randomness in generation. Lower temperatures mean less random generations.\n",
    "- **Top P** - If set to float less than 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.\n",
    "- **Top K** - Can be used to reduce repetitiveness of generated tokens. The higher the value, the stronger a penalty is applied to previously present tokens, proportional to how many times they have already appeared in the prompt or prior generation.\n",
    "- **Maximum Length** - Maximum number of tokens to generate. Responses are not guaranteed to fill up to the maximum desired length.\n",
    "- **Stop sequences** - Up to four sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a841ed0-eab4-4921-87b7-a2d07b961405",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Set the desired mistral model as the default model\n",
    "instruct_mistral7b_id = \"mistral.mistral-7b-instruct-v0:2\"\n",
    "instruct_mixtral8x7b_id = \"mistral.mixtral-8x7b-instruct-v0:1\"\n",
    "mistral_large_2402_id = \"mistral.mistral-large-2402-v1:0\"\n",
    "mistral_small = \"mistral.mistral-small-2402-v1:0\"\n",
    "\n",
    "DEFAULT_MODEL = mistral_large_2402_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d7901c8-7a58-4c98-bdeb-db0dc4d04dd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatBedrock(\n",
    "    model_id=DEFAULT_MODEL,\n",
    "    model_kwargs={\n",
    "        \"max_tokens\": 8192,  ## MAXIMUM NUMBER OF TOKENS for Mistral Large\n",
    "        \"temperature\": 0.5,\n",
    "        \"top_p\": 1\n",
    "    },\n",
    "    client=bedrock,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38569ac6-5fad-4162-b0c0-c624e742ecb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Hello! It's a pleasure to meet you. I'm here to provide information, answer questions, or just chat about a wide range of topics. How can I assist you today?\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialize conversation chain with Mistral Large on Bedrock\n",
    "conversation = ConversationChain(\n",
    "    # We set verbose to false to suppress the printing of logs during the execution of the conversation chain. This can be set to true when you're debugging your conversation chain or trying to understand how it's working under the hood.\n",
    "    llm=llm, verbose=False, memory=ConversationBufferMemory() \n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a8307f-d16d-427a-b9e7-d7deac5cdb05",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816dab8e-af09-456d-b6f1-4b11d33b5644",
   "metadata": {},
   "source": [
    "## Document Processing Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e23f76d-0a55-490e-8060-a6e821b8eb99",
   "metadata": {},
   "source": [
    "In this example, to demonstrate summarization, we will be using two documents that are both whitepapers from AWS. \n",
    "\n",
    "> The first document is a [whitepaper](https://docs.aws.amazon.com/whitepapers/latest/architecting-hipaa-security-and-compliance-on-aws/architecting-hipaa-security-and-compliance-on-aws.pdf) on architecting HIIPA compliant workloads on AWS.\n",
    "\n",
    "> The second document is a [whitepaper](https://docs.aws.amazon.com/whitepapers/latest/containers-on-aws/containers-on-aws.pdf) about containers on AWS. \n",
    "\n",
    "Let's first download these files to build our document store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c71e6ec-8500-404c-9dc3-d81b338fcd4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p ./data\n",
    "\n",
    "urls = [\n",
    "    'https://docs.aws.amazon.com/whitepapers/latest/architecting-hipaa-security-and-compliance-on-aws/architecting-hipaa-security-and-compliance-on-aws.pdf',\n",
    "    'https://docs.aws.amazon.com/whitepapers/latest/containers-on-aws/containers-on-aws.pdf'\n",
    "]\n",
    "\n",
    "filenames = [\n",
    "    'AWS-security-whitepaper.pdf',\n",
    "    'AWS-containers-whitepaper.pdf'\n",
    "]\n",
    "\n",
    "metadata = [\n",
    "    dict(year=2023, source=filenames[0]),\n",
    "    dict(year=2023, source=filenames[1])\n",
    "]\n",
    "\n",
    "data_root = \"./data/\"\n",
    "\n",
    "for idx, url in enumerate(urls):\n",
    "    file_path = data_root + filenames[idx]\n",
    "    urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2214de-07e2-4958-bf06-1339048abceb",
   "metadata": {},
   "source": [
    "After downloading we can load the documents with the help of `DirectoryLoader` from `PyPDF` available under LangChain and splitting them into smaller chunks.\n",
    "\n",
    "Note: For the sake of this use-case we are creating chunks of roughly 4000 characters with an overlap of 100 characters using `RecursiveCharacterTextSplitter`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595d8742-23aa-4814-8e15-1c720ff9234b",
   "metadata": {},
   "source": [
    "#### HIPAA Compliance document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fd92e7-cf45-49d3-aa4a-774b70f3e3e0",
   "metadata": {},
   "source": [
    "In this section, we will load the HIPAA compliance document with `PyPDFLoader`, append document fragments with the metadata, and use LangChain's `RecursiveCharacterTextSplitter` to split the documents in `hipaa_documents` list into smaller text chunks using the `split_documents` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "930338ac-1950-4e1b-8388-8972ca92e4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='AWS Whitepaper\\nArchitecting for HIPAA Security and \\nCompliance on Amazon Web Services\\nCopyright © 2024 Amazon Web Services, Inc. and/or its aﬃliates. All rights reserved.' metadata={'year': 2023, 'source': 'AWS-security-whitepaper.pdf'}\n",
      "\n",
      "Number of documents chunked and created from the HIPAA Security document: 152\n"
     ]
    }
   ],
   "source": [
    "#document 1 (HIPAA COMPLIANCE ON AWS)\n",
    "hipaa_documents = []\n",
    "\n",
    "# Load only the first file\n",
    "hipaa_file = filenames[0]\n",
    "hipaa_loader = PyPDFLoader(data_root + hipaa_file)\n",
    "hipaa_document = hipaa_loader.load()\n",
    "\n",
    "for idx, hipaa_document_fragment in enumerate(hipaa_document):\n",
    "    hipaa_document_fragment.metadata = metadata[0] if metadata else {}\n",
    "    hipaa_documents.append(hipaa_document_fragment)\n",
    "    \n",
    "#chunking\n",
    "hipaa_doc_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a  small chunk size, just to show.\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "hipaa_docs = hipaa_doc_text_splitter.split_documents(hipaa_documents)\n",
    "print(hipaa_docs[0])\n",
    "\n",
    "#chunked doc count\n",
    "hipaa_chunked_count = len(hipaa_docs)\n",
    "print(\n",
    "    f\"\\nNumber of documents chunked and created from the HIPAA Security document: {hipaa_chunked_count}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d759ea-f15f-4acd-a60d-15abc519135f",
   "metadata": {},
   "source": [
    "#### Containers on AWS Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eb32bc-e7b4-488d-bdff-a52b6dc42e45",
   "metadata": {},
   "source": [
    "In this section, we will load the Containers on AWS document with `PyPDFLoader`, append document fragments with the metadata, and use LangChain's `RecursiveCharacterTextSplitter` to split the documents in `container_documents` list into smaller text chunks using the `split_documents` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12233e31-3079-4128-8ff2-0e79d07e2461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=\"Containers on AWS AWS Whitepaper\\nContainers on AWS: AWS Whitepaper\\nCopyright © 2024 Amazon Web Services, Inc. and/or its aﬃliates. All rights reserved.\\nAmazon's trademarks and trade dress may not be used in connection with any product or service \\nthat is not Amazon's, in any manner that is likely to cause confusion among customers, or in any \\nmanner that disparages or discredits Amazon. All other trademarks not owned by Amazon are \\nthe property of their respective owners, who may or may not be aﬃliated with, connected to, or \\nsponsored by Amazon.\" metadata={'year': 2023, 'source': 'AWS-security-whitepaper.pdf'}\n",
      "\n",
      "Number of documents chunked and created from the original: 57\n"
     ]
    }
   ],
   "source": [
    "#document 2 (Containers on AWS)\n",
    "container_documents = []\n",
    "\n",
    "# Load only the second file\n",
    "container_file = filenames[1]\n",
    "container_loader = PyPDFLoader(data_root + container_file)\n",
    "container_document = container_loader.load()\n",
    "\n",
    "for idx, container_document_fragment in enumerate(container_document):\n",
    "    container_document_fragment.metadata = metadata[0] if metadata else {}\n",
    "    container_documents.append(container_document_fragment)\n",
    "    \n",
    "#chunking\n",
    "container_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a small chunk size, just to show.\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "container_docs = container_text_splitter.split_documents(container_documents)\n",
    "print(container_docs[1])\n",
    "\n",
    "#chunked doc count\n",
    "container_chunked_count = len(container_docs)\n",
    "print(\n",
    "    f\"\\nNumber of documents chunked and created from the original: {container_chunked_count}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983a712e-99d8-403d-85a2-cfa048ceaabf",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d90c42-7a08-447d-8143-10571c411c90",
   "metadata": {},
   "source": [
    "## Summarizing Long Documents with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34410f8c-6d6e-4163-9428-7481abd02ef6",
   "metadata": {},
   "source": [
    "In the following sections, we will go over three different summarization techniques with LangChain:\n",
    "    \n",
    " #####   1. Stuff\n",
    " #####   2. Map Reduce\n",
    " #####   3. Refine\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c27632-85ba-46cf-bd43-763e328a8001",
   "metadata": {},
   "source": [
    "### 1. Stuff with load_summarize_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840b9890-02bf-4927-8fbb-fea029ddc24c",
   "metadata": {},
   "source": [
    "Stuffing is the simplest method to pass data to a language model. It \"stuffs\" text into the prompt as context in a way that all of the relevant information can be processed by the model to get what you want.\n",
    "\n",
    "In LangChain, you can use `StuffDocumentsChain` as part of the `load_summarize_chain` method. What you need to do is set `stuff` as the `chain_type` of your chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4175ddb7-9a5a-4c9f-8524-0f36a98c8e18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stuff_summary_chain = load_summarize_chain(llm=llm, chain_type=\"stuff\", verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70c9859-f01f-4b60-8139-a9b3dd2d0fb2",
   "metadata": {},
   "source": [
    "Next, let's take a look at the Prompt template used by the Stuff summarize chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4690f8f-4883-4291-829c-030374ac3b4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stuff_summary_chain.llm_chain.prompt.template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03afeaa3-21ff-4dd0-b9d5-d691e86e2a77",
   "metadata": {},
   "source": [
    "Here, we see that by default, the Prompt template for `llm_chain` has been set to: 'Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:'\n",
    "\n",
    "This can be altered by instantiating using `from_template` with LangChain to set a new prompt. We can do that below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41262dc2-3d6e-4296-9954-49edb989ec7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stuff_prompt = PromptTemplate.from_template('Write a detailed and complete summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nDETAILED SUMMARY:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "469f2796-0dd6-4fc5-9101-fbcdc1e782d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stuff_summary_chain.llm_chain.prompt.template = stuff_prompt.template #set new prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cf5e04-8189-4294-8927-5ad4280b82ba",
   "metadata": {},
   "source": [
    "Now that we have set the new prompt template, let us first try generating a summary of the **Containers on AWS** whitepaper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4232059e-ef40-49b1-9290-14f38f5b1522",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    stuff_container_summary = stuff_summary_chain.invoke(container_docs) \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7293ada5-a3ed-4969-8f37-64ab3e1b7f7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AWS Whitepaper \"Containers on AWS\" provides guidance and options for running containers on AWS. Containers provide a way to develop, ship, and run applications in an isolated environment. AWS is a natural complement to containers and offers a wide range of scalable orchestration and infrastructure services, upon which containers can be deployed. This paper provides information about container orchestration and compute options such as AWS App Runner, Amazon Elastic Container Service (Amazon ECS), Amazon Elastic Kubernetes Service (Amazon EKS), and AWS Fargate and key considerations for container workloads on AWS.\n",
      "\n",
      "The paper starts with an abstract and introduction, which provides an overview of the benefits of using containers and the challenges they solve. It then discusses the benefits of using containers, including speed, consistency, density and resource efficiency, and portability.\n",
      "\n",
      "The paper then discusses container orchestration on AWS, including key considerations such as container runtime, container-enabled AMIs, compute options, scheduling, container repositories, observability, storage, networking, security, build and deploy automation, infrastructure as code and platform automation, and scaling.\n",
      "\n",
      "The paper concludes by discussing the AWS Well-Architected Framework and how it can help users understand the pros and cons of the decisions they make when building systems in the cloud. It also provides a list of contributors, further reading, document history, notices, and an AWS glossary.\n",
      "\n",
      "In summary, the AWS Whitepaper \"Containers on AWS\" provides a comprehensive overview of the options and considerations for running containers on AWS, including container orchestration and compute options, key considerations for container workloads, and the benefits of using containers. It is a useful resource for anyone looking to deploy and manage containers on AWS.\n"
     ]
    }
   ],
   "source": [
    "print(stuff_container_summary['output_text'].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6b054f-e936-4595-825d-ebab6019dfc4",
   "metadata": {},
   "source": [
    "From the cell ouput above, we can see that since Stuffing only requires a single call to the LLM, it can be faster than other methods that require multiple calls. When summarizing text, the model has access to all the data at once, which can result in a fast response for the summary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a33dffb-c033-49a2-bf2f-6af050d01160",
   "metadata": {},
   "source": [
    "Next, let us use the **HIPAA and Security Compliance** on AWS whitepaper to see how the model deals with summarization using the `StuffDocumentChain` when presented with a longer document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4b9f80d-bc8b-4850-bcb8-dd0a76df371d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error raised by bedrock service: An error occurred (ValidationException) when calling the InvokeModel operation: This model's maximum context length is 32768 tokens. Please reduce the length of the prompt\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    stuff_hipaa_summary = stuff_summary_chain.invoke(hipaa_docs) # (ValidationException error) prompt over 32k window length / number of tokens exceeds window size \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58cbe28-64c6-4302-8c42-deba21fa516f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Notes:\n",
    "In the output for the above cell, we see that an error is raised due to the prompt far exceeding the model's maximum context length. Since stuffing summarizes text by feeding the entire document to a large language model (LLM) in a single call, it is difficult to process long documents. The Mistral models have a context length of 32k tokens, which is the maximum number of tokens that can be processed in a single call. If the document is longer than the context length, stuffing will not work. Also the stuffing method is not suitable for summarizing large documents, as it can be slow and may not produce a good summary.\n",
    "\n",
    "Let's explore a couple chunk-wise summarization techniques with [LangChain](https://python.langchain.com/docs/get_started/introduction.html) to be able to mitigate the restrictions of your large documents not fitting into the context window of the model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b1252-171a-43c7-a184-41fc47e1b836",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Map Reduce with load_summarize_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccef8232-dc84-44c8-8124-4f4ff7e69f64",
   "metadata": {},
   "source": [
    "The `Map_Reduce` method involves summarizing each document individually (map step) and then combining these summaries into a final summary (reduce step). This approach is more scalable and can handle larger volumes of text. The map reduce technique is designed for summarizing large documents that exceed the token limit of the language model. It involves dividing the document into chunks, generating summaries for each chunk, and then combining these summaries to create a final summary. This method is efficient for handling large files and significantly reduces processing time.\n",
    "\n",
    "In LangChain, you can use `MapReduceDocumentsChain` as part of the `load_summarize_chain method`. What you need to do is set `map_reduce` as the `chain_type` of your chain.\n",
    "\n",
    "In this architecture:\n",
    "\n",
    "1. A large document (or a giant file appending small ones) is loaded\n",
    "2. Langchain utility is used to split it into multiple smaller chunks (chunking)\n",
    "3. Model generates individual summaries for all document chunks in parallel\n",
    "4. Reduce all these summaries to a condensed final summary\n",
    "---\n",
    "\n",
    "![map-reduce](imgs/mapreduce.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "103e086a-d021-4357-901b-5670731087e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain, it then combines and iteratively reduces the mapped document\n",
    "map_reduce_summary_chain = load_summarize_chain(llm=llm, chain_type=\"map_reduce\", verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae04456-08d3-4f3e-8491-e37d736f18db",
   "metadata": {
    "tags": []
   },
   "source": [
    "The `ReduceDocumentsChain` handles taking the document mapping results and reducing them into a single output. It wraps a generic `CombineDocumentsChain` (like `StuffDocumentsChain`) but adds the ability to collapse documents before passing it to the `CombineDocumentsChain` if their cumulative size exceeds token_max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e8de7ca-64d6-4720-9278-a477638b4b22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiation using from_template (recommended)\n",
    "#sets the prompt template for the summaries generated for all the individual document chunks.\n",
    "initial_map_prompt = PromptTemplate.from_template(\"\"\"\n",
    "                      Write a summary of this chunk of text that includes the main points and any important details.\n",
    "                      {text}\n",
    "                      \"\"\")\n",
    "\n",
    "map_reduce_summary_chain.llm_chain.prompt.template = initial_map_prompt.template\n",
    "\n",
    "#sets the prompt template for generating a cumulative summary of all the document chunks for reduce documents chain.\n",
    "reduce_documents_prompt= PromptTemplate.from_template(\"\"\"\n",
    "                      Write a detailed summary of the following text delimited by triple backquotes.\n",
    "                      Return your response in bullet points which covers the key points of the text.\n",
    "                      ```{text}```\n",
    "                      BULLET POINT SUMMARY:\n",
    "                      \"\"\")\n",
    "\n",
    "map_reduce_summary_chain.reduce_documents_chain.combine_documents_chain.llm_chain.prompt.template = reduce_documents_prompt.template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b9794d-7ff2-496c-8998-2801b1846a5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here, we perform summarization on the **HIPAA and Security Compliance** document with `Map-Reduce`. Since this is document is quite large, it can take a while to run.\n",
    "In order to see how Map_Reduce works, let us generate a summary of a subset of the document chunks **(50 to 70)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c76502a-13c8-4501-9544-d957ec56bdd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5740 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "#this cell might take 5-10 minutes to run\n",
    "try:\n",
    "    map_reduce_summary = map_reduce_summary_chain.invoke(hipaa_docs[50:71])  \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "334fad6e-1ded-4cb3-8da2-1b6bd534b115",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The text discusses various Amazon Web Services (AWS) features that support HIPAA (Health Insurance Portability and Accountability Act) security and compliance.\n",
      "- AWS Elastic File System (EFS) offers two methods for encrypting Protected Health Information (PHI) at rest: enabling encryption during file system creation and encrypting data before placing it on EFS.\n",
      "- Encryption of PHI during transit on Amazon EFS is provided by Transport Layer Security (TLS).\n",
      "- Use of PHI in file or folder names is discouraged.\n",
      "- Amazon Elastic Kubernetes Service (Amazon EKS) simplifies running Kubernetes on AWS.\n",
      "- Amazon ElastiCache for Redis, an in-memory data structure service, can store PHI under certain conditions and offers encryption at rest and in transit, Redis AUTH token for command authentication, and requires customers to keep their Redis clusters updated with the latest 'Security' type service updates.\n",
      "- Amazon EventBridge, a serverless event bus, encrypts data using 256-bit Advanced Encryption Standard (AES-256) under an AWS owned CMK.\n",
      "- AWS resources that store, process, or transmit PHI should be configured in accordance with best practices.\n",
      "- Amazon FSx, a fully-managed service that provides high-performance file systems, supports encryption of data in transit and at rest.\n",
      "- Amazon GuardDuty is a managed threat detection service that continuously monitors for malicious or unauthorized behavior.\n",
      "- Amazon HealthLake allows customers in the healthcare and life sciences industries to store, transform, query, and analyze health data at petabyte scale and encrypts data both in transit and at rest.\n",
      "- Amazon Inspector is an automated security assessment service that assesses applications for vulnerabilities or deviations from best practices.\n",
      "- Amazon Managed Service for Apache Flink allows customers to write SQL code that continuously reads, processes, and stores data in near real-time.\n",
      "- Amazon Kinesis Video Streams is a fully managed AWS service that allows customers to stream live video from devices to the AWS Cloud and build applications for real-time video processing.\n",
      "- Amazon Lex is a service for building conversational interfaces for applications using voice and text.\n",
      "- Amazon MSK (Managed Streaming for Kafka) and Amazon MQ offer encryption features for data at rest and data in-transit.\n",
      "- Amazon Neptune, a fast, reliable, fully managed graph database service, now allows for the retention of Personal Health Information (PHI) in an encrypted instance.\n",
      "- AWS Network Firewall is a managed firewall service that simplifies the deployment of essential network protections for all Amazon Virtual Private Cloud (Amazon VPC) and encrypts all data at rest and in transit between component AWS services.\n"
     ]
    }
   ],
   "source": [
    "print(map_reduce_summary['output_text'].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb754c4-4963-4d59-a816-cfa3363c7432",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "With `Map_Reduce`, the model is able to summarize a large document by overcoming the context limit of Stuffing method with parallel processing. \n",
    "However, it requires multiple calls to the model and potentially loses context between individual summaries of the chunks. To deal with this challenge, let us try another method that performs chunk-wise summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0c6d9a-7b92-44fc-88c1-aaba42176117",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113a9d41-399a-4e9b-928e-97afb4bc68f8",
   "metadata": {},
   "source": [
    "### 3. Refine with load_summarize_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391f8a89-f7e2-411f-b898-a454daf05a29",
   "metadata": {},
   "source": [
    "The `Refine` method is a technique that allows us to recursively summarize our input data. It iteratively updates its answer by looping over the input documents. This method is useful for refining a summary based on new context.`Refine` is a simpler alternative to `Map_Reduce`. It involves generating a summary for the first chunk, combining it with the second chunk, generating another summary, and continuing this process until a final summary is achieved. This method is suitable for large documents but requires less complexity compared to `Map_Reduce`.\n",
    "\n",
    "In this architecture:\n",
    "\n",
    "1. A large document (or a giant file appending small ones) is loaded\n",
    "2. Langchain utility is used to split it into multiple smaller chunks (chunking)\n",
    "3. First chunk is sent to the model; Model returns the corresponding summary\n",
    "4. Langchain gets next chunk and appends it to the returned summary and sends the combined text as a new request to the model; the process repeats until all chunks are processed\n",
    "5. In the end, you have final summary that has been recursively updated using all the document chunks\n",
    "\n",
    "---\n",
    "\n",
    "![map-reduce](imgs/refine.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8df1a4d-6ab0-4a2a-bed4-228973c532ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run an initial prompt on a small chunk of data to generate a summary. Then, for each subsequent document, the output from the previous document is passed in along with the new document, and the LLM is asked to refine the output based on the new document.\n",
    "refine_summary_chain = load_summarize_chain(llm=llm, chain_type=\"refine\", verbose=False)\n",
    "refine_summary_chain_french = load_summarize_chain(llm=llm, chain_type=\"refine\", verbose=False) #refine summary chain for summarization in french"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf21cbc-1b13-4341-b00a-300b579ddcfb",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here, we perform summarization on the **HIPAA and Security Compliance** document with `Refine`. Since this is document is quite large, it can take a while to run.\n",
    "In order to see how Refine works, let us generate a summary of a subset of the document chunks **(50 to 70)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f02e05ef-8a19-415c-ac84-41bac282f1f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#initial llm chain prompt template\n",
    "initial_refine_prompt = PromptTemplate.from_template(\"\"\"\n",
    "                      Write a summary of this chunk of text that includes the main points and any important details.\n",
    "                      {text}\n",
    "                      \"\"\")\n",
    "\n",
    "refine_summary_chain.initial_llm_chain.prompt.template = initial_refine_prompt.template\n",
    "\n",
    "#refine llm chain prompt template\n",
    "refine_documents_prompt= PromptTemplate.from_template(\"Your job is to produce a final summary.\\nWe have provided an existing summary up to a certain point: {existing_answer}\\nWe have the opportunity to refine the existing summary (only if needed) with some more context below.\\n------------\\n{text}\\n------------\\nGiven the new context, refine the original summary.\\nIf the context isn't useful, return the original summary.\")\n",
    "\n",
    "refine_summary_chain.refine_llm_chain.prompt.template = refine_documents_prompt.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de58d720-bccb-4bee-b836-020d6ddf56ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#this cell might take 5-10 minutes to run\n",
    "try:\n",
    "    refine_summary = refine_summary_chain.invoke(hipaa_docs[50:71])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0584649c-fd50-4527-a04f-8c4f1a4d25ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text discusses the measures to ensure HIPAA security and compliance on Amazon Web Services (AWS), focusing on various services including Amazon Elastic File System (EFS), Amazon Elastic Kubernetes Service (Amazon EKS), Amazon ElastiCache for Redis, Amazon OpenSearch Service, Amazon EMR, Amazon EventBridge, Amazon FSx, Amazon HealthLake, Amazon Inspector, Amazon Managed Service for Apache Flink, Amazon Kinesis Video Streams, Amazon Lex, Amazon Managed Streaming for Apache Kafka (Amazon MSK), and Amazon MQ.\n",
      "\n",
      "For EFS and Amazon FSx, two methods to encrypt Protected Health Information (PHI) at rest are outlined. The first method involves enabling encryption during the creation of a new file system, ensuring all data is encrypted using AES-256 encryption and AWS Key Management Service (KMS)-managed keys. The second method requires customers to encrypt data before placing it on EFS or FSx. The text advises against using PHI in file or folder names and suggests enabling Transport Layer Security (TLS) for PHI encryption during transit. Amazon FSx supports logging of all API calls using AWS CloudTrail.\n",
      "\n",
      "Amazon EKS is a managed service that simplifies running Kubernetes on AWS. For Amazon ElastiCache for Redis, customers must ensure that they are running the latest HIPAA-eligible ElastiCache for Redis engine version and current generation node types, with data encryption at rest and in transit.\n",
      "\n",
      "Amazon OpenSearch Service enables customers to run a managed OpenSearch or legacy Elasticsearch OSS cluster in a dedicated Amazon Virtual Private Cloud (Amazon VPC). Customers should use OpenSearch or Elasticsearch 6.0 or later, ensure PHI is encrypted at-rest and in-transit, and enable node-to-node encryption.\n",
      "\n",
      "Amazon EMR deploys and manages a cluster of Amazon EC2 instances into a customer’s account. For information on encryption with Amazon EMR, the text refers to the Encryption Options.\n",
      "\n",
      "Amazon EventBridge, a serverless event bus, is integrated with AWS CloudTrail. By default, EventBridge encrypts data using 256-bit Advanced Encryption Standard (AES-256), ensuring the integrity and confidentiality of electronic protected health information (ePHI).\n",
      "\n",
      "Amazon HealthLake enables customers to store, transform, query, and analyze health data at petabyte scale. Amazon HealthLake encrypts data both in transit and at rest. For the encryption of data in transit, clients must support Transport Layer Security (TLS) 1.2 or later. For the encryption of data at rest, Amazon HealthLake encrypts data in customer’s data stores with a customer-owned AWS KMS key or by a service-owned AWS KMS key by default.\n",
      "\n",
      "Amazon Inspector is a service that assesses applications for vulnerabilities or deviations from best practices. After performing an assessment, Amazon Inspector produces a detailed list of security findings prioritized by level of severity. Customers may run Amazon Inspector on EC2 instances that contain PHI.\n",
      "\n",
      "Amazon Managed Service for Apache Flink enables customers to quickly author SQL code that continuously reads, processes, and stores data in near real time. When using Firehose delivery streams as sources for analytics application, if the stream is encrypted, Managed Service for Apache Flink accesses the data in the encrypted stream seamlessly with no further configuration needed.\n",
      "\n",
      "Amazon Kinesis Video Streams is a fully managed AWS service that customers can use to stream live video from devices to the AWS Cloud. This service uses server-side encryption to automatically encrypt data at rest by using an AWS KMS key specified by the customer. Data is encrypted before it is written to the Kinesis Video Streams stream storage layer, and it is decrypted after it is retrieved from storage.\n",
      "\n",
      "Amazon Lex is an AWS service for building conversational interfaces for applications using voice and text. Amazon Lex provides the deep functionality and flexibility of natural language understanding (NLU) and automatic speech recognition (ASR) so customers can build highly engaging user experiences with lifelike, conversational interactions, and create new categories of products. Lex uses the HTTPS protocol to communicate both with clients as well as other AWS services.\n",
      "\n",
      "Amazon Managed Streaming for Apache Kafka (Amazon MSK) provides encryption features for data at rest and for data in-transit. For data at rest encryption, Amazon MSK cluster uses Amazon EBS server-side encryption and AWS KMS keys to encrypt data volumes attached to the brokers. For data in-transit, MSK uses TLS to encrypt data between clients and brokers, and between brokers in a cluster. MSK supports mutual authentication using TLS.\n",
      "\n",
      "Amazon MQ is a managed message broker service for Apache ActiveMQ that makes it easy to set up and operate message brokers in the cloud. To provide the encryption of PHI data while in transit, the following protocols with TLS enabled should be used to access brokers: AMQP, MQTT, MQTT over WebSocket, OpenWire, and STOMP. Amazon MQ encrypts messages at-rest and in transit using encryption keys that it manages and stores securely.\n",
      "\n",
      "Amazon Neptune is a fast, reliable, fully managed graph database service that makes it easy to build and run applications that work with highly connected datasets. Data containing PHI can now be retained in an encrypted instance of Amazon Neptune. An encrypted instance of Amazon Neptune can be specified only at the time of creation by choosing 'Enable Encryption' from the Amazon Neptune console. All logs, backups, and snapshots are encrypted for an Amazon Neptune encrypted instance. Key management for encrypted instances of Amazon Neptune is provided through the AWS KMS. Encryption of data in transit is provided through SSL/TLS.\n",
      "\n",
      "AWS Network Firewall is a managed firewall service that makes it easy to deploy essential network protections for all your Amazon Virtual Private Cloud (Amazon VPC). The service automatically scales with network traffic volume to provide high-availability protections without the need to set up or maintain the underlying infrastructure. Both customer rules and access logs may contain end user IP addresses, which are encrypted both at rest and in transit within the AWS architecture. Furthermore, AWS Network Firewall encrypts all data at rest and in transit between component AWS services (Amazon S3, Amazon DynamoDB, Amazon CloudWatch Logs, Amazon EBS).\n"
     ]
    }
   ],
   "source": [
    "print(refine_summary['output_text'].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fbf269-684d-43ea-9519-3aca9373aea9",
   "metadata": {},
   "source": [
    "---\n",
    "Now that we have seen how the `refine` document chain constructs a response, let us try altering the refine_llm_chain prompt template to help highlight some of the multilingual capabilties of the [Mistral Large](https://mistral.ai/news/mistral-large/) model. Mistral Large demonstrates superior capabilities in handling multi-lingual tasks. Mistral-large has been specifically trained to understand and generate text in multiple languages, especially in French, German, Spanish, and Italian. This can be especially valuable for businesses and users that need to communicate in multiple languages. In the below cell, we set the `refine llm chain` prompt template to return the final summary in French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3e0d891-1522-44c1-b1eb-dc24aea5c1a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#refine llm chain prompt template\n",
    "refine_documents_prompt_french= PromptTemplate.from_template(\"Your job is to produce a final summary\\n\"\n",
    "    \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing summary\"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original summary in French\"\n",
    "    \"If the context isn't useful, return the original summary.\")\n",
    "\n",
    "refine_summary_chain_french.refine_llm_chain.prompt.template = refine_documents_prompt_french.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c15ebb8-0db7-42d0-a712-83a0840f24f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this cell takes 5-10 minutes to run\n",
    "try:\n",
    "    refine_summary_french = refine_summary_chain_french.invoke(hipaa_docs[50:71])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "76d6cd69-38a5-415d-bb20-36e0420bbfb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le texte traite des mesures visant à garantir la sécurité et la conformité HIPAA sur Amazon Web Services (AWS), en se concentrant sur divers services tels qu'Amazon Elastic File System (EFS), Amazon Elastic Kubernetes Service (Amazon EKS), Amazon ElastiCache for Redis, Amazon OpenSearch Service, Amazon EventBridge, Amazon Forecast, Amazon FSx, Amazon GuardDuty, Amazon HealthLake, Amazon Inspector, Amazon Managed Service for Apache Flink, Amazon Kinesis Streams, Amazon Data Firehose, Amazon Kinesis Video Streams, Amazon Managed Streaming for Apache Kafka (Amazon MSK), Amazon MQ et Amazon Lex. Il souligne l'importance de surveiller et de journaliser toutes les activités liées aux PHI sur AWS à l'aide d'outils tels qu'AWS CloudTrail et Amazon CloudWatch.\n",
      "\n",
      "Pour chaque service, le texte insiste sur les méthodes de chiffrement des PHI au repos et en transit, ainsi que sur les meilleures pratiques pour garantir la conformité HIPAA. Le texte mentionne également l'utilisation d'Amazon Neptune, un service de base de données de graphes entièrement géré, pour stocker et traiter des données hautement connectées contenant des PHI. Les données contenant des PHI peuvent être conservées dans une instance chiffrée d'Amazon Neptune, avec un chiffrement activé au moment de la création. Tous les journaux, sauvegardes et instantanés sont chiffrés pour une instance Amazon Neptune chiffrée. La gestion des clés pour les instances chiffrées d'Amazon Neptune est assurée par AWS KMS. Le chiffrement des données en transit est fourni via SSL/TLS. Amazon Neptune utilise CloudTrail pour journaliser tous les appels d'API.\n",
      "\n",
      "En outre, AWS Network Firewall est un service de pare-feu géré qui facilite le déploiement de protections réseau essentielles pour tous les Amazon Virtual Private Cloud (Amazon VPC). Le service chiffre toutes les données au repos et en transit entre les services AWS composants (Amazon S3, Amazon DynamoDB, Amazon CloudWatch Logs, Amazon EBS). Les clients doivent toujours s'assurer que les PHI sont chiffrés au repos et en transit, et activer le chiffrement de nœud à nœud lorsque cela est applicable.\n",
      "\n",
      "Enfin, le texte met en garde contre l'utilisation de PHI dans le cadre d'un nom de fichier ou de dossier pour tous les services AWS. La surveillance est importante pour maintenir la fiabilité, la disponibilité et les performances des services AWS des clients. Pour suivre la santé des services AWS, les clients peuvent utiliser Amazon CloudWatch. Avec CloudWatch, les clients peuvent obtenir des métriques pour les opérations AWS individuelles ou pour l'ensemble des opérations AWS. Les clients peuvent également configurer des alarmes CloudWatch pour être avertis lorsqu'une ou plusieurs métriques dépassent un seuil défini par les clients.\n"
     ]
    }
   ],
   "source": [
    "print(refine_summary_french['output_text'].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fe47f1-a1cc-4a06-97ff-6494256f8ef7",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "`Refine` has the potential to incorporate more relevant context compared to `Map_Reduce`, potentially resulting in a more comprehensive and accurate summary. However, it comes with a trade-off: `Refine` necessitates a significantly higher number of calls to the LLM than the `Stuff` and `Map_Reduce` since it is an incremental process where the subsequent chunk's summary uses the previous chunk's summary. Moreover, these calls are not independent, which means they cannot be parallelized, potentially leading to longer processing times. Another consideration is that the Refine method may exhibit recency bias, where the most recent document chunks in the sequence could carry more weight or influence in the final summary, as the method processes documents in a specific order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543f979c-4d80-408d-9799-20043d9fbe0a",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, we have successfully looked at three different summarization techniques using LangChain; **Stuff**, **Map_Reduce**, and **Refine**. Each of these methods has its own distinct advantages/uses. \n",
    "\n",
    "- ***Stuff*** is straighforward and is the fastest method out of the three since it makes a single call to the LLM and fits the entire document within the model's context window. Although as we saw with the HIPAA Compliance document, it does not scale well to work with large volumes of text.\n",
    "\n",
    "- ***Map_Reduce*** deals with the issue of the context window length while being able to parallelize generation of summaries for individual chunks, thereby speeding up the model's response while being able to process long documents. An issue with Map_Reduce is that since this is not a recursive process, we lose context between chunks during this process.\n",
    "\n",
    "- ***Refine*** deals with the issues that arise with the previous methods. It performs recursive summarization by incrementally generating summaries for each of the chunks while retaining context between them. While this method generates the most accurate and comprehensive summary out of all 3 methods, the calls made to the LLM cannot be parallelized. This can result in longer processing times. Additionally, more recent document chunks tend to carry more weight due to the order that they are processed in.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca227dd3-5b63-4fd7-bf48-38bd36252a79",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Distributors\n",
    "- Amazon Web Services\n",
    "- Mistral AI\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
