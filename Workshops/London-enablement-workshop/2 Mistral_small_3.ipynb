{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Enterprise Use Cases with Mistral Small 3: High-Performance Large Language Model with Low Latency\n",
    "\n",
    "\n",
    "This notebook explores Mistral Small 3, a 24B parameter Large Language Model that achieves remarkable performance while maintaining exceptional efficiency. Released under Apache 2.0 license, the model demonstrates 81% MMLU accuracy and processes 150 tokens per second, rivaling larger models like Llama 3.3 70B while operating three times faster on identical hardware. Through practical examples in fraud detection, customer service, sentiment analysis, and emergency triage, we showcase its versatility in handling complex enterprise tasks while maintaining rapid response times.\n",
    "\n",
    "---\n",
    "\n",
    "## Model Card\n",
    "\n",
    "**Available regions:** *us-east-2, eu-west-3*\n",
    "\n",
    "**Model ID:** [*Mistral-Small-24B-Instruct-2501*](https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501)\n",
    "\n",
    "**Multilingual:** *Supports dozens of languages, including English, French, German, Spanish, Italian, Chinese, Japanese, Korean, Portuguese, Dutch, and Polish.*\n",
    "\n",
    "**Agent-Centric:** *Offers best-in-class agentic capabilities with native function calling and JSON outputting.*\n",
    "\n",
    "**Advanced Reasoning:** *State-of-the-art conversational and reasoning capabilities.*\n",
    "\n",
    "**Apache 2.0 License:** *Open license allowing usage and modification for both commercial and non-commercial purposes.*\n",
    "\n",
    "**Context Window:** *A 32k context window.*\n",
    "\n",
    "**System Prompt:** *Maintains strong adherence and support for system prompts.*\n",
    "\n",
    "**Tokenizer:** *Utilizes a Tekken tokenizer with a 131k vocabulary size.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Benchmarks\n",
    "\n",
    "### Human evaluations\n",
    "\n",
    "| Category | Gemma-2-27B | Qwen-2.5-32B | Llama-3.3-70B | Gpt4o-mini |\n",
    "|----------|-------------|--------------|---------------|------------|\n",
    "| Mistral is better | 0.536 | 0.496 | 0.192 | 0.200 |\n",
    "| Mistral is slightly better | 0.196 | 0.184 | 0.164 | 0.204 |\n",
    "| Ties | 0.052 | 0.060 | 0.236 | 0.160 |\n",
    "| Other is slightly better | 0.060 | 0.088 | 0.112 | 0.124 |\n",
    "| Other is better | 0.156 | 0.172 | 0.296 | 0.312 |\n",
    "\n",
    "### Instruct performance\n",
    "\n",
    "**Reasoning & Knowledge**\n",
    "\n",
    "| Evaluation | mistral-small-24B-instruct-2501 | gemma-2b-27b | llama-3.3-70b | qwen2.5-32b | gpt-4o-mini-2024-07-18 |\n",
    "|------------|---------------|--------------|---------------|---------------|-------------|\n",
    "| mmlu_pro_5shot_cot_instruct | 0.663 | 0.536 | 0.666 | 0.683 | 0.617 |\n",
    "| gpqa_main_cot_5shot_instruct | 0.453 | 0.344 | 0.531 | 0.404 | 0.377 |\n",
    "\n",
    "**Math & Coding**\n",
    "\n",
    "| Evaluation | mistral-small-24B-instruct-2501 | gemma-2b-27b | llama-3.3-70b | qwen2.5-32b | gpt-4o-mini-2024-07-18 |\n",
    "|------------|---------------|--------------|---------------|---------------|-------------|\n",
    "| humaneval_instruct_pass@1 | 0.848 | 0.732 | 0.854 | 0.909 | 0.890 |\n",
    "| math_instruct | 0.706 | 0.535 | 0.743 | 0.819 | 0.761 |\n",
    "\n",
    "**Instruct following**\n",
    "\n",
    "| Evaluation | mistral-small-24B-instruct-2501 | gemma-2b-27b | llama-3.3-70b | qwen2.5-32b | gpt-4o-mini-2024-07-18 |\n",
    "|------------|---------------|--------------|---------------|---------------|-------------|\n",
    "| mtbench_dev | 8.35 | 7.86 | 7.96 | 8.26 | 8.33 |\n",
    "| wildbench | 52.27 | 48.21 | 50.04 | 52.73 | 56.13 |\n",
    "| arena_hard | 0.873 | 0.788 | 0.840 | 0.860 | 0.897 |\n",
    "| ifeval | 0.829 | 0.8065 | 0.8835 | 0.8401 | 0.8499 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Deploy model from Amazon Bedrock Marketplace\n",
    "\n",
    "Please follow below instruction: \n",
    "1. On the Amazon Bedrock console, choose Model catalog under Foundation models in the navigation pane.\n",
    "2. Search for \"Mistral-Small-24B-Instruct-2501\" in the ‘Filter for a model’ input box.\n",
    "![Model Catalog](./images/search-model.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "3. To begin using Mistral Small 3 model, choose Deploy.\n",
    "\n",
    "![Model Information](./images/model-information.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "4. For Endpoint name, enter an endpoint name (between 1–50 alphanumeric characters).\n",
    "5. For Number of instances, enter a number of instances (between 1–100).\n",
    "6. For Instance type, choose your instance type. For optimal performance with Mistral Small 3, a GPU-based instance type like ml.g6.12xlarge is recommended.\n",
    "\n",
    "Optionally, you can configure advanced security and infrastructure settings, including virtual private cloud (VPC) networking, service role permissions, and encryption settings. For most use cases, the default settings will work well. However, for production deployments, you might want to review these settings to align with your organization’s security and compliance requirements.\n",
    "\n",
    "![Model Configuration](./images/model-configuration.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**❗Note: Model deployment may take 10 mins.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sagemaker\n",
    "import time\n",
    "from PIL import Image\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Copy the endpoint ARN from AWS Console and set this in endpoint_arn variable below.**\n",
    "\n",
    "![Model Endpoint](./images/model-endpoint.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_arn = '<REPLACE THIS WITH ENDPOINT ARN FROM AWS CONSOLE>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a function to invoke mistral model via Bedrock Converse  \n",
    "\n",
    "inferenceConfig={\n",
    "    'maxTokens': 2000,\n",
    "    'temperature':0.1,\n",
    "}\n",
    "\n",
    "def run_mistral_converse(user_prompt, system_prompt=None, model_id=endpoint_arn, inferenceConfig=inferenceConfig, additionalConfig=None):\n",
    "    bedrock = boto3.client('bedrock-runtime', 'us-west-2')\n",
    "    \n",
    "    request_payload = {\n",
    "        \"modelId\": model_id,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"text\": user_prompt}]\n",
    "            }\n",
    "        ],\n",
    "        \"inferenceConfig\": inferenceConfig,\n",
    "        \"additionalModelRequestFields\": additionalConfig\n",
    "    }\n",
    "    \n",
    "    # Only add 'system' if system_prompt is provided\n",
    "    if system_prompt:\n",
    "        request_payload[\"system\"] = [{\"text\": system_prompt}]\n",
    "    \n",
    "    response = bedrock.converse(**request_payload)\n",
    "    \n",
    "    return response['output']['message']['content'][0]['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case 1: Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we evaluate Mistral Small 3's ability to detect financial fraud and security threats. The model demonstrates advanced pattern recognition capabilities essential for financial services, including real-time phishing detection, transaction anomaly identification, and risk assessment. This application highlights the model's strength in providing structured reasoning with low-latency responses, crucial for time-sensitive security decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phishing Email detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mistral Small 3 analyzse email content and detect sophisticated phishing attempts that evade traditional rule-based filters. The model examines linguistic patterns, sender impersonation tactics, and psychological manipulation techniques to identify both common and novel phishing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''\n",
    "Please analyze this email for potential phishing activity. Rate each indicator as 1 (present) or 0 (absent):\n",
    "\n",
    "[SENSITIVE] Requests for sensitive data (passwords, financial, personal): [0/1]\n",
    "[DOMAIN] Sender's email domain mismatches claimed company: [0/1]\n",
    "[LINKS] Embedded links don't match legitimate organization domain: [0/1]\n",
    "[GREETING] Generic greetings used instead of personal name: [0/1]\n",
    "[ERRORS] Presence of spelling or grammatical mistakes: [0/1]\n",
    "[URGENT] Uses urgent or threatening language to create panic: [0/1]\n",
    "\n",
    "Total flags: [X/6]\n",
    "Brief analysis: [1-2 sentence conclusion on whether this appears to be a phishing attempt and why]\n",
    "'''\n",
    "user_prompt = '''\n",
    "To: john.doe@company.com\n",
    "Subject: URGENT: Your Octank account has been locked\n",
    "\n",
    "Dear Valued Customer,\n",
    "\n",
    "We have detected unusual login activity on your Octank account from an unrecognized device. As a precautionary measure, we have temporarily limited access to your account.\n",
    "\n",
    "To restore full account access, please verify your identity within 24 hours by clicking below:\n",
    "\n",
    "https://0ctank-account-verify.com/unlock-account\n",
    "\n",
    "If you do not complete this verification process within 24 hours, your account will be permanently suspended.\n",
    "\n",
    "Best regards,\n",
    "Account Security Team\n",
    "\n",
    "-------------------\n",
    "This is an automated security alert from Octank\n",
    "For support: support@octank-accounts.net\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run_mistral_converse(user_prompt, system_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fraudulent Call Detection\n",
    "\n",
    "Mistral Small 3 analyzes transcripts of suspicious phone calls, identifying common deception tactics and social engineering patterns used by scammers. The system automatically flags potential fraud indicators - like urgency manipulation, impersonation of authority figures, and unusual payment requests - helping financial institutions and call centers rapidly detect and respond to emerging scam attempts while protecting vulnerable customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''\n",
    "Please analyze this call/message for potential scam activity. Rate each indicator as 1 (present) or 0 (absent):\n",
    "\n",
    "[ID] Missing/incomplete identification (name/company/ID): [0/1]\n",
    "[OFFER] Suspicious offers or too-good-to-be-true promises: [0/1]\n",
    "[VAGUE] Non-specific references instead of account details: [0/1]\n",
    "[REDIRECT] Unsolicited direction to unofficial channels: [0/1]\n",
    "[URGENT] Pressure tactics or urgent deadlines: [0/1]\n",
    "Total flags: [X/5]\n",
    "Brief analysis: [1-2 sentence conclusion]'''\n",
    "\n",
    "user_prompt = ''' \n",
    "Hi there, this is Jessie calling in regards to your Honda warranty. The warranty is up for renewal. \n",
    "I’d like to congratulate you on your $2,000 instant rebate and free maintenance and oil change package for being a loyal customer. \n",
    "Call me back at 934-153-XXXX to redeem now. Once again that number was 934-153-XXXX. Thank you so much. Have a great day. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run_mistral_converse(user_prompt, system_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case 2: Virtual Customer Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The customer service demonstration showcases Mistral Small 3's prowess in maintaining contextual awareness during technical support conversations. We test its ability to provide accurate AWS-specific guidance while maintaining conversation flow and context. This example particularly highlights the model's fast-response capabilities and efficient memory management in multi-turn dialogues, essential features for real-world customer support applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "def chat_history_to_string(memory):\n",
    "    history_str = \"\"\n",
    "    for chat_item in memory:\n",
    "        role = chat_item.get(\"role\", \"\")\n",
    "        content = chat_item.get(\"content\", \"\")\n",
    "        history_str += f\"{role}: {content}\\n\\n\"\n",
    "    return history_str.strip()\n",
    "\n",
    "\n",
    "def format_conversation(user_input: str, memory: List[Dict[str, str]] = []) -> str:\n",
    "    \n",
    "    history = chat_history_to_string(memory)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are a knowledgeable helpful AWS customer service assistant. You are helpful and provide general guidance from the context less than 100 words in the scope.\n",
    "    {history} \n",
    "    {user_input}\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def chat_with_agent(user_input: str, memory: List[Dict[str, str]]):\n",
    "    # Format the conversation using the helper functions\n",
    "    formatted_prompt = format_conversation(user_input, memory)\n",
    "    \n",
    "    # Get response\n",
    "    response_content = run_mistral_converse(user_prompt = formatted_prompt)\n",
    "    \n",
    "    # Display response\n",
    "    display(Markdown(response_content))\n",
    "    \n",
    "    # Update memory\n",
    "    memory.append({\"role\": \"user\", \"content\": user_input})\n",
    "    memory.append({\"role\": \"assistant\", \"content\": response_content})\n",
    "    \n",
    "    return response_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = []\n",
    "chat_with_agent(\"Hi there\", memory);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_with_agent(\"How to select an EC2 instance type?\", memory);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_with_agent(\"Cool. Will that work for my Linux workload?\", memory);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case 3: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our sentiment analysis implementation leverages Mistral Small 3's high-throughput text processing capabilities, focusing on financial news and market sentiment. The model exhibits consistent performance in categorical classification while maintaining rapid processing speeds. This example demonstrates the model's ability to handle varied content types while providing reliable, structured outputs suitable for automated trading and market analysis systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\" Respond with \"positive\" or \"neutral\" or \"negative\" and nothing else for the following 3 statements. \"\"\"\n",
    "\n",
    "user_prompt1 = \"\"\" I absolutely love this new phone, it's amazing how fast it works and the camera quality is incredible. \"\"\"\n",
    "user_prompt2 = \"\"\" The weather today is cloudy with some sun, temperature around 70 degrees. \"\"\"\n",
    "user_prompt3 = \"\"\" This is the worst customer service I've ever experienced. They lost my order and refused to help me resolve the issue. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = user_prompt1+user_prompt2+user_prompt3\n",
    "print(run_mistral_converse(user_prompt,system_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case 4: Healthcare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Healthcare professionals struggle to efficiently process large volumes of medical text under time pressure. These examples showcases how Mistral Small 3 can transform complex medical content - from patient records to research papers - into focused, actionable insights. Mistral extracts and prioritizes critical information while maintaining clinical accuracy and relevance, enabling healthcare providers to make better-informed decisions quickly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medical Research Paper Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Medical professionals face an overwhelming challenge keeping pace with the exponentially growing body of clinical research, where thousands of new studies are published daily across numerous specialties. Mistral Small 3 can help clinicians efficiently process and evaluate research papers by automatically identifying crucial elements including study design, population characteristics, primary outcomes, statistical significance, and potential methodological limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Details:**\n",
    "\n",
    "+ **Repository:** [PubMed Central (PMC)](https://pmc.ncbi.nlm.nih.gov/) \n",
    "+ Research article: [Clinical Trials and Clinical Research: A Comprehensive Review](https://pmc.ncbi.nlm.nih.gov/articles/PMC10023071/pdf/cureus-0015-00000035077.pdf)\n",
    "\n",
    "**Description:**\n",
    "\n",
    "PubMed Central (PMC) serves as a central repository to archive biomedical and life sciences literature maintained by the U.S. National Institutes of Health's National Library of Medicine (NIH/NLM) for free full-text research papers, enabling broad access to scientific knowledge in the medical and life sciences domains.\n",
    "\n",
    "Key aspects:\n",
    "\n",
    "+ Acts as a freely accessible digital repository of full-text biomedical and life sciences research articles, promoting open access to scientific literature\n",
    "+ Maintained by NIH/NLM, ensuring high-quality curation and reliable archiving of scientific publications\n",
    "+ Facilitates comprehensive access to clinical trials and research papers, supporting evidence-based medical practice and research advancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_pdf(url, output_filename):\n",
    "    # Headers to mimic a browser request\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Referer': 'https://www.google.com'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Send GET request with headers\n",
    "        response = requests.get(url, headers=headers, stream=True)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            return response.content\n",
    "        else:\n",
    "            print(f\"Failed to download PDF. Status code: {response.status_code}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://pmc.ncbi.nlm.nih.gov/articles/PMC10023071/pdf/cureus-0015-00000035077.pdf\"\n",
    "filename = 'Clinical Trials and Clinical Research.pdf'\n",
    "research_article = download_pdf(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def parse_pdf_bytes(pdf_bytes):\n",
    "    \"\"\"Parses PDF content from bytes and returns the text.\"\"\"\n",
    "    pdf_file = io.BytesIO(pdf_bytes)\n",
    "    reader = PdfReader(pdf_file)\n",
    "    text_content = \"\"\n",
    "    for page in reader.pages:\n",
    "        text_content += page.extract_text() + \"\\n\"\n",
    "    return text_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = parse_pdf_bytes(research_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\"\n",
    "You are a medical research analyst helping clinicians evaluate scientific papers. For each paper:\n",
    "- Summarize the key findings and clinical implications\n",
    "- Evaluate the methodology and study design\n",
    "- Highlight potential limitations or biases\n",
    "- Compare findings with existing literature\n",
    "- Suggest practical applications\n",
    "Use evidence-based analysis and maintain scientific rigor. Flag any potential conflicts of interest or statistical concerns.\n",
    "\n",
    "<RESEARCH_PAPER>\n",
    "{doc}\n",
    "</RESEARCH_PAPER>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(run_mistral_converse(user_prompt=prompt))\n",
    "# this step may take 1 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinical Note Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emergency department physicians face the critical challenge of rapidly processing extensive patient histories while making time-sensitive medical decisions. This example showcases how to transform lengthy clinical documentation into concise, clinically relevant summaries. Mistral Small 3 analyzes multiple sources including previous visit notes, discharge summaries, and specialist consultations, extracting key information about past medical conditions, current medications, allergies, and recent interventions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Details:**\n",
    "\n",
    "+ **Dataset Name:** [aisc-team-a1/augmented-clinical-notes](https://huggingface.co/datasets/aisc-team-a1/augmented-clinical-notes) \n",
    "+ **Language(s):** English only\n",
    "+ **Repository:** [EPFL-IC-Make-Team/ClinicalNotes](https://github.com/EPFL-IC-Make-Team/medinote)\n",
    "+ Paper: MediNote: Automated Clinical Notes\n",
    "\n",
    "**Description:**\n",
    "\n",
    "This is a medical dataset developed for the AISC class at Harvard Medical School, comprising 30,000 clinical note triplets. The dataset combines real clinical notes from PMC-Patients, synthetic doctor-patient dialogues (NoteChat), and structured patient information, serving as the foundation for training MediNote-7B and MediNote-13B clinical note generators.\n",
    "\n",
    "Key components:\n",
    "\n",
    "+ Real clinical notes are sourced from PubMed Central case studies through the PMC-Patients dataset, providing detailed patient summaries, medical histories, treatments, and outcomes\n",
    "+ Synthetic dialogues were generated using GPT-3.5 to address the scarcity of real patient-doctor conversation data due to confidentiality constraints\n",
    "+ Structured patient information was extracted from the 30,000 longest clinical notes using GPT-4, following a specialized medical information template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"aisc-team-a1/augmented-clinical-notes\", split='train[:1%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medical_note = dataset['full_note'][0]\n",
    "\n",
    "prompt = f\"\"\"\"\n",
    "You are a clinical documentation specialist tasked with summarizing patient records. Focus on:\n",
    "- Key diagnoses and dates\n",
    "- Current medications and allergies\n",
    "- Recent procedures or hospitalizations\n",
    "- Relevant family history\n",
    "- Critical lab values\n",
    "Present information in a structured format prioritizing recent and clinically significant findings. Flag any critical values or concerning trends. Use standard medical terminology and maintain all dates and specific values exactly as provided.\n",
    "\n",
    "<MEDICAL_NOTE>\n",
    "{medical_note}\n",
    "</MEDICAL_NOTE>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(run_mistral_converse(user_prompt=prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions:\n",
    "\n",
    "Mistral Small 3 establishes itself as a powerful, versatile model that successfully balances performance with efficiency. Its ability to match larger models while maintaining significantly lower latency makes it particularly suitable for production deployments. The model's Apache 2.0 license and efficient resource utilization make it an attractive option for both enterprise applications and local deployments. Our testing confirms its capability to handle complex tasks while maintaining the speed and reliability necessary for real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "It's important to cleanup the provisioned resources to avoid incurring costs. You have two options to delete the endpoint created in this notebook.\n",
    "In AWS Console, navigate to Amazon Bedrock service and click on Marketplace deployments under Foundation models. Here, select the deployed endpoint and click on Delete button. \n",
    "\n",
    "![Delete-Endpoint](./images/endpoint-delete.png)\n",
    "\n",
    "Upon clicking the Delete button, a confirmation popup shows up. Here you read the warning carefully and confirm deletion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can run below cell to delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_client=boto3.client('bedrock')\n",
    "bedrock_client.delete_marketplace_model_endpoint(endpointArn=endpoint_arn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
