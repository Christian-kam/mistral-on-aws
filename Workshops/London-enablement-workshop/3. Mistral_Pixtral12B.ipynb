{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Deploy and Run Pixtral 12B from Amazon Bedrock Marketplace: Exploring Potential Use Cases\n",
    "\n",
    "This is an accompanying notebook of AWS Blog on Pixtral 12B model usage via Amazon Bedrock Marketplace. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary libraries for grabbing sagemaker execution role, bedrock and bedrock runtime API operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sagemaker\n",
    "import time\n",
    "from PIL import Image\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Model from Bedrock Marketplace\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Amazon Bedrock console, you can choose Model catalog in the Foundation models section of the navigation pane. Here, you can search for models that help you with a specific use case or language. The results of the search include both serverless models and models available in Amazon Bedrock Marketplace. You can filter results by provider, modality (such as text, image, or audio), or task (such as classification or text summarization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access Pixtral 12B in Amazon Bedrock, follow below steps:\n",
    "\n",
    "1. On the Amazon Bedrock console, choose Model catalog under Foundation models in the navigation pane. \n",
    "2. Filter for Mistral as a provider and choose the Pixtral 12B model OR Search for Pixtral in the ‘Filter for a model’ input box\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bedrock Marketplace Model Catalog](./images/br_model_catalog.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model detail page provides essential information about the model’s capabilities, pricing structure, and implementation guidelines. You can find detailed usage instructions, including sample API calls and code snippets for integration. \n",
    "\n",
    "The page also includes deployment options and licensing information to help you get started with Pixtral 12B in your applications.\n",
    "\n",
    "3. To begin using Pixtral 12B, choose Deploy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pixtral 12B Model Card](./images/br_marketplace_model_card.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be prompted to configure the deployment details for Pixtral 12B. The model ID will be pre-populated.\n",
    "\n",
    "4. Read carefully and Accept the End User License Agreement (EULA).\n",
    "5. For Endpoint name, enter an endpoint name (between 1–50 alphanumeric characters).\n",
    "6. For Number of instances, enter a number of instances (between 1–100).\n",
    "7. For Instance type, choose your instance type. For optimal performance with Pixtral 12B, a GPU-based instance type like ml.g6.12xlarge is recommended.\n",
    "\n",
    "Optionally, you can configure advanced security and infrastructure settings, including virtual private cloud (VPC) networking, service role permissions, and encryption settings. For most use cases, the default settings will work well. However, for production deployments, you might want to review these settings to align with your organization’s security and compliance requirements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Deploy Model](./images/br_marketplace_deploy.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the deployment is complete, Endpoint status should chanage to In Service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Model Endpoint Status](./images/br_marketplace_model_endpoint.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the endpoint ARN from AWS Console and set this in endpoint_arn variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_arn = '<REPLACE THIS WITH ENDPOINT ARN FROM AWS CONSOLE>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we provide examples use cases of Pixtral 12B using sample prompts. We have defined helper functions to invoke Pixtral 12B model using Bedrock Converse APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bedrock runtime object\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_format(image_path):\n",
    "    with Image.open(image_path) as img:\n",
    "        # Normalize the format to a known valid one\n",
    "        fmt = img.format.lower() if img.format else 'jpeg'\n",
    "        # Convert 'jpg' to 'jpeg'\n",
    "        if fmt == 'jpg':\n",
    "            fmt = 'jpeg'\n",
    "    return fmt\n",
    "\n",
    "def call_bedrock_model(model_id=None, prompt=\"\", image_paths=None, system_prompt=\"\", temperature=0.6, top_p=0.9, max_tokens=3000):\n",
    "    \n",
    "    if isinstance(image_paths, str):\n",
    "        image_paths = [image_paths]\n",
    "    if image_paths is None:\n",
    "        image_paths = []\n",
    "    \n",
    "    # Start building the content array for the user message\n",
    "    content_blocks = []\n",
    "\n",
    "    # Include a text block if prompt is provided\n",
    "    if prompt.strip():\n",
    "        content_blocks.append({\"text\": prompt})\n",
    "\n",
    "    # Add images as raw bytes\n",
    "    for img_path in image_paths:\n",
    "        fmt = get_image_format(img_path)\n",
    "        # Read the raw bytes of the image (no base64 encoding!)\n",
    "        with open(img_path, 'rb') as f:\n",
    "            image_raw_bytes = f.read()\n",
    "\n",
    "        content_blocks.append({\n",
    "            \"image\": {\n",
    "                \"format\": fmt,\n",
    "                \"source\": {\n",
    "                    \"bytes\": image_raw_bytes\n",
    "                }\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # Construct the messages structure\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content_blocks\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Prepare additional kwargs if system prompts are provided\n",
    "    kwargs = {}\n",
    "    \n",
    "    kwargs[\"system\"] = [{\"text\": system_prompt}]\n",
    "\n",
    "    # Build the arguments for the `converse` call\n",
    "    converse_kwargs = {\n",
    "        \"messages\": messages,\n",
    "        \"inferenceConfig\": {\n",
    "            \"maxTokens\": 4000,\n",
    "            \"temperature\": temperature,\n",
    "            \"topP\": top_p\n",
    "        },\n",
    "        **kwargs\n",
    "    }\n",
    "\n",
    "    \n",
    "    converse_kwargs[\"modelId\"] = model_id\n",
    "\n",
    "    # Call the converse API\n",
    "    try:\n",
    "        response = bedrock_runtime.converse(**converse_kwargs)\n",
    "    \n",
    "        # Parse the assistant response\n",
    "        assistant_message = response.get('output', {}).get('message', {})\n",
    "        assistant_content = assistant_message.get('content', [])\n",
    "        result_text = \"\".join(block.get('text', '') for block in assistant_content)\n",
    "    except Exception as e:\n",
    "        result_text = f\"Error message: {e}\"\n",
    "    return result_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Logical Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the interesting use case of vision models is solving logical reasoning problems or visual puzzles. Pixtral 12B vision models are highly capable in solving and answering logical reasoning questions. Here is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt='You are solving logical reasoning problems.'\n",
    "task = 'Which of these figures differ from the other four?'\n",
    "image_path = './images/logical_reasoning.jpg'\n",
    "\n",
    "\n",
    "print('Input Image:\\n\\n')\n",
    "Image.open(image_path).show()\n",
    "\n",
    "response = call_bedrock_model(model_id=endpoint_arn, \n",
    "                   prompt=task, \n",
    "                   system_prompt=system_prompt,\n",
    "                   image_paths = image_path)\n",
    "\n",
    "print(f'\\nResponse from the model:\\n\\n{response}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Product Information\n",
    "\n",
    "Extracting product information is crucial for the retail industry, especially on platforms that host third-party sellers, where product images are the most accessible resource. Accurately capturing relevant details from these images is vital for a product's success in e-commerce. For instance, using advanced visual models like Pixtral 12B, retailers can efficiently extract key attributes from clothing product images, such as color, style, and patterns. This capability not only streamlines inventory management but also enhances customer experiences by providing essential information that aids in informed purchasing decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_prompt='''You are a catalog manager for an ecommerce portal. You have an image of a product. \n",
    "Think very carefully in a step-by-step manner to extract product information. \n",
    "Always provide response in prescribed JSON format.\n",
    "'''\n",
    "image_path = './images/cap.png'\n",
    "task = '''\n",
    "Organize product information in JSON format to store in a database. \n",
    "\n",
    "Output json schema:\n",
    "\n",
    "{\n",
    "\"product_name\": \"\",\n",
    "\"description\": \"\",\n",
    "\"category\": \"\",\n",
    "\"sub_category\": \"\",\n",
    "\"color\": \"\",\n",
    "\"size\": \"\",\n",
    "\"brand\": \"\",\n",
    "\"material\": \"\",\n",
    "\"features\": [],\n",
    "\"image_alt_text\": \"\"\n",
    "}\n",
    "'''\n",
    "\n",
    "print('Input Image:\\n\\n')\n",
    "Image.open(image_path).show()\n",
    "\n",
    "response = call_bedrock_model(model_id=endpoint_arn, \n",
    "                   prompt=task, \n",
    "                   system_prompt=system_prompt,\n",
    "                   image_paths = image_path)\n",
    "\n",
    "print(f'\\nResponse from the model:\\n\\n{response}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vehicle Damage Assessment\n",
    "\n",
    "In the insurance industry, image analysis plays a crucial role in claims processing. For vehicle damage assessment, vision models like Pixtral 12B can be leveraged to compare images taken at policy issuance with those submitted during a claim. This approach can streamline the evaluation process, potentially reducing loss adjustment expenses and expediting claim resolution. By automating the identification and characterization of automobile damage, insurers can enhance efficiency, improve accuracy, and ultimately provide a better experience for policyholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [\n",
    "    \"./images/car_image_before.png\",\n",
    "    \"./images/car_image_after.png\"\n",
    "]\n",
    "\n",
    "system_prompt='''\n",
    "You are a helpful ai assistant for an insurance agent. \n",
    "Insurance agent has received a claim for a vehicle damage. \n",
    "'''\n",
    "\n",
    "task = '''This claim includes two images. \n",
    "One of the image was taken before the incident and another was taken after the incident.\n",
    "\n",
    "Analyse these images and answer below questions:\n",
    "1. describe if there is any damage to the vehicle\n",
    "2. should insurance agent accept or reject the claim\n",
    "\n",
    "'''\n",
    "\n",
    "print('Input Images:\\n\\n')\n",
    "Image.open(image_paths[0]).show()\n",
    "Image.open(image_paths[1]).show()\n",
    "\n",
    "response = call_bedrock_model(model_id=endpoint_arn, \n",
    "                   prompt=task, \n",
    "                   system_prompt=system_prompt,\n",
    "                   image_paths = image_paths)\n",
    "\n",
    "print(f'\\nResponse from the model:\\n\\n{response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handwriting Recognition\n",
    "\n",
    "Another feature in Vision language models is their ability to recognise handwriting and extract handwritten text. Pixtral 12B performs well on extracting content from complex and poorly handwritten notes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt='You are a Graphologists'\n",
    "task = '''\n",
    "Analyze the image and transcribe any handwritten text present. \n",
    "Convert the handwriting into a single, continuous string of text. \n",
    "Maintain the original spelling, punctuation, and capitalization as written. Ignore any printed text, drawings, or other non-handwritten elements in the image.\n",
    "'''\n",
    "\n",
    "image_path = './images/a01-000u-04.png'\n",
    "\n",
    "\n",
    "print('Input Image:\\n\\n')\n",
    "Image.open(image_path).show()\n",
    "\n",
    "response = call_bedrock_model(model_id=endpoint_arn, \n",
    "                   prompt=task, \n",
    "                   system_prompt=system_prompt,\n",
    "                   image_paths = image_path)\n",
    "\n",
    "print(f'\\nResponse from the model:\\n\\n{response}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasoning over complex figures\n",
    "\n",
    "Vision Language Models (VLMs) excel at interpreting and reasoning about complex figures, charts, and diagrams. In this particular use case, we leverage Pixtral 12B, a powerful multimodal model, to analyze an intricate image containing GDP data. Pixtral 12B's advanced capabilities in document understanding and complex figure analysis make it well-suited for extracting insights from visual representations of economic data. By processing both the visual elements and  accompanying text, Pixtral 12B can provide detailed interpretations and reasoned analysis of the GDP figures presented in the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt='You are a Global Economist.'\n",
    "task = 'List the top 5 countries in Europe with the highest GDP'\n",
    "image_path = './images/gdp.png'\n",
    "\n",
    "print('Input Image:\\n\\n')\n",
    "Image.open(image_path).show()\n",
    "\n",
    "response = call_bedrock_model(model_id=endpoint_arn, \n",
    "                   prompt=task, \n",
    "                   system_prompt=system_prompt,\n",
    "                   image_paths = image_path)\n",
    "\n",
    "print(f'\\nResponse from the model:\\n\\n{response}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we showed you how to get started with Pixtral 12B model in Bedrock and deploy the model for inference. Pixtral 12B vision model enables you to solve multiple use cases, including document understanding, logical reasoning, handwriting recognition, image comparison, entity extraction, extraction of structured data from scanned images and caption generation. These capabilities can drive productivity in a number of enterprise use cases, including ecommerce (retail), marketing, FSI and much more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to cleanup the provisioned resources to avoid incurring costs. You have two options to delete the endpoint created in this notebook.\n",
    "\n",
    "In AWS Console, navigate to Amazon Bedrock service and click on Marketplace deployments under Foundation models. Here, select the deployed endpoint and click on Delete button. \n",
    "\n",
    "![Delete Endpoint](./images/br_marketplace_delete_endpoint.jpg)\n",
    "\n",
    "Upon clicking the Delete button, a confirmation popup shows up. Here you read the warning carefully and confirm deletion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can run below cell to delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_client = boto3.client('bedrock')\n",
    "bedrock_client.delete_marketplace_model_endpoint(endpointArn=endpoint_arn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
