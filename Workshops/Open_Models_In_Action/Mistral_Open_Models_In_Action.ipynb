{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9869e451-26c5-4846-866f-f8fd736ea9f6",
   "metadata": {},
   "source": [
    "# Mistral AI: Open Models in Action on AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf3475a-1741-40d4-9907-a494f7ef5f26",
   "metadata": {},
   "source": [
    "In this notebook, we will explore the capabilities of Mistral AI's open language and vision models, and demonstrate how to leverage services such as Amazon Bedrock and Amazon SageMaker to build powerful AI-powered applications.\n",
    "\n",
    "Mistral AI is a leading provider of state-of-the-art AI models for a wide range of use cases. Their open models are designed to be highly flexible and customizable, allowing developers to quickly integrate advanced AI capabilities into their applications.\n",
    "\n",
    "In this notebook, we will cover the following topics:\n",
    "\n",
    "1. **Using Pixtral Large to Explore and Implement Vision Capabilities on Amazon Bedrock**:\n",
    "   - Introduce Pixtral Large, Mistral AI's state-of-the-art text and vision model\n",
    "   - Demonstrate how to use Amazon Bedrock to leverage Pixtral Large for fully serverless inference\n",
    "   - Explore the performance and capabilities of Pixtral Large on various use cases.\n",
    "\n",
    "2. **Leveraging Mistral Small 3 to Analyze Text Data and Generate Insights on Amazon SageMaker and Amazon Bedrock**:\n",
    "   - Introduce Mistral Small 3, Mistral AI's versatile language model\n",
    "   - Show how to use Amazon SageMaker deploy and Amazon Bedrock to integrate Mistral Small for natural language processing tasks, such as text classification, sentiment analysis, and language generation\n",
    "\n",
    "3. **Building a Pipeline of Open Models Combining Multiple AWS Services**:\n",
    "   - Demonstrate how to combine Pixtral Large and Mistral Small 3 in a **unified AI pipeline**\n",
    "   - Leverage additional AWS services (e.g., Amazon S3, Amazon Lambda) to create a complete end-to-end solution for a specific use case\n",
    "   - Discuss the benefits and challenges of using open-source models in a production environment\n",
    "\n",
    "By the end of this notebook, you will have a solid understanding of how to leverage Mistral AI's open models and integrate them with AWS services to build powerful, scalable AI-powered applications.\n",
    "\n",
    "## Getting Started\n",
    "To begin, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8531b4-97b7-4198-b427-a6b070b2308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image as IPImage\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc314495-1af2-4162-bf4e-d7d8ee322566",
   "metadata": {},
   "source": [
    "## 1. Using Pixtral Large to Explore and Implement Vision Capabilities on Amazon Bedrock\n",
    "Pixtral Large represents a significant advancement in multimodal AI technology, combining sophisticated image understanding capabilities with powerful language processing. Built upon the foundation of Mistral Large 2, this 124B parameter model (123B multimodal decoder + 1B parameter vision encoder) demonstrates exceptional performance across various visual and textual understanding tasks while maintaining a substantial 128K context window that can accommodate at least 30 high-resolution images.\n",
    "\n",
    "## Model Architecture and Capabilities\n",
    "\n",
    "- Total Parameters: 124B\n",
    "- Multimodal Decoder: 123B\n",
    "- Vision Encoder: 1B\n",
    "- Context Window: 128K\n",
    "- Image Capacity: up to 30 images per request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606cc791-5d25-4983-a640-5b6517e0c262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mistral_response(prompt_text, image_path=None, show_image=True, temperature=0.6):\n",
    "    \n",
    "    model_id = \"us.mistral.pixtral-large-2502-v1:0\"\n",
    "    bedrock_client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "\n",
    "    image_paths = image_path if isinstance(image_path, list) else [image_path] if image_path else []\n",
    "\n",
    "    if image_paths and show_image:\n",
    "        for img_path in image_paths:\n",
    "            print(\"Input Image:\\n\")\n",
    "            display(IPImage(filename=img_path))\n",
    "            print(\"\\n\")\n",
    "\n",
    "    # Initialize message content with prompt text\n",
    "    message_content = [{\"text\": prompt_text}]\n",
    "\n",
    "    # Add images to content if provided\n",
    "    if image_paths:\n",
    "        for img_path in image_paths:\n",
    "            image_ext = img_path.split(\".\")[-1]\n",
    "\n",
    "            if (image_ext.lower() == 'jpg'):\n",
    "                image_ext = 'jpeg'\n",
    "\n",
    "            with open(img_path, \"rb\") as f:\n",
    "                image = f.read()\n",
    "            \n",
    "            message_content.append({\n",
    "                \"image\": {\n",
    "                    \"format\": image_ext,\n",
    "                    \"source\": {\"bytes\": image}\n",
    "                }\n",
    "            })\n",
    "\n",
    "    message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": message_content\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = bedrock_client.converse(\n",
    "            modelId=model_id,\n",
    "            messages=[message],\n",
    "            inferenceConfig={\n",
    "                \"temperature\": temperature\n",
    "            }\n",
    "        )\n",
    "\n",
    "        output_message = response['output']['message']\n",
    "        content_blocks = output_message['content']\n",
    "        result_text = \"\\n\".join(f\"{block['text']}\" for block in content_blocks)\n",
    "        return result_text\n",
    "\n",
    "    except Exception as err:\n",
    "        return f\"A client error occurred: {err}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2e9feb-e99f-4b2d-9240-28557fc5b991",
   "metadata": {},
   "source": [
    "## Use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1af262-731a-476e-9982-e437878eb2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Examine this visualization:\n",
    "\n",
    "1. First, describe what this visualization represents:\n",
    "   - What information is being shown?\n",
    "   - How is the data displayed?\n",
    "   - What do the different components represent?\n",
    "   - What does the size variation indicate?\n",
    "\n",
    "2. Analyze specific patterns:\n",
    "   - Which region shows the highest proportion of the first category?\n",
    "   - Which region shows the highest proportion of the second category?\n",
    "   - Where do you observe the largest total values?\n",
    "   - Which areas show the most balanced distribution?\n",
    "\n",
    "3. Compare regional trends:\n",
    "   - How do the proportions differ between continents?\n",
    "   - What patterns emerge between different hemispheres?\n",
    "   - Are there clear differences between different economic zones?\n",
    "\n",
    "4. Identify interesting outliers:\n",
    "   - Which regions stand out from their neighbors?\n",
    "   - Can you identify any unexpected patterns?\n",
    "   - Where do you notice significant data variations?\n",
    "\n",
    "5. Consider geographical and demographic factors:\n",
    "   - How might local conditions influence these patterns?\n",
    "   - What socioeconomic factors might explain the variations?\n",
    "   - Can you identify any correlation between size and proportions?\n",
    "\n",
    "6. Make comparisons between:\n",
    "   - Different geographical zones\n",
    "   - Various population densities\n",
    "   - Different economic development levels\"\"\"\n",
    "image_path = \"../Pixtral-samples/Pixtral_data/Map_Motorcycles_vs_cars_by_population_millions_2002.png\"\n",
    "response = get_mistral_response(prompt, image_path)\n",
    "\n",
    "print(\"\\nModel Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724125b6-f942-429f-ba77-e8f60b187b6a",
   "metadata": {},
   "source": [
    "By Dennis Bratland - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=15186498"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2756815c-5f06-49ff-8bd2-8d5eddac151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Extract organization hierarchy from the given org structure. provide response in a structured json format with below:\n",
    "- role\n",
    "- name\n",
    "- reporting_manager\n",
    "\n",
    "\"\"\"\n",
    "image_path = \"../Pixtral-samples/Pixtral_data/org_hierarchy.jpeg\"\n",
    "response = get_mistral_response(prompt, image_path, temperature=0.1)\n",
    "\n",
    "print(\"\\nModel Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2fb36c-5853-4d9a-80c4-a11018639704",
   "metadata": {},
   "source": [
    "## 2. Leveraging Mistral Small 3 to Analyze Text Data and Generate Insights on Amazon SageMaker and Amazon Bedrock:\n",
    "Mistral Small 3 is a 24B parameter Large Language Model that achieves remarkable performance while maintaining exceptional efficiency. Released under **Apache 2.0 license**, the model demonstrates 81% MMLU accuracy and processes 150 tokens per second, rivaling larger models like Llama 3.3 70B while operating three times faster on identical hardware. Through practical examples in fraud detection, customer service, sentiment analysis, and emergency triage, we showcase its versatility in handling complex enterprise tasks while maintaining rapid response times.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Model Card\n",
    "---\n",
    "\n",
    "**Available regions:** *us-east-2, eu-west-3*\n",
    "\n",
    "**Model ID:** [*Mistral-Small-24B-Instruct-2501*](https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501)\n",
    "\n",
    "**Multilingual:** *Supports dozens of languages, including English, French, German, Spanish, Italian, Chinese, Japanese, Korean, Portuguese, Dutch, and Polish.*\n",
    "\n",
    "**Agent-Centric:** *Offers best-in-class agentic capabilities with native function calling and JSON outputting.*\n",
    "\n",
    "**Advanced Reasoning:** *State-of-the-art conversational and reasoning capabilities.*\n",
    "\n",
    "**Apache 2.0 License:** *Open license allowing usage and modification for both commercial and non-commercial purposes.*\n",
    "\n",
    "**Context Window:** *A 32k context window.*\n",
    "\n",
    "**System Prompt:** *Maintains strong adherence and support for system prompts.*\n",
    "\n",
    "**Tokenizer:** *Utilizes a Tekken tokenizer with a 131k vocabulary size.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd366c2-ebd8-4e04-a391-b66d43a3753b",
   "metadata": {},
   "source": [
    "Let's important the additional libraries that we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347618c2-c1d1-44d9-882d-66b9a73f1400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sagemaker\n",
    "import time\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bba08f-74da-4f86-bcf1-c4239aa581b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set configuration\n",
    "MODEL_SOURCE_ID = 'huggingface-llm-mistral-small-24B-Instruct-2501'\n",
    "MODEL_SOURCE_ARN = 'arn:aws:sagemaker:{region}:aws:hub-content/SageMakerPublicHub/Model/huggingface-llm-mistral-small-24B-Instruct-2501/2.0.3'\n",
    "INSTANCE_TYPE = 'ml.g6.12xlarge'\n",
    "ENDPOINT_NAME = 'mistral-small-serverfull'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6832bb5-351b-4cc3-a6f2-0b2ed9c43fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to grab aws account id, sagemaker execution role and region\n",
    "def get_current_session_info():\n",
    "    sagemaker_role_arn = sagemaker.get_execution_role()\n",
    "    session = sagemaker.Session()\n",
    "    account_id = session.account_id()\n",
    "    region = session._region_name\n",
    "\n",
    "    return account_id, region, sagemaker_role_arn\n",
    "\n",
    "aws_account_id, aws_region, sagemaker_role_arn = get_current_session_info()\n",
    "\n",
    "print(f'aws region: {aws_region}')\n",
    "\n",
    "MODEL_SOURCE_ARN = MODEL_SOURCE_ARN.format(region=aws_region)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9e0b45-2ce3-4569-80f9-df2680313ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bedrock client object\n",
    "bedrock_client = boto3.client('bedrock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f325afac-6448-44c6-8b0a-366949c4d146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bedrock marketplace endpoint\n",
    "def create_endpoint(model_source_arn: str, \n",
    "                    endpoint_name: str,\n",
    "                    instance_type: str, \n",
    "                    instance_count: int = 1):\n",
    "\n",
    "    response = bedrock_client.create_marketplace_model_endpoint(\n",
    "            modelSourceIdentifier=model_source_arn,\n",
    "            endpointConfig={\n",
    "                'sageMaker': {\n",
    "                    'initialInstanceCount': instance_count,\n",
    "                    'instanceType': instance_type,\n",
    "                    'executionRole': sagemaker_role_arn,\n",
    "                }\n",
    "            },\n",
    "            acceptEula=True,\n",
    "            endpointName=endpoint_name\n",
    "        )\n",
    "    return response\n",
    "\n",
    "create_response = create_endpoint(model_source_arn=MODEL_SOURCE_ARN, endpoint_name=ENDPOINT_NAME, instance_type=INSTANCE_TYPE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8558c05c-1d1b-43bf-a2d6-e9d450d15752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bedrock runtime object\n",
    "\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\")\n",
    "\n",
    "# Helper function to invoke model using Converse API (without streaming)\n",
    "def invoke_model(system_prompt: str, messages: list, display_usage=False):\n",
    "    system = [ { \"text\": system_prompt } ]\n",
    "    \n",
    "    inf_params = {\"max_tokens\": 2000, \"temperature\": 1.0}\n",
    "    \n",
    "    response = bedrock_runtime.converse(modelId=endpoint_arn, \n",
    "                                        messages=messages,\n",
    "                                        system=system,\n",
    "                                        additionalModelRequestFields=inf_params)\n",
    "\n",
    "    # Print Response\n",
    "    output_message = response['output']['message']\n",
    "    output_content = ''\n",
    "    for content in output_message['content']:\n",
    "        output_content = output_content.join(content['text'])\n",
    "\n",
    "    if (display_usage):\n",
    "        token_usage = response['usage']\n",
    "        print(\"\\t--- Token Usage ---\")\n",
    "        print(f\"\\tInput tokens:  {token_usage['inputTokens']}\")\n",
    "        print(f\"\\tOutput tokens:  {token_usage['outputTokens']}\")\n",
    "        print(f\"\\tTotal tokens:  {token_usage['totalTokens']}\")\n",
    "        \n",
    "        print(f\"\\tLatency: {response['metrics']['latencyMs']}\")\n",
    "\n",
    "    \n",
    "    return output_content\n",
    "    \n",
    "# Helper function to invoke model using Converse API with streaming\n",
    "def invoke_model_with_stream(system_prompt: str, messages: list):\n",
    "    system = [ { \"text\": system_prompt } ]\n",
    "    \n",
    "    inf_params = {\"max_tokens\": 2000, \"temperature\": 0.4}\n",
    "    \n",
    "    response = bedrock_runtime.converse_stream(modelId=endpoint_arn, \n",
    "                                        messages=messages,\n",
    "                                        system=system,\n",
    "                                        additionalModelRequestFields=inf_params)\n",
    "    stream = response.get('stream')\n",
    "    output_content = ''\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "\n",
    "            if 'messageStart' in event:\n",
    "                print(f\"\\nRole: {event['messageStart']['role']}\")\n",
    "\n",
    "            if 'contentBlockDelta' in event:\n",
    "                print(event['contentBlockDelta']['delta']['text'], end=\"\")\n",
    "                output_content = output_content.join(event['contentBlockDelta']['delta']['text'])\n",
    "\n",
    "            if 'messageStop' in event:\n",
    "                print(f\"\\nStop reason: {event['messageStop']['stopReason']}\")\n",
    "\n",
    "            if 'metadata' in event:\n",
    "                print(event)\n",
    "                metadata = event['metadata']\n",
    "                if 'usage' in metadata:\n",
    "                    print(\"\\nToken usage\")\n",
    "                    print(f\"Input tokens: {metadata['usage']['inputTokens']}\")\n",
    "                    print(\n",
    "                        f\":Output tokens: {metadata['usage']['outputTokens']}\")\n",
    "                    print(f\":Total tokens: {metadata['usage']['totalTokens']}\")\n",
    "                if 'metrics' in event['metadata']:\n",
    "                    print(\n",
    "                        f\"Latency: {metadata['metrics']['latencyMs']} milliseconds\")\n",
    "\n",
    "    return output_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9d044c-d681-4331-a190-2aa4b985823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve endpoint arn from response text\n",
    "\n",
    "endpoint_arn = create_response['marketplaceModelEndpoint']['endpointArn']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb7b165-4ee9-471a-a463-95ec61fd1e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check endpoint creation status until it's in service\n",
    "\n",
    "while(True):\n",
    "    endpoint_reponse = bedrock_client.get_marketplace_model_endpoint(endpointArn=endpoint_arn)\n",
    "    status = endpoint_reponse['marketplaceModelEndpoint']['endpointStatus']\n",
    "    print(f'endpoint status: {status}')\n",
    "    if (status != 'Creating'):\n",
    "        break\n",
    "\n",
    "    # wait for 10 seconds\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aefa1df-5c75-44a6-bba3-9ca7c9103c7a",
   "metadata": {},
   "source": [
    "## Usage Cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91279802-11e8-43a9-8978-340962d15664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bedrock runtime object\n",
    "\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\")\n",
    "\n",
    "\n",
    "# Helper function to invoke model using Converse API (without streaming)\n",
    "def invoke_model(system_prompt: str, messages: list, display_usage=False):\n",
    "    system = [ { \"text\": system_prompt } ]\n",
    "    \n",
    "    inf_params = {\"max_tokens\": 2000, \"temperature\": 1.0}\n",
    "    \n",
    "    response = bedrock_runtime.converse(modelId=endpoint_arn, \n",
    "                                        messages=messages,\n",
    "                                        system=system,\n",
    "                                        additionalModelRequestFields=inf_params)\n",
    "\n",
    "    # Print Response\n",
    "    output_message = response['output']['message']\n",
    "    output_content = ''\n",
    "    for content in output_message['content']:\n",
    "        output_content = output_content.join(content['text'])\n",
    "\n",
    "    if (display_usage):\n",
    "        token_usage = response['usage']\n",
    "        print(\"\\t--- Token Usage ---\")\n",
    "        print(f\"\\tInput tokens:  {token_usage['inputTokens']}\")\n",
    "        print(f\"\\tOutput tokens:  {token_usage['outputTokens']}\")\n",
    "        print(f\"\\tTotal tokens:  {token_usage['totalTokens']}\")\n",
    "        \n",
    "        print(f\"\\tLatency: {response['metrics']['latencyMs']}\")\n",
    "\n",
    "    \n",
    "    return output_content\n",
    "\n",
    "# Helper function to invoke model using Converse API with streaming\n",
    "def invoke_model_with_stream(system_prompt: str, messages: list):\n",
    "    system = [ { \"text\": system_prompt } ]\n",
    "    \n",
    "    inf_params = {\"max_tokens\": 2000, \"temperature\": 0.4}\n",
    "    \n",
    "    response = bedrock_runtime.converse_stream(modelId=endpoint_arn, \n",
    "                                        messages=messages,\n",
    "                                        system=system,\n",
    "                                        additionalModelRequestFields=inf_params)\n",
    "    stream = response.get('stream')\n",
    "    output_content = ''\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "\n",
    "            if 'messageStart' in event:\n",
    "                print(f\"\\nRole: {event['messageStart']['role']}\")\n",
    "\n",
    "            if 'contentBlockDelta' in event:\n",
    "                print(event['contentBlockDelta']['delta']['text'], end=\"\")\n",
    "                output_content = output_content.join(event['contentBlockDelta']['delta']['text'])\n",
    "\n",
    "            if 'messageStop' in event:\n",
    "                print(f\"\\nStop reason: {event['messageStop']['stopReason']}\")\n",
    "\n",
    "            if 'metadata' in event:\n",
    "                print(event)\n",
    "                metadata = event['metadata']\n",
    "                if 'usage' in metadata:\n",
    "                    print(\"\\nToken usage\")\n",
    "                    print(f\"Input tokens: {metadata['usage']['inputTokens']}\")\n",
    "                    print(\n",
    "                        f\":Output tokens: {metadata['usage']['outputTokens']}\")\n",
    "                    print(f\":Total tokens: {metadata['usage']['totalTokens']}\")\n",
    "                if 'metrics' in event['metadata']:\n",
    "                    print(\n",
    "                        f\"Latency: {metadata['metrics']['latencyMs']} milliseconds\")\n",
    "\n",
    "    return output_content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aec628f-3b0d-4ed6-9fba-b55d273e825f",
   "metadata": {},
   "source": [
    "### Text Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6165f1d-a6c4-43c5-9a3f-87706d567295",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''\n",
    "You are an AI assistant tasked with classifying data based on its sensitivity level. The sensitivity levels and their definitions are:\n",
    "\n",
    "Sensitive: Data that is to have the most limited access and requires a high degree of integrity. This is typically data that will do the most damage to the organization should it be disclosed.\n",
    "Confidential: Data that might be less restrictive within the company but might cause damage if disclosed.\n",
    "Private: Private data is usually compartmental data that might not do the company damage but must be kept private for other reasons. Human resources data is one example of data that can be classified as private.\n",
    "Proprietary: Proprietary data is data that is disclosed outside the company on a limited basis or contains information that could reduce the company's competitive advantage, such as the technical specifications of a new product.\n",
    "Public: Public data is the least sensitive data used by the company and would cause the least harm if disclosed. This could be anything from data used for marketing to the number of employees in the company.\n",
    "\n",
    "For each user inquery provided, classify it into one of the above sensitivity levels. Do not include the word \"Category\". Do not provide explanations or notes.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fbf7f2-c6bb-461d-9fcc-8a4371ace0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": \"I'm an HR recruiter. What data classifiction category are resumes gathered based on referral by employees?\"}]\n",
    "    }\n",
    "]\n",
    "\n",
    "invoke_model_with_stream(system_prompt, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770beff2-bdfb-434c-be96-aa68e980e968",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": \"I require the financial statements for the past three fiscal years.\"}]\n",
    "    }\n",
    "]\n",
    "invoke_model_with_stream(system_prompt, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a3a020-b007-4336-9386-b3ba6af6dd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": \"Company's source code\"}]\n",
    "    }\n",
    "]\n",
    "invoke_model_with_stream(system_prompt, messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af04964-10a7-4aae-9462-d86cd4957005",
   "metadata": {},
   "source": [
    "### Fraudulent Call Detection\n",
    "\n",
    "Mistral Small 3 analyzes transcripts of suspicious phone calls, identifying common deception tactics and social engineering patterns used by scammers. The system automatically flags potential fraud indicators - like urgency manipulation, impersonation of authority figures, and unusual payment requests - helping financial institutions and call centers rapidly detect and respond to emerging scam attempts while protecting vulnerable customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a93d20-7153-475b-8745-66c2fdc36f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''\n",
    "Please analyze this call/message for potential scam activity. Rate each indicator as 1 (present) or 0 (absent):\n",
    "\n",
    "[ID] Missing/incomplete identification (name/company/ID): [0/1]\n",
    "[OFFER] Suspicious offers or too-good-to-be-true promises: [0/1]\n",
    "[VAGUE] Non-specific references instead of account details: [0/1]\n",
    "[REDIRECT] Unsolicited direction to unofficial channels: [0/1]\n",
    "[URGENT] Pressure tactics or urgent deadlines: [0/1]\n",
    "Total flags: [X/5]\n",
    "Brief analysis: [1-2 sentence conclusion]'''\n",
    "\n",
    "user_prompt = ''' \n",
    "Hi there, this is Jessie calling in regards to your Honda warranty. The warranty is up for renewal. \n",
    "I’d like to congratulate you on your $2,000 instant rebate and free maintenance and oil change package for being a loyal customer. \n",
    "Call me back at 934-153-XXXX to redeem now. Once again that number was 934-153-XXXX. Thank you so much. Have a great day. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bebfab2-65ae-4353-9bfd-7f91d0b9893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [{\"text\": user_prompt}]}\n",
    "]\n",
    "invoke_model_with_stream(system_prompt, messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08483563-5a3d-4a07-9a8e-3ed8666c4c55",
   "metadata": {},
   "source": [
    "### Virtual Customer Service\n",
    "The customer service demonstration showcases Mistral Small 3's prowess in maintaining contextual awareness during technical support conversations. We test its ability to provide accurate AWS-specific guidance while maintaining conversation flow and context. This example particularly highlights the model's fast-response capabilities and efficient memory management in multi-turn dialogues, essential features for real-world customer support applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6f4203-d7a7-43d0-aedc-d31d6b78cb82",
   "metadata": {},
   "source": [
    "## 3. Building a Pipeline of Open Models Combining Multiple AWS Services:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582ebbc8-0806-48ec-9c51-4c70443db6bd",
   "metadata": {},
   "source": [
    "### Multimodal Content Generation for Technical Documentation\n",
    "\n",
    "Using a combination of a Vision Language Model (VLM) and a Language Model (LLM) can be a powerful approach for generating comprehensive and user-friendly technical documentation. The VLM serves as the visual processing engine, capable of analyzing and extracting relevant information from various types of technical images, diagrams, and schematics. This could include identifying key components, systems, and features present in the visual content.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c24767-daff-4057-92cb-fce75b44180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision Language Model (VLM) Prompt\n",
    "vlm_prompt = \"\"\"\n",
    "Analyze the provided technical image and identify the key components, systems, and features present. For each identified element, provide a concise label or description that accurately represents its function and role within the overall system or design.\n",
    "\"\"\"\n",
    "\n",
    "# Language Model (LLM) Prompt\n",
    "llm_prompt = \"\"\"\n",
    "Based on the technical elements identified in the provided image, please generate a detailed, user-friendly explanation of the overall system or device. The explanation should cover the purpose, functionality, and interrelationships of the key components in a clear and concise manner, suitable for a non-technical audience.\n",
    "\"\"\"\n",
    "\n",
    "def get_vlm_response(image_path):\n",
    "    \"\"\"\n",
    "    Function to call the Vision Language Model and extract the technical components.\n",
    "    \"\"\"\n",
    "    # Call the VLM and get the response\n",
    "    vlm_response = get_mistral_response(vlm_prompt, image_path, temperature=0.1)\n",
    "    return vlm_response\n",
    "\n",
    "def get_llm_response(vlm_response):\n",
    "    \"\"\"\n",
    "    Function to call the Language Model and generate the technical documentation.\n",
    "    \"\"\"\n",
    "    # Call the LLM and get the response\n",
    "    llm_messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": vlm_response}]\n",
    "        }\n",
    "    ]\n",
    "    invoke_model_with_stream(llm_prompt, llm_messages)\n",
    "\n",
    "def run_pipeline(image_path):\n",
    "    \"\"\"\n",
    "    Main function to run the pipeline.\n",
    "    \"\"\"\n",
    "    # Get the VLM response\n",
    "    vlm_response = get_vlm_response(image_path)\n",
    "    print(\"VLM Response:\")\n",
    "    print(vlm_response)\n",
    "\n",
    "    # Get the LLM response\n",
    "    get_llm_response(vlm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffddb5db-a306-494e-b7f2-847ae122d45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "image_path = \"technical_diagram.jpg\"\n",
    "run_pipeline(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0087e74-d0a6-4cf6-a228-4d4f3825db54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "image_path = \"robotics.png\"\n",
    "run_pipeline(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25627857-a212-458f-8636-a8ad1d15f0cb",
   "metadata": {},
   "source": [
    "### Agentic Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5533a429-90a4-4e0e-b0f0-4011b96ce510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554d10f3-8cb4-40cd-8508-0fc97e4086f0",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3826cd2d-2c41-4c05-9d1a-94180b9d0ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_client.delete_marketplace_model_endpoint(endpointArn=endpoint_arn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
