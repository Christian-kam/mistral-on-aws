{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7c2a0c3-8d15-4b44-9b2e-7d1f3e8f91a4",
   "metadata": {},
   "source": [
    "# Workshop 2: MCP Chat Application Workshop\n",
    "## Building AI Assistants with Mistral models on Amazon Bedrock & Model Context Protocol\n",
    "\n",
    "Welcome to this hands-on workshop! By the end of this session, you'll have built a fully functional AI chat application that connects Amazon Bedrock models with external tools via the Model Context Protocol (MCP).\n",
    "\n",
    "### What You'll Learn:\n",
    "- How to connect AI models to MCP servers and external tools\n",
    "- Working with Mistral models on Amazon Bedrock\n",
    "- Building chat interfaces using Gradio\n",
    "\n",
    "\n",
    "### Workshop Overview:\n",
    "1. **Setup & Configuration** - Get your environment ready\n",
    "2. **Understanding MCP** - Learn about Model Context Protocol\n",
    "3. **Building the Core Chat** - Create a command-line chat interface\n",
    "4. **Adding a Web Interface** - Build a Gradio-based web app\n",
    "5. **Testing & Experimentation** - Try out your application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-explanation",
   "metadata": {},
   "source": [
    "## Step 1: Configuration Files\n",
    "\n",
    "Our application uses configuration files to manage settings. Let's create and understand these configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "config-files",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T13:45:06.790407Z",
     "iopub.status.busy": "2025-06-30T13:45:06.790247Z",
     "iopub.status.idle": "2025-06-30T13:45:06.796023Z",
     "shell.execute_reply": "2025-06-30T13:45:06.795582Z",
     "shell.execute_reply.started": "2025-06-30T13:45:06.790390Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration files created!\n",
      "üìÅ config.py - AWS Bedrock settings\n",
      "üìÅ server_configs.py - MCP server configurations\n"
     ]
    }
   ],
   "source": [
    "# Let's create our configuration files\n",
    "\n",
    "# AWS Configuration\n",
    "aws_config_content = '''\n",
    "\"\"\"AWS Bedrock configuration settings.\"\"\"\n",
    "import os\n",
    "\n",
    "AWS_CONFIG = {\n",
    "    \"region\": \"us-west-2\", #change region if needed\n",
    "    \"model_id\": \"us.mistral.pixtral-large-2502-v1:0\",  # # EU region use: eu.mistral.pixtral-large-2502-v1:0\n",
    "}\n",
    "'''\n",
    "\n",
    "\n",
    "# MCP Server Configuration  \n",
    "server_config_content = '''\n",
    "\"\"\"MCP Server configurations for different tools.\"\"\"\n",
    "from mcp import StdioServerParameters\n",
    "import boto3\n",
    "\n",
    "# Configuration for different MCP servers\n",
    "SERVER_CONFIGS = [\n",
    "    # Time utilities server\n",
    "    StdioServerParameters(\n",
    "        command=\"npx\",\n",
    "        args=[\"-y\", \"time-mcp\"]\n",
    "    ),\n",
    "    # AWS documents \n",
    "    StdioServerParameters(\n",
    "        command=\"uvx\",\n",
    "        args=[\"awslabs.aws-documentation-mcp-server@latest\"],\n",
    "        env= {\n",
    "        \"FASTMCP_LOG_LEVEL\": \"ERROR\",\n",
    "        \"AWS_DOCUMENTATION_PARTITION\": \"aws\"\n",
    "      },\n",
    "    ),\n",
    "    # Calculator\n",
    "    StdioServerParameters(\n",
    "        command=\"uvx\",\n",
    "        args=[\"mcp-server-calculator\"],\n",
    "    ),\n",
    "    # Uncomment and configure if you have Google Maps API key\n",
    "    # StdioServerParameters(\n",
    "    #     command=\"npx\",\n",
    "    #     args=[\"-y\", \"@modelcontextprotocol/server-google-maps\"],\n",
    "    #     env={\"GOOGLE_MAPS_API_KEY\": \"<GOOGLE_MAPS_API>\"} # \n",
    "    # ),\n",
    "]\n",
    "'''\n",
    "\n",
    "# Write configuration files\n",
    "with open('config.py', 'w') as f:\n",
    "    f.write(aws_config_content)\n",
    "    \n",
    "with open('server_configs.py', 'w') as f:\n",
    "    f.write(server_config_content)\n",
    "    \n",
    "print(\"‚úÖ Configuration files created!\")\n",
    "print(\"üìÅ config.py - AWS Bedrock settings\")\n",
    "print(\"üìÅ server_configs.py - MCP server configurations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mcp-explanation",
   "metadata": {},
   "source": [
    "## Step 3: Understanding Model Context Protocol (MCP) and building MCP components \n",
    "\n",
    "**What is MCP?**\n",
    "The Model Context Protocol is a standard protocol that allows AI models to  connect to external tools and data sources. \n",
    "\n",
    "\n",
    "**How it works:**\n",
    "1. **MCP Host**:Maintains conversation, initiates MCP clients and has helper functions such as supporting image processing.\n",
    "2. **MCP Clients**: Communication component within the host that connects to MCP servers.\n",
    "3. **MCP Servers**: Connect to specific data sources (files, APIs, databases) and serve that data back through the protocol, providing specific tools (like time, maps,file access).\n",
    "4. **Bedrock AI Models**: Provide LLM capabilities to understand user questions, decide which MCP tools to use, and generate the final answer.\n",
    "\n",
    "<img src=\"images/architecture.png\" width=\"800px\" alt=\"Architecture diagram\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24176e3b-4571-4655-a91c-1b32b8e817ab",
   "metadata": {},
   "source": [
    "Let's see what MCP servers and tools are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "mcp-demo",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T13:45:15.711751Z",
     "iopub.status.busy": "2025-06-30T13:45:15.711335Z",
     "iopub.status.idle": "2025-06-30T13:45:16.122777Z",
     "shell.execute_reply": "2025-06-30T13:45:16.122315Z",
     "shell.execute_reply.started": "2025-06-30T13:45:15.711735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ AWS Configuration:\n",
      "   region: us-west-2\n",
      "   model_id: us.mistral.pixtral-large-2502-v1:0\n",
      "\n",
      "üîß MCP Servers Configured: 4\n",
      "   Server 1: -y time-mcp\n",
      "   Server 2: awslabs.aws-documentation-mcp-server@latest\n",
      "   Server 3: mcp-server-calculator\n",
      "   Server 4: -y @modelcontextprotocol/server-google-maps\n"
     ]
    }
   ],
   "source": [
    "from config import AWS_CONFIG\n",
    "from server_configs import SERVER_CONFIGS\n",
    "\n",
    "print(\"ü§ñ AWS Configuration:\")\n",
    "for key, value in AWS_CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(f\"\\nüîß MCP Servers Configured: {len(SERVER_CONFIGS)}\")\n",
    "for i, server in enumerate(SERVER_CONFIGS, 1):\n",
    "    print(f\"   Server {i}: {' '.join(server.args)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "core-chat-explanation",
   "metadata": {},
   "source": [
    "## Next let's set up the Bedrock Model, and MCP clients. \n",
    "\n",
    "1. **BedrockModel** - Connects to Amazon Bedrock for AI language model access (e.g. Mistral Models)\n",
    "2. **MCPClient** -Communicates with MCP servers to access external data sources, APIs, and tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "import-libraries",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T13:45:21.618123Z",
     "iopub.status.busy": "2025-06-30T13:45:21.617481Z",
     "iopub.status.idle": "2025-06-30T13:45:21.767894Z",
     "shell.execute_reply": "2025-06-30T13:45:21.767418Z",
     "shell.execute_reply.started": "2025-06-30T13:45:21.618107Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import asyncio\n",
    "from strands import Agent\n",
    "from strands.tools.mcp import MCPClient\n",
    "from strands.models import BedrockModel\n",
    "from mcp import stdio_client\n",
    "from datetime import datetime\n",
    "from contextlib import ExitStack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bedrock-model",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T13:45:23.808032Z",
     "iopub.status.busy": "2025-06-30T13:45:23.807619Z",
     "iopub.status.idle": "2025-06-30T13:45:23.868922Z",
     "shell.execute_reply": "2025-06-30T13:45:23.868478Z",
     "shell.execute_reply.started": "2025-06-30T13:45:23.808016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing Bedrock model: us.mistral.pixtral-large-2502-v1:0\n",
      "‚úÖ Bedrock model initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create the Bedrock model\n",
    "\n",
    "model_id = AWS_CONFIG[\"model_id\"]\n",
    "print(f\"üöÄ Initializing Bedrock model: {model_id}\")\n",
    "\n",
    "bedrock_model = BedrockModel(\n",
    "    model_id=model_id,\n",
    "    streaming=False  # We'll use non-streaming for Mistral models\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Bedrock model initialized successfully!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mcp-connection",
   "metadata": {},
   "source": [
    "Now let's connect to our MCP tool servers and see what tools they provide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mcp-setup",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T13:45:26.395035Z",
     "iopub.status.busy": "2025-06-30T13:45:26.394632Z",
     "iopub.status.idle": "2025-06-30T13:45:30.970153Z",
     "shell.execute_reply": "2025-06-30T13:45:30.969698Z",
     "shell.execute_reply.started": "2025-06-30T13:45:26.395018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Setting up MCP clients...\n",
      "‚úÖ Connected to MCP server 1\n",
      "‚úÖ Connected to MCP server 2\n",
      "‚úÖ Connected to MCP server 3\n",
      "‚úÖ Connected to MCP server 4\n",
      "üî® Loaded 6 tools from server 1\n",
      "üî® Loaded 3 tools from server 2\n",
      "üî® Loaded 1 tools from server 3\n",
      "üî® Loaded 7 tools from server 4\n",
      "\n",
      "üéâ Total tools available: 17\n",
      "\n",
      "üîß Available Tools:\n",
      "   ‚Ä¢ current_time\n",
      "   ‚Ä¢ relative_time\n",
      "   ‚Ä¢ days_in_month\n",
      "   ‚Ä¢ get_timestamp\n",
      "   ‚Ä¢ convert_time\n",
      "   ‚Ä¢ get_week_year\n",
      "   ‚Ä¢ read_documentation\n",
      "   ‚Ä¢ search_documentation\n",
      "   ‚Ä¢ recommend\n",
      "   ‚Ä¢ calculate\n",
      "   ‚Ä¢ maps_geocode\n",
      "   ‚Ä¢ maps_reverse_geocode\n",
      "   ‚Ä¢ maps_search_places\n",
      "   ‚Ä¢ maps_place_details\n",
      "   ‚Ä¢ maps_distance_matrix\n",
      "   ‚Ä¢ maps_elevation\n",
      "   ‚Ä¢ maps_directions\n"
     ]
    }
   ],
   "source": [
    "# Set up MCP clients and tools\n",
    "print(\"üîß Setting up MCP clients...\")\n",
    "\n",
    "# Create MCP clients for each server configuration\n",
    "mcp_clients = [\n",
    "    MCPClient(lambda cfg=server_config: stdio_client(cfg))\n",
    "    for server_config in SERVER_CONFIGS\n",
    "]\n",
    "\n",
    "\n",
    "# Use ExitStack to manage the lifecycle of our MCP connections\n",
    "exit_stack = ExitStack()\n",
    "\n",
    "try:\n",
    "    # Connect to all MCP servers\n",
    "    for i, mcp_client in enumerate(mcp_clients):\n",
    "        exit_stack.enter_context(mcp_client)\n",
    "        print(f\"‚úÖ Connected to MCP server {i+1}\")\n",
    "    \n",
    "    # Collect all available tools\n",
    "    tools = []\n",
    "    for i, mcp_client in enumerate(mcp_clients):\n",
    "        try:\n",
    "            client_tools = mcp_client.list_tools_sync()\n",
    "            tools.extend(client_tools)\n",
    "            print(f\"üî® Loaded {len(client_tools)} tools from server {i+1}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error getting tools from server {i+1}: {e}\")\n",
    "    \n",
    "    print(f\"\\nüéâ Total tools available: {len(tools)}\")\n",
    "    \n",
    "    # Display available tools\n",
    "    if tools:\n",
    "        print(\"\\nüîß Available Tools:\")\n",
    "        for tool in tools:\n",
    "            tool_spec = tool.tool_spec\n",
    "            # print(f\"   ‚Ä¢ {tool_spec['name']}: {tool_spec['description']}\")\n",
    "            print(f\"   ‚Ä¢ {tool_spec['name']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error setting up MCP tools: {e}\")\n",
    "    tools = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent-creation",
   "metadata": {},
   "source": [
    "### Creating the Strands Agent and chat loop \n",
    "\n",
    "Now we combine our Bedrock model with the MCP tools to create an intelligent Strands agent:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "create-agent",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T13:46:17.587600Z",
     "iopub.status.busy": "2025-06-30T13:46:17.587193Z",
     "iopub.status.idle": "2025-06-30T13:46:17.591226Z",
     "shell.execute_reply": "2025-06-30T13:46:17.590756Z",
     "shell.execute_reply.started": "2025-06-30T13:46:17.587585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ AI Agent created successfully!\n",
      "üß† Agent has access to 17 tools\n"
     ]
    }
   ],
   "source": [
    "# Create the AI agent\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant that can use tools to help you answer questions and perform tasks.\n",
    "Be friendly, helpful, and make use of your tools when appropriate.\n",
    "\"\"\"\n",
    "\n",
    "agent = Agent(\n",
    "    model=bedrock_model, \n",
    "    tools=tools,\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "print(\"ü§ñ AI Agent created successfully!\")\n",
    "print(f\"üß† Agent has access to {len(tools)} tools\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-agent",
   "metadata": {},
   "source": [
    "Let's test our agent with a simple chat loop to make sure everything is working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7df4f6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(\"üí¨ Chat with the assistant. Type 'quit' to exit.\\n\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"üë§ User: \")\n",
    "    if user_input.strip().lower() == \"quit\":\n",
    "        print(\"üëã Goodbye!\")\n",
    "        break\n",
    "\n",
    "    # Replace this with your own agent/response logic\n",
    "    response = agent(user_input)\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    print(f\"ü§ñ Agent: {response}\")\n",
    "    print(\"-\" * 120)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311d67f2-01ec-4abf-838b-74c3a61f09af",
   "metadata": {},
   "source": [
    "### test questions: \n",
    "- Suggest the top 3 stores in London.\n",
    "- I'm at the British Museum. How can I go to Liberty by bus?\n",
    "- Can you check the documents and give me the Mistral model IDs that supported on Amazon Bedrock?\n",
    "- What is 1234 square root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gradio-section",
   "metadata": {},
   "source": [
    "## Step 4: Building a Web Interface with Gradio\n",
    "\n",
    "Now let's take our chat application to the next level by creating a beautiful web interface using Gradio. This will allow users to:\n",
    "\n",
    "- üí¨ Chat with the AI in a web browser\n",
    "- üñºÔ∏è Upload and analyze images\n",
    "- üîß View available tools and configuration\n",
    "- üì± Access from any device\n",
    "\n",
    "### Understanding Gradio\n",
    "\n",
    "Gradio is a Python library that makes it easy to create web interfaces for machine learning models and applications. With just a few lines of code, we can create:\n",
    "- Chat interfaces\n",
    "- File upload areas\n",
    "- Information panels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "gradio-imports",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T13:47:44.835686Z",
     "iopub.status.busy": "2025-06-30T13:47:44.835273Z",
     "iopub.status.idle": "2025-06-30T13:47:46.276272Z",
     "shell.execute_reply": "2025-06-30T13:47:46.275767Z",
     "shell.execute_reply.started": "2025-06-30T13:47:44.835669Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Gradio and other necessary libraries for the web interface\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import base64\n",
    "import io\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gradio-functions",
   "metadata": {},
   "source": [
    "### Helper Functions for the Web Interface\n",
    "\n",
    "Let's create some helper functions for our web interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "web-helpers",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T13:47:54.165080Z",
     "iopub.status.busy": "2025-06-30T13:47:54.164519Z",
     "iopub.status.idle": "2025-06-30T13:47:54.173317Z",
     "shell.execute_reply": "2025-06-30T13:47:54.172856Z",
     "shell.execute_reply.started": "2025-06-30T13:47:54.165063Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_available_tools():\n",
    "    \"\"\"Get list of available tools for display\"\"\"\n",
    "    global tools\n",
    "    if not tools:\n",
    "        return \"No tools available yet.\"\n",
    "    \n",
    "    html = \"<h3>Available Tools</h3>\"\n",
    "    for tool in tools:\n",
    "        try:\n",
    "            tool_spec = tool.tool_spec\n",
    "            name = tool_spec.get('name', 'Unknown Tool')\n",
    "            description = tool_spec.get('description', 'No description available')\n",
    "            \n",
    "            html += f\"\"\"\n",
    "            <div style=\"border: 1px solid #ddd; margin-bottom: 10px; padding: 10px; border-radius: 5px;\">\n",
    "                <div style=\"font-weight: bold; color: #2C3E50;\">{name}</div>\n",
    "                <div style=\"margin-top: 5px;\">{description}</div>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error displaying tool: {str(e)}\")\n",
    "            html += f\"\"\"\n",
    "            <div style=\"border: 1px solid #ddd; margin-bottom: 10px; padding: 10px; border-radius: 5px; color: red;\">\n",
    "                <div>Error displaying tool: {str(e)}</div>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "    \n",
    "    return html\n",
    "\n",
    "\n",
    "def convert_image_to_bytes(image):\n",
    "    \"\"\"Convert PIL Image to bytes for Bedrock message format\"\"\"\n",
    "    if image is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Convert PIL Image to bytes\n",
    "        buffered = io.BytesIO()\n",
    "        # Determine format based on image\n",
    "        format_type = image.format if image.format else 'PNG'\n",
    "        if format_type not in ['PNG', 'JPEG', 'JPG']:\n",
    "            format_type = 'PNG'\n",
    "        \n",
    "        image.save(buffered, format=format_type)\n",
    "        image_bytes = buffered.getvalue()\n",
    "        \n",
    "        return {\n",
    "            'bytes': image_bytes,\n",
    "            'format': format_type.lower()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error converting image to bytes: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_message(message, image=None):\n",
    "    \"\"\"Process a message from the user and get a response from the agent\"\"\"\n",
    "    global agent, tools\n",
    "    \n",
    "    if agent is None:\n",
    "        # First-time initialization\n",
    "        if not initialize_agent():\n",
    "            return \"Error: Failed to initialize the agent. Please check the logs.\"\n",
    "    \n",
    "    try:\n",
    "        # Handle image input by appending message to agent\n",
    "        if image is not None:\n",
    "            # Convert image to bytes\n",
    "            image_data = convert_image_to_bytes(image)\n",
    "            if image_data:\n",
    "                # Create message with image and text content\n",
    "                new_message = {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"image\": {\n",
    "                                \"format\": image_data['format'],\n",
    "                                \"source\": {\n",
    "                                    \"bytes\": image_data['bytes']\n",
    "                                }\n",
    "                            }\n",
    "                        },\n",
    "                        {\n",
    "                            \"text\": message if message.strip() else \"Please analyze the content of the image.\"\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "                \n",
    "                # Append the new message to the agent's messages\n",
    "                agent.messages.append(new_message)\n",
    "                \n",
    "                # Get response from agent\n",
    "                response = agent(message)\n",
    "            else:\n",
    "                # Fallback to text-only if image conversion failed\n",
    "                response = agent(message)\n",
    "        else:\n",
    "            # Text-only message\n",
    "            response = agent(message)\n",
    "        \n",
    "        \n",
    "        # Extract the text content from the agent result\n",
    "        if hasattr(response, 'text'):\n",
    "            display_response = response.text\n",
    "        elif hasattr(response, 'content'):\n",
    "            display_response = response.content\n",
    "        else:\n",
    "            # Fallback to string representation\n",
    "            display_response = str(response)\n",
    "        \n",
    "        return display_response\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing message: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return f\"I encountered an error: {str(e)}\"\n",
    "\n",
    "def reset_conversation():\n",
    "    \"\"\"Reset conversation history\"\"\"\n",
    "    global agent, tool_usage_history\n",
    "    tool_usage_history = []\n",
    "    return [], \"Conversation has been reset.\"\n",
    "\n",
    "def respond(message, chat_history, image=None):\n",
    "    \"\"\"Process the message and update the chat history\"\"\"\n",
    "    if not message.strip() and image is None:\n",
    "        return chat_history, \"\", None\n",
    "    \n",
    "    # Create user message content\n",
    "    user_content = message\n",
    "    if image is not None:\n",
    "        user_content += \" [Image uploaded]\"\n",
    "    \n",
    "    # Add user message to history\n",
    "    chat_history.append({\"role\": \"user\", \"content\": user_content})\n",
    "    \n",
    "    # Process user message with image\n",
    "    bot_response = process_message(message, image)\n",
    "    \n",
    "    # Add assistant response to history\n",
    "    chat_history.append({\"role\": \"assistant\", \"content\": bot_response})\n",
    "    \n",
    "    return chat_history, \"\", None  # Return empty message and clear image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gradio-interface",
   "metadata": {},
   "source": [
    "### Building the Gradio Interface\n",
    "\n",
    "Now let's create our beautiful web interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ae4838b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-30T13:48:00.869545Z",
     "iopub.status.busy": "2025-06-30T13:48:00.869132Z",
     "iopub.status.idle": "2025-06-30T13:48:01.007084Z",
     "shell.execute_reply": "2025-06-30T13:48:01.006593Z",
     "shell.execute_reply.started": "2025-06-30T13:48:00.869529Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the Gradio interface\n",
    "with gr.Blocks(css=\"\"\"\n",
    "    .container {\n",
    "        max-width: 1200px;\n",
    "        margin: auto;\n",
    "    }\n",
    "    .tools-panel {\n",
    "        background-color: #f9f9f9;\n",
    "        border-radius: 10px;\n",
    "        padding: 15px;\n",
    "        margin-top: 15px;\n",
    "    }\n",
    "\"\"\", title=\"MCP Application Demo with Mistral Models on Amazon Bedrock\") as demo:\n",
    "    \n",
    "    gr.HTML(\"\"\"\n",
    "        <div style=\"text-align: center; margin-bottom: 1rem\">\n",
    "            <h1>ü§ñ MCP Application Demo with Mistral Models on Amazon Bedrock</h1>\n",
    "            <p>Chat with AI powered by Mistral models on Amazon Bedrock and MCP tools</p>\n",
    "        </div>\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            # Chat interface\n",
    "            chatbot = gr.Chatbot(\n",
    "                height=500,\n",
    "                show_label=False,\n",
    "                container=True,\n",
    "                show_copy_button=True,\n",
    "                type=\"messages\"\n",
    "            )\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=6):\n",
    "                    msg = gr.Textbox(\n",
    "                        placeholder=\"Type your message here...\",\n",
    "                        show_label=False,\n",
    "                        container=False\n",
    "                    )\n",
    "                with gr.Column(scale=3):\n",
    "                    img_input = gr.Image(\n",
    "                        type=\"pil\",\n",
    "                        label=\"Upload Image\",\n",
    "                        sources=[\"upload\", \"clipboard\"],\n",
    "                    )\n",
    "                submit = gr.Button(\"Send\", scale=1, variant=\"primary\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                clear = gr.Button(\"Clear Chat\", variant=\"secondary\")\n",
    "        \n",
    "        with gr.Column(scale=1):\n",
    "            # Tools panel with tabs\n",
    "            with gr.Tab(\"Available Tools\"):\n",
    "                tools_display = gr.HTML(\n",
    "                    value=get_available_tools,\n",
    "                    show_label=False\n",
    "                )\n",
    "                refresh_tools = gr.Button(\"Refresh Tools\", size=\"sm\")\n",
    "            \n",
    "            with gr.Tab(\"Configuration\"):\n",
    "                gr.Markdown(\"### Current Configuration\")\n",
    "                with gr.Group():\n",
    "                    gr.Textbox(\n",
    "                        label=\"AWS Region\",\n",
    "                        value=AWS_CONFIG[\"region\"],\n",
    "                        interactive=False\n",
    "                    )\n",
    "                    gr.Textbox(\n",
    "                        label=\"Model ID\", \n",
    "                        value=AWS_CONFIG[\"model_id\"],\n",
    "                        interactive=False\n",
    "                    )\n",
    "                    gr.Markdown(\"*To change configuration, edit config.py and restart*\")\n",
    "    \n",
    "    # Event handlers\n",
    "    msg.submit(\n",
    "        fn=respond,\n",
    "        inputs=[msg, chatbot, img_input],\n",
    "        outputs=[chatbot, msg, img_input]\n",
    "    ).then(\n",
    "        fn=get_available_tools,\n",
    "        inputs=None,\n",
    "        outputs=tools_display\n",
    "    )\n",
    "    \n",
    "    submit.click(\n",
    "        fn=respond,\n",
    "        inputs=[msg, chatbot, img_input], \n",
    "        outputs=[chatbot, msg, img_input]\n",
    "    ).then(\n",
    "        fn=get_available_tools,\n",
    "        inputs=None,\n",
    "        outputs=tools_display\n",
    "    )\n",
    "    \n",
    "    clear.click(\n",
    "        fn=reset_conversation,\n",
    "        inputs=None,\n",
    "        outputs=[chatbot, gr.Textbox()]\n",
    "    )\n",
    "    \n",
    "    refresh_tools.click(\n",
    "        fn=get_available_tools,\n",
    "        inputs=None,\n",
    "        outputs=tools_display\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "launch-app",
   "metadata": {},
   "source": [
    "### Launch Your Web Application!\n",
    "\n",
    "Now for the exciting part - let's launch your web application!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e712aba4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "demo.queue()\n",
    "demo.launch(\n",
    "    share=True,\n",
    "    server_name=\"0.0.0.0\",\n",
    "    server_port=7861,\n",
    "    show_error=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "workshop-summary",
   "metadata": {},
   "source": [
    "## üéâ Congratulations! Workshop Complete!\n",
    "\n",
    "You've successfully built a complete AI chat application with:\n",
    "\n",
    "### ‚úÖ What You've Accomplished:\n",
    "1. **Set up** Amazon Bedrock integration\n",
    "2. **Connected** MCP tool servers\n",
    "3. **Created** an intelligent AI agent\n",
    "4. **Deployed** a beautiful web applicatio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
