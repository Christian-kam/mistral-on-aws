AWSTemplateFormatVersion: '2010-09-09'
Description: 'SageMaker Studio Domain with JupyterLab, ml.m5.4xlarge instance, and comprehensive IAM permissions'

Parameters:
  DomainName:
    Type: String
    Default: 'mistral-workshop-domain'
    Description: 'Name for the SageMaker Studio Domain'
  
  UserProfileName:
    Type: String
    Default: 'workshop-user'
    Description: 'Name for the SageMaker Studio User Profile'
  
  DefaultInstanceType:
    Type: String
    Default: 'ml.m5.4xlarge'
    AllowedValues:
      - ml.t3.medium
      - ml.t3.large
      - ml.t3.xlarge
      - ml.t3.2xlarge
      - ml.m5.large
      - ml.m5.xlarge
      - ml.m5.2xlarge
      - ml.m5.4xlarge
      - ml.m5.12xlarge
      - ml.c5.large
      - ml.c5.xlarge
      - ml.c5.2xlarge
      - ml.c5.4xlarge
      - ml.g4dn.xlarge
      - ml.g4dn.2xlarge
      - ml.g4dn.4xlarge
      - ml.g4dn.8xlarge
      - ml.g5.xlarge
      - ml.g5.2xlarge
      - ml.g5.4xlarge
      - ml.g5.8xlarge
      - ml.g5.16xlarge
      - ml.p3.2xlarge
      - ml.p3.8xlarge
      - ml.p3.16xlarge
    Description: 'Default instance type for Kernel Gateway (compute kernels) - users can choose this when creating notebooks'
  
  VpcCidr:
    Type: String
    Default: '10.0.0.0/16'
    Description: 'CIDR block for the VPC'
  
  CreateVpcResources:
    Type: String
    Default: 'Yes'
    AllowedValues:
      - 'Yes'
      - 'No'
    Description: 'Create new VPC and networking resources? Choose No if you want to use existing VPC'
  
  ExistingVpcId:
    Type: String
    Default: ''
    Description: 'Existing VPC ID (only used if CreateVpcResources = No)'
  
  ExistingSubnetIds:
    Type: CommaDelimitedList
    Default: ''
    Description: 'Existing Subnet IDs (only used if CreateVpcResources = No)'

Mappings:
  SageMakerImageArn:
    us-east-1:
      JupyterServer: 'arn:aws:sagemaker:us-east-1:081325390199:image/jupyter-server-3'
      JupyterLab: 'arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-distribution-cpu'
    us-east-2:
      JupyterServer: 'arn:aws:sagemaker:us-east-2:429704687514:image/jupyter-server-3'
      JupyterLab: 'arn:aws:sagemaker:us-east-2:429704687514:image/sagemaker-distribution-cpu'
    us-west-1:
      JupyterServer: 'arn:aws:sagemaker:us-west-1:742091327244:image/jupyter-server-3'
      JupyterLab: 'arn:aws:sagemaker:us-west-1:742091327244:image/sagemaker-distribution-cpu'
    us-west-2:
      JupyterServer: 'arn:aws:sagemaker:us-west-2:236514542706:image/jupyter-server-3'
      JupyterLab: 'arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-distribution-cpu'
    eu-west-1:
      JupyterServer: 'arn:aws:sagemaker:eu-west-1:470317259841:image/jupyter-server-3'
      JupyterLab: 'arn:aws:sagemaker:eu-west-1:470317259841:image/sagemaker-distribution-cpu'
    eu-west-2:
      JupyterServer: 'arn:aws:sagemaker:eu-west-2:712779665605:image/jupyter-server-3'
      JupyterLab: 'arn:aws:sagemaker:eu-west-2:712779665605:image/sagemaker-distribution-cpu'
    eu-central-1:
      JupyterServer: 'arn:aws:sagemaker:eu-central-1:936697816551:image/jupyter-server-3'
      JupyterLab: 'arn:aws:sagemaker:eu-central-1:936697816551:image/sagemaker-distribution-cpu'
    ap-southeast-1:
      JupyterServer: 'arn:aws:sagemaker:ap-southeast-1:492261229750:image/jupyter-server-3'
      JupyterLab: 'arn:aws:sagemaker:ap-southeast-1:492261229750:image/sagemaker-distribution-cpu'
    ap-southeast-2:
      JupyterServer: 'arn:aws:sagemaker:ap-southeast-2:452832661640:image/jupyter-server-3'
      JupyterLab: 'arn:aws:sagemaker:ap-southeast-2:452832661640:image/sagemaker-distribution-cpu'
    ap-northeast-1:
      JupyterServer: 'arn:aws:sagemaker:ap-northeast-1:102112518831:image/jupyter-server-3'
      JupyterLab: 'arn:aws:sagemaker:ap-northeast-1:102112518831:image/sagemaker-distribution-cpu'

Conditions:
  CreateVpc: !Equals [!Ref CreateVpcResources, 'Yes']
  UseExistingVpc: !Equals [!Ref CreateVpcResources, 'No']

Resources:
  # VPC and Networking Resources (created only if CreateVpcResources = Yes)
  WorkshopVPC:
    Type: AWS::EC2::VPC
    Condition: CreateVpc
    Properties:
      CidrBlock: !Ref VpcCidr
      EnableDnsHostnames: true
      EnableDnsSupport: true
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-Workshop-VPC'
        - Key: Purpose
          Value: 'SageMaker Studio Workshop'

  # Internet Gateway
  InternetGateway:
    Type: AWS::EC2::InternetGateway
    Condition: CreateVpc
    Properties:
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-IGW'

  # Attach Internet Gateway to VPC
  InternetGatewayAttachment:
    Type: AWS::EC2::VPCGatewayAttachment
    Condition: CreateVpc
    Properties:
      InternetGatewayId: !Ref InternetGateway
      VpcId: !Ref WorkshopVPC

  # Public Subnet 1 (for NAT Gateway)
  PublicSubnet1:
    Type: AWS::EC2::Subnet
    Condition: CreateVpc
    Properties:
      VpcId: !Ref WorkshopVPC
      AvailabilityZone: !Select [0, !GetAZs '']
      CidrBlock: !Select [0, !Cidr [!Ref VpcCidr, 6, 8]]
      MapPublicIpOnLaunch: true
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-Public-Subnet-1'

  # Public Subnet 2 (for high availability)
  PublicSubnet2:
    Type: AWS::EC2::Subnet
    Condition: CreateVpc
    Properties:
      VpcId: !Ref WorkshopVPC
      AvailabilityZone: !Select [1, !GetAZs '']
      CidrBlock: !Select [1, !Cidr [!Ref VpcCidr, 6, 8]]
      MapPublicIpOnLaunch: true
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-Public-Subnet-2'

  # Private Subnet 1 (for SageMaker Studio)
  PrivateSubnet1:
    Type: AWS::EC2::Subnet
    Condition: CreateVpc
    Properties:
      VpcId: !Ref WorkshopVPC
      AvailabilityZone: !Select [0, !GetAZs '']
      CidrBlock: !Select [2, !Cidr [!Ref VpcCidr, 6, 8]]
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-Private-Subnet-1'

  # Private Subnet 2 (for SageMaker Studio HA)
  PrivateSubnet2:
    Type: AWS::EC2::Subnet
    Condition: CreateVpc
    Properties:
      VpcId: !Ref WorkshopVPC
      AvailabilityZone: !Select [1, !GetAZs '']
      CidrBlock: !Select [3, !Cidr [!Ref VpcCidr, 6, 8]]
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-Private-Subnet-2'

  # Elastic IP for NAT Gateway
  NatGatewayEIP:
    Type: AWS::EC2::EIP
    Condition: CreateVpc
    DependsOn: InternetGatewayAttachment
    Properties:
      Domain: vpc
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-NAT-EIP'

  # NAT Gateway
  NatGateway:
    Type: AWS::EC2::NatGateway
    Condition: CreateVpc
    Properties:
      AllocationId: !GetAtt NatGatewayEIP.AllocationId
      SubnetId: !Ref PublicSubnet1
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-NAT-Gateway'

  # Public Route Table
  PublicRouteTable:
    Type: AWS::EC2::RouteTable
    Condition: CreateVpc
    Properties:
      VpcId: !Ref WorkshopVPC
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-Public-Routes'

  # Public Route to Internet Gateway
  DefaultPublicRoute:
    Type: AWS::EC2::Route
    Condition: CreateVpc
    DependsOn: InternetGatewayAttachment
    Properties:
      RouteTableId: !Ref PublicRouteTable
      DestinationCidrBlock: 0.0.0.0/0
      GatewayId: !Ref InternetGateway

  # Associate Public Subnets with Public Route Table
  PublicSubnet1RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Condition: CreateVpc
    Properties:
      RouteTableId: !Ref PublicRouteTable
      SubnetId: !Ref PublicSubnet1

  PublicSubnet2RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Condition: CreateVpc
    Properties:
      RouteTableId: !Ref PublicRouteTable
      SubnetId: !Ref PublicSubnet2

  # Private Route Table
  PrivateRouteTable:
    Type: AWS::EC2::RouteTable
    Condition: CreateVpc
    Properties:
      VpcId: !Ref WorkshopVPC
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-Private-Routes'

  # Private Route to NAT Gateway
  DefaultPrivateRoute:
    Type: AWS::EC2::Route
    Condition: CreateVpc
    Properties:
      RouteTableId: !Ref PrivateRouteTable
      DestinationCidrBlock: 0.0.0.0/0
      NatGatewayId: !Ref NatGateway

  # Associate Private Subnets with Private Route Table
  PrivateSubnet1RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Condition: CreateVpc
    Properties:
      RouteTableId: !Ref PrivateRouteTable
      SubnetId: !Ref PrivateSubnet1

  PrivateSubnet2RouteTableAssociation:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Condition: CreateVpc
    Properties:
      RouteTableId: !Ref PrivateRouteTable
      SubnetId: !Ref PrivateSubnet2

  # IAM Role for Lambda Cleanup Function
  CleanupLambdaRole:
    Type: AWS::IAM::Role
    DeletionPolicy: Retain
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: CleanupResources
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - ec2:DescribeNetworkInterfaces
                  - ec2:DeleteNetworkInterface
                  - ec2:DetachNetworkInterface
                  - ec2:ModifyNetworkInterfaceAttribute
                  - ec2:DescribeSecurityGroups
                  - ec2:DeleteSecurityGroup
                  - ec2:RevokeSecurityGroupIngress
                  - ec2:RevokeSecurityGroupEgress
                  - ec2:DescribeVpcs
                  - ec2:DescribeRouteTables
                  - ec2:DescribeNatGateways
                  - ec2:DeleteNatGateway
                  - sagemaker:ListApps
                  - sagemaker:DeleteApp
                  - sagemaker:DescribeApp
                  - sagemaker:ListSpaces
                  - sagemaker:DescribeSpace
                  - elasticfilesystem:DescribeFileSystems
                  - elasticfilesystem:DescribeMountTargets
                  - elasticfilesystem:DeleteMountTarget
                  - elasticfilesystem:DeleteFileSystem
                  - s3:ListBucket
                  - s3:ListBucketVersions
                  - s3:DeleteObject
                  - s3:DeleteObjectVersion
                  - s3:DeleteBucket
                Resource: '*'

  # Lambda Function for Cleanup
  CleanupLambdaFunction:
    Type: AWS::Lambda::Function
    DeletionPolicy: Retain
    Properties:
      FunctionName: !Sub '${AWS::StackName}-cleanup-function'
      Runtime: python3.9
      Handler: index.lambda_handler
      Role: !GetAtt CleanupLambdaRole.Arn
      Timeout: 900
      Code:
        ZipFile: |
          import boto3
          import json
          import time
          import cfnresponse
          
          def wait_for_sagemaker_apps_deleted(sagemaker, domain_id, max_wait_time=600):
              """Wait for all SageMaker apps to be deleted"""
              print(f"Waiting for SageMaker apps in domain {domain_id} to be deleted...")
              start_time = time.time()
              
              while time.time() - start_time < max_wait_time:
                  try:
                      apps = sagemaker.list_apps(DomainIdEquals=domain_id)
                      active_apps = [app for app in apps.get('Apps', []) if app.get('Status') not in ['Deleted', 'Failed']]
                      
                      if not active_apps:
                          print("All SageMaker apps have been deleted")
                          return True
                      
                      print(f"Waiting for {len(active_apps)} apps to be deleted...")
                      for app in active_apps:
                          print(f"- {app.get('AppName', 'Unknown')} ({app.get('AppType', 'Unknown')}) - Status: {app.get('Status', 'Unknown')}")
                      
                      time.sleep(30)
                  except Exception as e:
                      print(f"Error checking app status: {e}")
                      time.sleep(30)
              
              print(f"Timeout waiting for SageMaker apps to be deleted after {max_wait_time} seconds")
              return False
          
          def wait_for_network_interfaces_detached(ec2, subnet_ids, max_wait_time=300):
              """Wait for network interfaces to be detached and available for deletion"""
              print("Waiting for network interfaces to be detached...")
              start_time = time.time()
              
              while time.time() - start_time < max_wait_time:
                  total_enis = 0
                  attached_enis = 0
                  
                  for subnet_id in subnet_ids:
                      try:
                          response = ec2.describe_network_interfaces(
                              Filters=[{'Name': 'subnet-id', 'Values': [subnet_id]}]
                          )
                          
                          for eni in response.get('NetworkInterfaces', []):
                              total_enis += 1
                              attachment = eni.get('Attachment')
                              if attachment and attachment.get('Status') in ['attached', 'attaching']:
                                  attached_enis += 1
                                  print(f"ENI {eni['NetworkInterfaceId']} still attached: {attachment}")
                      except Exception as e:
                          print(f"Error checking ENIs in subnet {subnet_id}: {e}")
                  
                  if attached_enis == 0:
                      print(f"All {total_enis} network interfaces are now detached")
                      return True
                  
                  print(f"Waiting for {attached_enis} of {total_enis} network interfaces to detach...")
                  time.sleep(15)
              
              print(f"Timeout waiting for network interfaces to detach after {max_wait_time} seconds")
              return False
          
          def delete_network_interfaces_with_retry(ec2, subnet_ids, max_attempts=5):
              """Delete network interfaces with multiple retry attempts"""
              print("Starting network interface cleanup with retries...")
              
              for attempt in range(max_attempts):
                  print(f"Network interface deletion attempt {attempt + 1}/{max_attempts}")
                  deleted_count = 0
                  remaining_enis = []
                  
                  for subnet_id in subnet_ids:
                      try:
                          response = ec2.describe_network_interfaces(
                              Filters=[{'Name': 'subnet-id', 'Values': [subnet_id]}]
                          )
                          
                          for eni in response.get('NetworkInterfaces', []):
                              eni_id = eni['NetworkInterfaceId']
                              eni_status = eni.get('Status', 'unknown')
                              eni_desc = eni.get('Description', 'N/A')
                              
                              print(f"Processing ENI {eni_id} - Status: {eni_status}, Description: {eni_desc}")
                              
                              # Skip if attached to EC2 instance
                              attachment = eni.get('Attachment')
                              if attachment and attachment.get('InstanceId'):
                                  print(f"ENI {eni_id} attached to EC2 instance {attachment['InstanceId']}, skipping")
                                  continue
                              
                              # Force detach if still attached
                              if attachment and attachment.get('Status') in ['attached', 'attaching']:
                                  attachment_id = attachment.get('AttachmentId')
                                  if attachment_id:
                                      try:
                                          print(f"Force detaching ENI {eni_id} (attachment: {attachment_id})")
                                          ec2.detach_network_interface(
                                              AttachmentId=attachment_id,
                                              Force=True
                                          )
                                          time.sleep(5)  # Brief wait after detach
                                      except Exception as detach_error:
                                          print(f"Error detaching ENI {eni_id}: {detach_error}")
                              
                              # Attempt to delete the ENI
                              try:
                                  ec2.delete_network_interface(NetworkInterfaceId=eni_id)
                                  print(f"Successfully deleted ENI {eni_id}")
                                  deleted_count += 1
                              except Exception as delete_error:
                                  print(f"Error deleting ENI {eni_id}: {delete_error}")
                                  remaining_enis.append(eni_id)
                      
                      except Exception as e:
                          print(f"Error processing subnet {subnet_id}: {e}")
                  
                  print(f"Attempt {attempt + 1}: Deleted {deleted_count} ENIs, {len(remaining_enis)} remaining")
                  
                  if not remaining_enis:
                      print("All network interfaces deleted successfully")
                      return True
                  
                  if attempt < max_attempts - 1:
                      print(f"Waiting 30 seconds before retry...")
                      time.sleep(30)
              
              print(f"Failed to delete {len(remaining_enis)} network interfaces after {max_attempts} attempts")
              return False
          
          def lambda_handler(event, context):
              try:
                  print(f"Event: {json.dumps(event)}")
                  
                  if event['RequestType'] == 'Delete':
                      ec2 = boto3.client('ec2')
                      sagemaker = boto3.client('sagemaker')
                      efs = boto3.client('efs')
                      
                      # Get parameters from event
                      subnet_ids = event['ResourceProperties'].get('SubnetIds', [])
                      domain_id = event['ResourceProperties'].get('DomainId', '')
                      vpc_id = event['ResourceProperties'].get('VpcId', '')
                      s3_bucket_name = event['ResourceProperties'].get('S3BucketName', '')
                      
                      print(f"Cleaning up subnets: {subnet_ids}")
                      print(f"Domain ID: {domain_id}")
                      print(f"VPC ID: {vpc_id}")
                      print(f"S3 Bucket: {s3_bucket_name}")
                      
                      # Step 1: Delete SageMaker Apps and wait for completion
                      if domain_id:
                          try:
                              print("Step 1: Deleting SageMaker Apps...")
                              apps = sagemaker.list_apps(DomainIdEquals=domain_id)
                              for app in apps.get('Apps', []):
                                  if app.get('Status') not in ['Deleted', 'Failed']:
                                      print(f"Deleting app: {app.get('AppName', 'Unknown')} ({app.get('AppType', 'Unknown')})")
                                      try:
                                          sagemaker.delete_app(
                                              DomainId=app['DomainId'],
                                              UserProfileName=app.get('UserProfileName'),
                                              SpaceName=app.get('SpaceName'),
                                              AppType=app['AppType'],
                                              AppName=app['AppName']
                                          )
                                      except Exception as app_error:
                                          print(f"Error deleting app {app.get('AppName', 'Unknown')}: {app_error}")
                              
                              # Wait for all apps to be deleted
                              wait_for_sagemaker_apps_deleted(sagemaker, domain_id)
                              
                          except Exception as e:
                              print(f"Error in SageMaker app cleanup: {e}")
                      
                      # Step 2: Clean up S3 Bucket
                      if s3_bucket_name:
                          try:
                              print("Step 2: Cleaning up S3 Bucket...")
                              s3 = boto3.client('s3')
                              
                              # Delete all objects in the bucket
                              print(f"Deleting all objects from bucket {s3_bucket_name}")
                              
                              # List and delete all object versions
                              paginator = s3.get_paginator('list_object_versions')
                              for page in paginator.paginate(Bucket=s3_bucket_name):
                                  delete_list = []
                                  
                                  # Add current versions
                                  if 'Versions' in page:
                                      for version in page['Versions']:
                                          delete_list.append({
                                              'Key': version['Key'],
                                              'VersionId': version['VersionId']
                                          })
                                  
                                  # Add delete markers
                                  if 'DeleteMarkers' in page:
                                      for marker in page['DeleteMarkers']:
                                          delete_list.append({
                                              'Key': marker['Key'],
                                              'VersionId': marker['VersionId']
                                          })
                                  
                                  # Delete objects in batches
                                  if delete_list:
                                      print(f"Deleting {len(delete_list)} objects/versions")
                                      s3.delete_objects(
                                          Bucket=s3_bucket_name,
                                          Delete={'Objects': delete_list}
                                      )
                              
                              print(f"All objects deleted from bucket {s3_bucket_name}")
                              
                          except Exception as e:
                              print(f"Error cleaning up S3 bucket: {e}")
                      
                      # Step 3: Delete EFS Mount Targets
                      print("Step 3: Cleaning up EFS Mount Targets...")
                      efs_to_delete = []
                      for subnet_id in subnet_ids:
                          try:
                              print(f"Checking EFS mount targets in subnet {subnet_id}")
                              filesystems = efs.describe_file_systems()
                              
                              for filesystem in filesystems['FileSystems']:
                                  fs_id = filesystem['FileSystemId']
                                  try:
                                      mount_targets = efs.describe_mount_targets(FileSystemId=fs_id)
                                      
                                      for mt in mount_targets['MountTargets']:
                                          if mt['SubnetId'] == subnet_id:
                                              mt_id = mt['MountTargetId']
                                              print(f"Deleting EFS mount target {mt_id} for filesystem {fs_id} in subnet {subnet_id}")
                                              efs.delete_mount_target(MountTargetId=mt_id)
                                              
                                              if fs_id not in efs_to_delete:
                                                  efs_to_delete.append(fs_id)
                                  except Exception as mt_error:
                                      print(f"Error processing mount targets for filesystem {fs_id}: {mt_error}")
                                      
                          except Exception as e:
                              print(f"Error cleaning up EFS in subnet {subnet_id}: {e}")
                      
                      # Wait for EFS mount targets to be deleted
                      if efs_to_delete:
                          print(f"Waiting 120 seconds for {len(efs_to_delete)} EFS mount targets to be deleted...")
                          time.sleep(120)
                          
                          for fs_id in efs_to_delete:
                              try:
                                  remaining_mts = efs.describe_mount_targets(FileSystemId=fs_id)
                                  if not remaining_mts['MountTargets']:
                                      print(f"Deleting EFS filesystem {fs_id}")
                                      efs.delete_file_system(FileSystemId=fs_id)
                                  else:
                                      print(f"EFS filesystem {fs_id} still has mount targets, skipping deletion")
                              except Exception as e:
                                  print(f"Error deleting EFS filesystem {fs_id}: {e}")
                      
                      # Step 4: Wait for network interfaces to be detached
                      print("Step 4: Waiting for network interfaces to detach...")
                      wait_for_network_interfaces_detached(ec2, subnet_ids)
                      
                      # Step 5: Delete network interfaces with retries
                      print("Step 5: Deleting network interfaces...")
                      delete_network_interfaces_with_retry(ec2, subnet_ids)
                      
                      # Step 6: Clean up VPC dependencies
                      if vpc_id:
                          print("Step 6: Cleaning up VPC dependencies...")
                          try:
                              # Clean up security groups
                              print("Cleaning up security groups...")
                              security_groups = ec2.describe_security_groups(
                                  Filters=[{'Name': 'vpc-id', 'Values': [vpc_id]}]
                              )
                              
                              non_default_sgs = [sg for sg in security_groups['SecurityGroups'] if sg['GroupName'] != 'default']
                              
                              # Remove all rules first
                              for sg in non_default_sgs:
                                  sg_id = sg['GroupId']
                                  try:
                                      if sg.get('IpPermissions'):
                                          ec2.revoke_security_group_ingress(
                                              GroupId=sg_id,
                                              IpPermissions=sg['IpPermissions']
                                          )
                                      if sg.get('IpPermissionsEgress'):
                                          ec2.revoke_security_group_egress(
                                              GroupId=sg_id,
                                              IpPermissions=sg['IpPermissionsEgress']
                                          )
                                  except Exception as rule_error:
                                      print(f"Error removing rules from security group {sg_id}: {rule_error}")
                              
                              time.sleep(30)  # Wait for rule changes to propagate
                              
                              # Delete security groups
                              for sg in non_default_sgs:
                                  sg_id = sg['GroupId']
                                  try:
                                      ec2.delete_security_group(GroupId=sg_id)
                                      print(f"Deleted security group {sg_id}")
                                  except Exception as delete_error:
                                      print(f"Could not delete security group {sg_id}: {delete_error}")
                              
                              # Clean up NAT Gateways
                              print("Cleaning up NAT Gateways...")
                              nat_gateways = ec2.describe_nat_gateways(
                                  Filters=[{'Name': 'vpc-id', 'Values': [vpc_id]}]
                              )
                              
                              for nat in nat_gateways['NatGateways']:
                                  if nat['State'] not in ['deleted', 'deleting']:
                                      nat_id = nat['NatGatewayId']
                                      try:
                                          ec2.delete_nat_gateway(NatGatewayId=nat_id)
                                          print(f"Initiated deletion of NAT Gateway {nat_id}")
                                      except Exception as nat_error:
                                          print(f"Error deleting NAT Gateway {nat_id}: {nat_error}")
                                          
                          except Exception as e:
                              print(f"Error cleaning up VPC dependencies: {e}")
                      
                      print("Cleanup completed successfully")
                  
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
                  
              except Exception as e:
                  print(f"Error in cleanup function: {e}")
                  import traceback
                  traceback.print_exc()
                  cfnresponse.send(event, context, cfnresponse.FAILED, {})

  # IAM Role for Bedrock Model Import
  BedrockImportRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub '${AWS::StackName}-BedrockImportRole'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: bedrock.amazonaws.com
            Action: sts:AssumeRole
            Condition:
              StringEquals:
                'aws:SourceAccount': !Ref 'AWS::AccountId'
              ArnEquals:
                'aws:SourceArn': !Sub 'arn:aws:bedrock:${AWS::Region}:${AWS::AccountId}:model-import-job/*'
      Policies:
        - PolicyName: BedrockImportS3Access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:GetObjectVersion
                  - s3:ListBucket
                  - s3:PutObject
                  - s3:PutObjectAcl
                Resource:
                  - !GetAtt WorkshopS3Bucket.Arn
                  - !Sub '${WorkshopS3Bucket.Arn}/*'
        - PolicyName: BedrockImportLogging
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: !Sub 'arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/bedrock/*'
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-BedrockImportRole'
        - Key: Purpose
          Value: 'Bedrock Model Import Jobs'

  # IAM Role for SageMaker Execution
  SageMakerExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub '${AWS::StackName}-SageMakerExecutionRole'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - sagemaker.amazonaws.com
            Action:
              - sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
        - arn:aws:iam::aws:policy/AmazonBedrockFullAccess
      Policies:
        - PolicyName: S3WorkshopBucketFullAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:GetObjectVersion
                  - s3:ListBucket
                  - s3:ListBucketVersions
                  - s3:PutObject
                  - s3:PutObjectAcl
                  - s3:DeleteObject
                  - s3:GetBucketLocation
                  - s3:GetBucketVersioning
                  - s3:GetBucketNotification
                  - s3:PutBucketNotification
                  - s3:GetBucketTagging
                  - s3:PutBucketTagging
                Resource:
                  - !GetAtt WorkshopS3Bucket.Arn
                  - !Sub '${WorkshopS3Bucket.Arn}/*'
              - Effect: Allow
                Action:
                  - s3:ListAllMyBuckets
                  - s3:GetBucketLocation
                Resource: '*'
        - PolicyName: BedrockModelImportPermissions
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - bedrock:CreateModelImportJob
                  - bedrock:GetModelImportJob
                  - bedrock:ListModelImportJobs
                  - bedrock:CreateModelCustomizationJob
                  - bedrock:GetModelCustomizationJob
                  - bedrock:ListModelCustomizationJobs
                  - bedrock:StopModelCustomizationJob
                  - bedrock:CreateProvisionedModelThroughput
                  - bedrock:GetProvisionedModelThroughput
                  - bedrock:ListProvisionedModelThroughputs
                  - bedrock:DeleteProvisionedModelThroughput
                  - bedrock:UpdateProvisionedModelThroughput
                  - bedrock:TagResource
                  - bedrock:UntagResource
                  - bedrock:ListTagsForResource
                  - bedrock:GetFoundationModel
                  - bedrock:ListFoundationModels
                  - bedrock:InvokeModel
                  - bedrock:InvokeModelWithResponseStream
                Resource: '*'
              - Effect: Allow
                Action:
                  - iam:PassRole
                Resource: 
                  - !Sub 'arn:aws:iam::${AWS::AccountId}:role/${AWS::StackName}-SageMakerExecutionRole'
                  - !GetAtt BedrockImportRole.Arn
                Condition:
                  StringEquals:
                    'iam:PassedToService':
                      - bedrock.amazonaws.com
                      - sagemaker.amazonaws.com
        - PolicyName: AdditionalPermissions
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                  - logs:DescribeLogStreams
                  - logs:DescribeLogGroups
                Resource: !Sub 'arn:aws:logs:${AWS::Region}:${AWS::AccountId}:*'
              - Effect: Allow
                Action:
                  - ec2:CreateNetworkInterface
                  - ec2:CreateNetworkInterfacePermission
                  - ec2:DeleteNetworkInterface
                  - ec2:DeleteNetworkInterfacePermission
                  - ec2:DescribeNetworkInterfaces
                  - ec2:DescribeVpcs
                  - ec2:DescribeSubnets
                  - ec2:DescribeSecurityGroups
                Resource: '*'
              - Effect: Allow
                Action:
                  - kms:Decrypt
                  - kms:DescribeKey
                  - kms:GenerateDataKey
                Resource: '*'

  # Security Group for SageMaker Studio
  SageMakerStudioSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupName: !Sub '${AWS::StackName}-SageMakerStudio-SG'
      GroupDescription: 'Security Group for SageMaker Studio Domain'
      VpcId: !If [CreateVpc, !Ref WorkshopVPC, !Ref ExistingVpcId]
      SecurityGroupEgress:
        - IpProtocol: '-1'
          CidrIp: '0.0.0.0/0'
          Description: 'Allow all outbound traffic'
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-SageMakerStudio-SG'

  # Security Group Ingress Rule (separate to avoid circular dependency)
  SageMakerStudioSecurityGroupIngress:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      GroupId: !Ref SageMakerStudioSecurityGroup
      IpProtocol: 'tcp'
      FromPort: 443
      ToPort: 443
      SourceSecurityGroupId: !Ref SageMakerStudioSecurityGroup
      Description: 'Allow HTTPS traffic within security group'

  # SageMaker Studio Domain
  SageMakerStudioDomain:
    Type: AWS::SageMaker::Domain
    DeletionPolicy: Delete
    DependsOn:
      - SageMakerStudioSecurityGroup
      - SageMakerExecutionRole
    Properties:
      DomainName: !Ref DomainName
      AuthMode: 'IAM'
      DefaultUserSettings:
        ExecutionRole: !GetAtt SageMakerExecutionRole.Arn
        SecurityGroups:
          - !Ref SageMakerStudioSecurityGroup
        JupyterServerAppSettings:
          DefaultResourceSpec:
            InstanceType: 'system'
            SageMakerImageArn: !FindInMap [SageMakerImageArn, !Ref 'AWS::Region', JupyterServer]
        KernelGatewayAppSettings:
          DefaultResourceSpec:
            InstanceType: !Ref DefaultInstanceType
        SpaceStorageSettings:
          DefaultEbsStorageSettings:
            DefaultEbsVolumeSizeInGb: 100
            MaximumEbsVolumeSizeInGb: 200
      VpcId: !If [CreateVpc, !Ref WorkshopVPC, !Ref ExistingVpcId]
      SubnetIds: !If 
        - CreateVpc
        - [!Ref PrivateSubnet1, !Ref PrivateSubnet2]
        - !Ref ExistingSubnetIds
      DomainSettings:
        SecurityGroupIds:
          - !Ref SageMakerStudioSecurityGroup
      Tags:
        - Key: Name
          Value: !Ref DomainName
        - Key: Purpose
          Value: 'MCP Workshop Environment'

  # SageMaker Studio User Profile
  SageMakerUserProfile:
    Type: AWS::SageMaker::UserProfile
    DeletionPolicy: Delete
    DependsOn:
      - SageMakerStudioDomain
    Properties:
      DomainId: !Ref SageMakerStudioDomain
      UserProfileName: !Ref UserProfileName
      UserSettings:
        ExecutionRole: !GetAtt SageMakerExecutionRole.Arn
        SecurityGroups:
          - !Ref SageMakerStudioSecurityGroup
        JupyterServerAppSettings:
          DefaultResourceSpec:
            InstanceType: 'system'
            SageMakerImageArn: !FindInMap [SageMakerImageArn, !Ref 'AWS::Region', JupyterServer]
        KernelGatewayAppSettings:
          DefaultResourceSpec:
            InstanceType: !Ref DefaultInstanceType
        SpaceStorageSettings:
          DefaultEbsStorageSettings:
            DefaultEbsVolumeSizeInGb: 100
            MaximumEbsVolumeSizeInGb: 200
      Tags:
        - Key: Name
          Value: !Ref UserProfileName
        - Key: Purpose
          Value: 'MCP Workshop User'

  # JupyterLab Space for ml.g5.8xlarge with 100GB storage
  JupyterLabSpace:
    Type: AWS::SageMaker::Space
    DeletionPolicy: Delete
    DependsOn:
      - SageMakerUserProfile
    Properties:
      DomainId: !Ref SageMakerStudioDomain
      SpaceName: 'jupyterlab-workspace'
      OwnershipSettings:
        OwnerUserProfileName: !Ref UserProfileName
      SpaceSharingSettings:
        SharingType: 'Private'
      SpaceSettings:
        AppType: 'JupyterLab'
        JupyterLabAppSettings:
          DefaultResourceSpec:
            InstanceType: 'ml.g5.8xlarge'
            SageMakerImageArn: !FindInMap [SageMakerImageArn, !Ref 'AWS::Region', JupyterLab]
        SpaceStorageSettings:
          EbsStorageSettings:
            EbsVolumeSizeInGb: 100
      Tags:
        - Key: Name
          Value: 'JupyterLab Workspace'
        - Key: Purpose
          Value: 'MCP Workshop JupyterLab Environment'

  # S3 Bucket for SageMaker and Bedrock
  WorkshopS3Bucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub '${AWS::StackName}-workshop-bucket-${AWS::AccountId}'
      VersioningConfiguration:
        Status: Enabled
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      LifecycleConfiguration:
        Rules:
          - Id: DeleteIncompleteMultipartUploads
            Status: Enabled
            AbortIncompleteMultipartUpload:
              DaysAfterInitiation: 7
          - Id: DeleteOldVersions
            Status: Enabled
            NoncurrentVersionExpirationInDays: 30
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-Workshop-Bucket'
        - Key: Purpose
          Value: 'SageMaker and Bedrock Workshop Storage'

  # Custom Resource to trigger cleanup on stack deletion
  CleanupResource:
    Type: AWS::CloudFormation::CustomResource
    DeletionPolicy: Delete
    DependsOn:
      - JupyterLabSpace
      - SageMakerUserProfile
      - SageMakerStudioDomain
      - CleanupLambdaFunction
      - WorkshopS3Bucket
    Properties:
      ServiceToken: !GetAtt CleanupLambdaFunction.Arn
      SubnetIds: !If 
        - CreateVpc
        - [!Ref PrivateSubnet1, !Ref PrivateSubnet2]
        - !Ref ExistingSubnetIds
      DomainId: !Ref SageMakerStudioDomain
      VpcId: !If [CreateVpc, !Ref WorkshopVPC, !Ref ExistingVpcId]
      S3BucketName: !Ref WorkshopS3Bucket


Outputs:
  SageMakerDomainId:
    Description: 'SageMaker Studio Domain ID'
    Value: !Ref SageMakerStudioDomain
    Export:
      Name: !Sub '${AWS::StackName}-DomainId'

  SageMakerDomainArn:
    Description: 'SageMaker Studio Domain ARN'
    Value: !Sub 'arn:aws:sagemaker:${AWS::Region}:${AWS::AccountId}:domain/${SageMakerStudioDomain}'
    Export:
      Name: !Sub '${AWS::StackName}-DomainArn'

  UserProfileName:
    Description: 'SageMaker Studio User Profile Name'
    Value: !Ref SageMakerUserProfile
    Export:
      Name: !Sub '${AWS::StackName}-UserProfile'

  JupyterLabSpaceName:
    Description: 'JupyterLab Space Name'
    Value: !Ref JupyterLabSpace
    Export:
      Name: !Sub '${AWS::StackName}-JupyterLabSpace'

  ExecutionRoleArn:
    Description: 'SageMaker Execution Role ARN'
    Value: !GetAtt SageMakerExecutionRole.Arn
    Export:
      Name: !Sub '${AWS::StackName}-ExecutionRole'

  BedrockImportRoleArn:
    Description: 'Bedrock Model Import Role ARN'
    Value: !GetAtt BedrockImportRole.Arn
    Export:
      Name: !Sub '${AWS::StackName}-BedrockImportRole'

  StudioUrl:
    Description: 'SageMaker Studio URL'
    Value: !Sub 'https://${SageMakerStudioDomain}.studio.${AWS::Region}.sagemaker.aws/jupyter/default/lab'

  ConsoleUrl:
    Description: 'SageMaker Studio Console URL'
    Value: !Sub 'https://console.aws.amazon.com/sagemaker/home?region=${AWS::Region}#/studio/${SageMakerStudioDomain}'

  SecurityGroupId:
    Description: 'Security Group ID for SageMaker Studio'
    Value: !Ref SageMakerStudioSecurityGroup
    Export:
      Name: !Sub '${AWS::StackName}-SecurityGroup'

  WorkshopS3BucketName:
    Description: 'S3 Bucket for SageMaker and Bedrock Workshop'
    Value: !Ref WorkshopS3Bucket
    Export:
      Name: !Sub '${AWS::StackName}-WorkshopBucket'

  WorkshopS3BucketArn:
    Description: 'S3 Bucket ARN for SageMaker and Bedrock Workshop'
    Value: !GetAtt WorkshopS3Bucket.Arn
    Export:
      Name: !Sub '${AWS::StackName}-WorkshopBucketArn'

  # VPC-related outputs (only shown if VPC was created)
  VpcId:
    Condition: CreateVpc
    Description: 'VPC ID created for the workshop'
    Value: !Ref WorkshopVPC
    Export:
      Name: !Sub '${AWS::StackName}-VPC'

  PrivateSubnet1Id:
    Condition: CreateVpc
    Description: 'Private Subnet 1 ID'
    Value: !Ref PrivateSubnet1
    Export:
      Name: !Sub '${AWS::StackName}-PrivateSubnet1'

  PrivateSubnet2Id:
    Condition: CreateVpc
    Description: 'Private Subnet 2 ID'
    Value: !Ref PrivateSubnet2
    Export:
      Name: !Sub '${AWS::StackName}-PrivateSubnet2'

  PublicSubnet1Id:
    Condition: CreateVpc
    Description: 'Public Subnet 1 ID'
    Value: !Ref PublicSubnet1
    Export:
      Name: !Sub '${AWS::StackName}-PublicSubnet1'

  PublicSubnet2Id:
    Condition: CreateVpc
    Description: 'Public Subnet 2 ID'
    Value: !Ref PublicSubnet2
    Export:
      Name: !Sub '${AWS::StackName}-PublicSubnet2'

  NatGatewayId:
    Condition: CreateVpc
    Description: 'NAT Gateway ID'
    Value: !Ref NatGateway

  DeploymentInstructions:
    Description: 'Post-deployment instructions'
    Value: |
      1. Navigate to the SageMaker Console URL provided above
      2. Select your user profile (workshop-user) and click 'Launch' -> 'Studio'
      3. A pre-configured JupyterLab space (jupyterlab-workspace) with ml.m5.4xlarge and 100GB storage is available
      4. Click on the 'jupyterlab-workspace' space and select 'Run space' to launch JupyterLab
      5. Alternatively, create new notebooks with your preferred instance type (default: ml.g5.8xlarge)
      6. Storage is configured with 100GB default, expandable to 200GB
      7. For Bedrock Model Import: Use the BedrockImportRoleArn from outputs
      8. S3 Bucket for model artifacts: Use WorkshopS3BucketName from outputs
      9. Upload your Mistral workshop notebook and begin!

Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label:
          default: "SageMaker Configuration"
        Parameters:
          - DomainName
          - UserProfileName
          - DefaultInstanceType
      - Label:
          default: "Network Configuration"
        Parameters:
          - CreateVpcResources
          - VpcCidr
          - ExistingVpcId
          - ExistingSubnetIds
    ParameterLabels:
      DomainName:
        default: "Domain Name"
      UserProfileName:
        default: "User Profile Name"
      DefaultInstanceType:
        default: "Default Instance Type"
      CreateVpcResources:
        default: "Create VPC Resources?"
      VpcCidr:
        default: "VPC CIDR Block"
      ExistingVpcId:
        default: "Existing VPC ID"
      ExistingSubnetIds:
        default: "Existing Subnet IDs"
